[{"content":"To be doneğŸ˜ ","permalink":"https://zhouyeyu.github.io/posts/test/","summary":"\u003ch1 id=\"to-be-done\"\u003eTo be doneğŸ˜\u003c/h1\u003e","title":"Hugoéƒ¨ç½²github Pages"},{"content":"[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352 https://github.com/pytorch/pytorch/pull/158352\nä¸“æ å¦ä¸€ç¯‡æ–‡ç« è§£è¯»äº†record_streamç›¸å…³å†…å®¹ï¼Œä½†CUDACachingAllocatoræºç ä¸­å­˜åœ¨ä¸€ä¸ªåˆ†æ”¯ï¼Œä¼šæ ¹æ®CUDAAllocatorConfig::graph_capture_record_stream_reuse()åˆ¤æ–­æ˜¯å¦freeå’ŒmallocæŸä¸ªblockï¼Œè¿™é‡Œå¡«ä¸ªå‘ï¼Œæ‰¾ä¸€ä¸‹å¯¹åº”çš„prçœ‹æ˜¯æ€ä¹ˆå¤ç”¨çš„ã€‚\nèƒŒæ™¯ï¼šCUDA Graph ä¸‹æ˜¾å­˜ç®¡ç†é™åˆ¶ åœ¨ CUDA Graph æ•è·è¿‡ç¨‹ä¸­ï¼ŒCUDACachingAllocatorå¯¹å†…å­˜çš„ç®¡ç†å­˜åœ¨ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå¿…é¡»ç­‰åˆ°æ•è·ç»“æŸåæ‰èƒ½å›æ”¶å†…å­˜å—ã€‚è¿™ä¸€é™åˆ¶æºäº CUDA çš„åº•å±‚çº¦æŸ â€”â€” æ•è·é˜¶æ®µä¸å…è®¸æŸ¥è¯¢eventçŠ¶æ€ï¼ˆå› ä¸ºæ­¤æ—¶ CUDA æ“ä½œå°šæœªæ‰§è¡Œï¼‰ï¼Œè€ŒCUDACachingAllocatorä¾èµ–äº‹ä»¶é©±åŠ¨é€»è¾‘åˆ¤æ–­å†…å­˜å—æ˜¯å¦å¯å›æ”¶ã€‚è€Œè¿™ä¼šå¯¼è‡´æœ¬è¯¥è¢«é‡Šæ”¾çš„blockæœªèƒ½è¢«æ­£ç¡®é‡Šæ”¾ï¼Œç›´åˆ°æ•è·å®Œå…¨ç»“æŸï¼Œæ¯”å¦‚freeä¸­çš„ä»£ç ç‰‡æ®µï¼š\nif (!block-\u0026gt;stream_uses.empty()) { if (C10_UNLIKELY(!captures_underway.empty())) { // It\u0026#39;s forbidden to cudaEventQuery an event recorded during CUDA graph // capture. We conservatively defer recording end-of-life events until // the next call to process_events() (which won\u0026#39;t happen until no // captures are underway) needs_events_deferred_until_no_capture.push_back(block); cudaGraphä¸‹å®‰å…¨çš„å¤šæµå¤ç”¨ ä¸ºå®ç°å®‰å…¨é‡ç”¨ï¼ŒPR é¦–å…ˆæ˜ç¡®äº†ä¸¤ä¸ªæ ¸å¿ƒæœ¯è¯­ï¼Œä½œä¸ºåç»­åˆ¤æ–­é€»è¾‘çš„åŸºç¡€ï¼š\nFree markerï¼šé€šè¿‡cudaGraphAddEmptyNodeåˆ›å»ºçš„ â€œæ•è·åˆæ³•â€ ç©ºèŠ‚ç‚¹ï¼Œæ’å…¥åˆ°æ¯ä¸ªä½¿ç”¨è¿‡è¯¥å†…å­˜å—çš„streamä¸­ï¼Œä¸”ä½äºè¯¥å—æœ€åä¸€æ¬¡è¢«ä½¿ç”¨çš„æ“ä½œä¹‹åï¼Œç”¨äºæ ‡è®° â€œå†…å­˜å—å·²ç©ºé—²â€ã€‚ Terminal Nodeï¼šæµæˆ–æ•è·å›¾ä¸­ â€œæœ€æ–°æ“ä½œâ€ çš„é›†åˆï¼Œæ–°æ•è·çš„æ“ä½œä¼šé™„åŠ åœ¨ç»ˆç«¯èŠ‚ç‚¹ä¹‹åã€‚å¯¹äºæ­£åœ¨æ•è·çš„æµï¼Œå¯é€šè¿‡cudaStreamGetCaptureInfoçš„dependencies_outå‚æ•°è·å–ç»ˆç«¯èŠ‚ç‚¹é›†åˆã€‚ å†…å­˜å—å¯é‡ç”¨æ€§åˆ¤æ–­è§„åˆ™ cudaGraphç”Ÿæˆä¸€ä¸ªDAGï¼Œå› æ­¤ä½œè€…æå‡ºäº†ä¸¤ç§å¹³è¡¡ â€œå®‰å…¨æ€§â€ ä¸ â€œçµæ´»æ€§â€ çš„åˆ¤æ–­è§„åˆ™ï¼š\nStrong Rule (Graph-Wide Safety)ï¼šä»å…¨å±€ä¿éšœå®‰å…¨ï¼Œè§„å®šè‹¥å†…å­˜å—çš„æ‰€æœ‰ç©ºé—²æ ‡è®°å‡ä¸ºæ‰€æœ‰æ´»è·ƒæµçš„ç»ˆç«¯èŠ‚ç‚¹çš„ â€œå‰é©±èŠ‚ç‚¹â€ï¼Œåˆ™è¯¥å—å¯å®‰å…¨é‡ç”¨ï¼Œé€šè¿‡ä¸¥æ ¼çš„å…¨å±€æ‰§è¡Œé¡ºåºé¿å…é‡æ”¾æ—¶çš„ç”Ÿå‘½å‘¨æœŸé‡å ï¼› Per-stream Rule (A Practical Optimization)ï¼š ä»…éªŒè¯å•æµï¼Œå¯¹äºæµ S ä¸Šçš„åˆ†é…è¯·æ±‚ï¼Œè‹¥å†…å­˜å—çš„æ‰€æœ‰ç©ºé—²æ ‡è®°æ˜¯æµ S çš„ç»ˆç«¯èŠ‚ç‚¹çš„å‰é©±èŠ‚ç‚¹ï¼Œå³å¯åœ¨æµ S ä¸Šé‡ç”¨ï¼Œå› æ­¤æ— éœ€éªŒè¯å…¨å›¾å®‰å…¨æ€§ã€‚ æ ¸å¿ƒå®ç°æµç¨‹ æ•è·æœŸé—´æ‰§è¡Œfree å¯¹block-\u0026gt;stream_usesä¸­çš„æ¯ä¸ªæµåŠ â€œåˆ†é…æµâ€ï¼Œæ’å…¥free markerï¼Œå…³è”å¯¹åº”çš„blockå’Œfree markeræ‰€å¯¹åº”çš„ç©ºnode if (CUDAAllocatorConfig::graph_capture_record_stream_reuse()) { deferred_blocks.emplace(block, insert_free_marker(block)); // æ’å…¥free marker } else { deferred_blocks.emplace(block, std::vector\u0026lt;cudaGraphNode_t\u0026gt;{}); // æ•è·ç»“æŸåè®¿é—® } æ•è·æœŸé—´malloc if (CUDAAllocatorConfig::graph_capture_record_stream_reuse()) { free_safe_blocks_in_capture(context, stream); } free_safe_blocks_in_captureå‡½æ•°å†…å®¹ï¼š get_reusable_empty_nodesé€šè¿‡è·å–å½“å‰æµçš„ç»ˆç«¯èŠ‚ç‚¹ï¼Œé€šè¿‡åå‘ DFS éå†æ¯ä¸ªç»ˆç«¯èŠ‚ç‚¹çš„ä¾èµ–é“¾ï¼Œç»Ÿè®¡ç©ºé—²èŠ‚ç‚¹èƒ½åˆ°è¾¾çš„ç»ˆç«¯æ•°é‡ï¼›æœ€ç»ˆç­›é€‰å‡º â€œèƒ½åˆ°è¾¾æ‰€æœ‰ç»ˆç«¯èŠ‚ç‚¹â€ çš„ç©ºé—²èŠ‚ç‚¹ï¼Œæ„æˆå½“å‰æµå¯å¤ç”¨çš„ç©ºé—²èŠ‚ç‚¹é›†åˆï¼Œä»è€Œåˆ¤æ–­å“ªäº›free markerå±äºå½“å‰æµå¯å¤ç”¨ç©ºé—²èŠ‚ç‚¹ï¼Œä¹‹åï¼Œåˆ¤æ–­freeæ—¶è½½å…¥è¿‡çš„blockä¸­å“ªäº›å¯ä»¥è¢«æ­£ç¡®å¤ç”¨ï¼Œå¦‚æœå¯å¤ç”¨åˆ™é‡Šæ”¾ï¼š\nauto reusable_empty_nodes = get_reusable_empty_nodes(stream); if (reusable_empty_nodes.empty()) { return; } std::vector\u0026lt;Block*\u0026gt; blocks_to_erase; // è®°å½•å¾…åˆ é™¤çš„å— for (auto\u0026amp; [block, inserted_empty_nodes] : deferred_blocks) { // è·³è¿‡ä¸¤ç±»å—ï¼š1. æ— ç©ºé—²æ ‡è®°çš„å—ï¼ˆæ— æ³•åˆ¤æ–­å®‰å…¨ï¼‰ï¼›2. åˆ†é…æµâ‰ å½“å‰æµçš„å— if (inserted_empty_nodes.empty() || block-\u0026gt;stream != stream) { continue; } // è¯¥å—çš„æ‰€æœ‰ç©ºé—²æ ‡è®°æ˜¯å¦éƒ½åœ¨reusable_empty_nodesä¸­ bool is_reusable = true; for (const auto\u0026amp; node : inserted_empty_nodes) { if (reusable_empty_nodes.find(node) == reusable_empty_nodes.end()) { is_reusable = false; break; } } // è‹¥æ‰€æœ‰æ ‡è®°éƒ½å®‰å…¨ â†’ è¯¥å—å¯å›æ”¶ if (is_reusable) { // æ¸…é™¤stream_usesï¼šGraphå·²é€šè¿‡ä¾èµ–ä¿è¯åŒæ­¥ï¼Œæ— éœ€å†è·Ÿè¸ªå¤šæµä½¿ç”¨ block-\u0026gt;stream_uses.clear(); // å°†å—å›æ”¶åˆ°ç©ºé—²åˆ—è¡¨ free_block(block, context); // è®°å½•è¯¥å—ï¼Œåç»­ä»deferred_blocksä¸­åˆ é™¤ blocks_to_erase.push_back(block); } } ç°åœ¨ï¼Œåªè¦å¼€å¯graph_capture_record_stream_reuseï¼Œpytorchä¼šè‡ªåŠ¨å°è¯•åœ¨cuda_graphä¸­é€šè¿‡æ’å…¥free markeræ¥æå‰é‡Šæ”¾ä¸€äº›blockï¼Œä½¿å…¶ç”Ÿå‘½å‘¨æœŸå˜çŸ­ï¼Œè€Œä¸å†æ˜¯ç­‰åˆ°æ•è·å®Œå…¨ç»“æŸæ—¶ä¸€æ¬¡æ€§é‡Šæ”¾ã€‚\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/","summary":"\u003ch2 id=\"cuda-reuse-blocks-with-record_stream-during-cuda-graph-capture-in-the-cudacachingallocator-158352\"\u003e[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/pytorch/pytorch/pull/158352\"\u003ehttps://github.com/pytorch/pytorch/pull/158352\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eä¸“æ å¦ä¸€ç¯‡æ–‡ç« è§£è¯»äº†record_streamç›¸å…³å†…å®¹ï¼Œä½†CUDACachingAllocatoræºç ä¸­å­˜åœ¨ä¸€ä¸ªåˆ†æ”¯ï¼Œä¼šæ ¹æ®CUDAAllocatorConfig::graph_capture_record_stream_reuse()åˆ¤æ–­æ˜¯å¦freeå’ŒmallocæŸä¸ªblockï¼Œè¿™é‡Œå¡«ä¸ªå‘ï¼Œæ‰¾ä¸€ä¸‹å¯¹åº”çš„prçœ‹æ˜¯æ€ä¹ˆå¤ç”¨çš„ã€‚\u003c/p\u003e\n\u003ch2 id=\"èƒŒæ™¯cuda-graph-ä¸‹æ˜¾å­˜ç®¡ç†é™åˆ¶\"\u003eèƒŒæ™¯ï¼šCUDA Graph ä¸‹æ˜¾å­˜ç®¡ç†é™åˆ¶\u003c/h2\u003e\n\u003cp\u003eåœ¨ CUDA Graph æ•è·è¿‡ç¨‹ä¸­ï¼ŒCUDACachingAllocatorå¯¹å†…å­˜çš„ç®¡ç†å­˜åœ¨ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå¿…é¡»ç­‰åˆ°æ•è·ç»“æŸåæ‰èƒ½å›æ”¶å†…å­˜å—ã€‚è¿™ä¸€é™åˆ¶æºäº CUDA çš„åº•å±‚çº¦æŸ â€”â€” æ•è·é˜¶æ®µä¸å…è®¸æŸ¥è¯¢eventçŠ¶æ€ï¼ˆå› ä¸ºæ­¤æ—¶ CUDA æ“ä½œå°šæœªæ‰§è¡Œï¼‰ï¼Œè€ŒCUDACachingAllocatorä¾èµ–äº‹ä»¶é©±åŠ¨é€»è¾‘åˆ¤æ–­å†…å­˜å—æ˜¯å¦å¯å›æ”¶ã€‚è€Œè¿™ä¼šå¯¼è‡´æœ¬è¯¥è¢«é‡Šæ”¾çš„blockæœªèƒ½è¢«æ­£ç¡®é‡Šæ”¾ï¼Œç›´åˆ°æ•è·å®Œå…¨ç»“æŸï¼Œæ¯”å¦‚freeä¸­çš„ä»£ç ç‰‡æ®µï¼š\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003estream_uses\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eempty\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eC10_UNLIKELY\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ecaptures_underway\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eempty\u003c/span\u003e\u003cspan class=\"p\"\u003e()))\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e// It\u0026#39;s forbidden to cudaEventQuery an event recorded during CUDA graph\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e// capture. We conservatively defer recording end-of-life events until\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e// the next call to process_events() (which won\u0026#39;t happen until no\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e// captures are underway)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eneeds_events_deferred_until_no_capture\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epush_back\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"cudagraphä¸‹å®‰å…¨çš„å¤šæµå¤ç”¨\"\u003ecudaGraphä¸‹å®‰å…¨çš„å¤šæµå¤ç”¨\u003c/h2\u003e\n\u003cp\u003eä¸ºå®ç°å®‰å…¨é‡ç”¨ï¼ŒPR é¦–å…ˆæ˜ç¡®äº†ä¸¤ä¸ªæ ¸å¿ƒæœ¯è¯­ï¼Œä½œä¸ºåç»­åˆ¤æ–­é€»è¾‘çš„åŸºç¡€ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFree markerï¼šé€šè¿‡cudaGraphAddEmptyNodeåˆ›å»ºçš„ â€œæ•è·åˆæ³•â€ ç©ºèŠ‚ç‚¹ï¼Œæ’å…¥åˆ°æ¯ä¸ªä½¿ç”¨è¿‡è¯¥å†…å­˜å—çš„streamä¸­ï¼Œä¸”ä½äºè¯¥å—æœ€åä¸€æ¬¡è¢«ä½¿ç”¨çš„æ“ä½œä¹‹åï¼Œç”¨äºæ ‡è®° â€œå†…å­˜å—å·²ç©ºé—²â€ã€‚\u003c/li\u003e\n\u003cli\u003eTerminal Nodeï¼šæµæˆ–æ•è·å›¾ä¸­ â€œæœ€æ–°æ“ä½œâ€ çš„é›†åˆï¼Œæ–°æ•è·çš„æ“ä½œä¼šé™„åŠ åœ¨ç»ˆç«¯èŠ‚ç‚¹ä¹‹åã€‚å¯¹äºæ­£åœ¨æ•è·çš„æµï¼Œå¯é€šè¿‡cudaStreamGetCaptureInfoçš„dependencies_outå‚æ•°è·å–ç»ˆç«¯èŠ‚ç‚¹é›†åˆã€‚\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"å†…å­˜å—å¯é‡ç”¨æ€§åˆ¤æ–­è§„åˆ™\"\u003eå†…å­˜å—å¯é‡ç”¨æ€§åˆ¤æ–­è§„åˆ™\u003c/h3\u003e\n\u003cp\u003ecudaGraphç”Ÿæˆä¸€ä¸ªDAGï¼Œå› æ­¤ä½œè€…æå‡ºäº†ä¸¤ç§å¹³è¡¡ â€œå®‰å…¨æ€§â€ ä¸ â€œçµæ´»æ€§â€ çš„åˆ¤æ–­è§„åˆ™ï¼š\u003c/p\u003e","title":"PyTorch pr(2) cudaGraphä¸‹å¤šæµæ˜¾å­˜å¤ç”¨"},{"content":"æ¦‚è¿° record_streamæ˜¯ PyTorch ä¸­ç”¨äºç®¡ç† CUDA æµä¹‹é—´Tensorç”Ÿå‘½å‘¨æœŸå’Œä¾èµ–å…³ç³»çš„ä¸€ä¸ªå…³é”®æœºåˆ¶ã€‚å®ƒçš„æ ¸å¿ƒç›®çš„æ˜¯ç¡®ä¿åœ¨å¼‚æ­¥æ‰§è¡Œçš„ GPU æ“ä½œä¸­ï¼Œæ•°æ®çš„ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼Œé˜²æ­¢å› æ“ä½œé‡å å¯¼è‡´çš„æ•°æ®æŸåæˆ–è®¡ç®—é”™è¯¯ã€‚\nä¸ºä»€ä¹ˆéœ€è¦ record_streamï¼Ÿ è¦ç†è§£ record_streamï¼Œé¦–å…ˆçœ‹ä¸€ä¸‹ CUDA æµçš„ç›¸å…³æ¦‚å¿µï¼š\nCUDA æµï¼šå¯ä»¥çœ‹ä½œæ˜¯ GPU ä¸Šçš„ä¸€ä¸ªâ€œä»»åŠ¡é˜Ÿåˆ—â€ã€‚æäº¤ç»™åŒä¸€ä¸ªæµçš„æ“ä½œä¼šæŒ‰ç…§å…ˆè¿›å…ˆå‡ºçš„é¡ºåºå¼‚æ­¥æ‰§è¡Œï¼› å¼‚æ­¥æ‰§è¡Œï¼šCPU å°†ä»»åŠ¡æäº¤ç»™ GPU çš„æŸä¸ªæµåï¼Œä¸ä¼šç­‰å¾… GPU å®Œæˆå°±ç›´æ¥è¿”å›ï¼Œç»§ç»­æ‰§è¡Œåç»­çš„ CPU ä»£ç ï¼› å¤šæµå¹¶è¡Œï¼šå…è®¸åˆ›å»ºå¤šä¸ªæµï¼Œè®© GPU åŒæ—¶æ‰§è¡Œæ¥è‡ªä¸åŒæµçš„ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´é«˜å±‚æ¬¡çš„å¹¶è¡Œã€‚ ä¸ºä»€ä¹ˆä¼šå‡ºç°è®¡ç®—é”™è¯¯ï¼Ÿä¸¾ä¸ªä¾‹å­ï¼š å‡è®¾Stream Aæ˜¯ä¸€ä¸ªé€šä¿¡æµï¼Œè´Ÿè´£é€šè¿‡all-gatheræ”¶é›†ä¸€ä¸ªTensorTï¼ŒStream B æ˜¯ä¸€ä¸ªè®¡ç®—æµï¼Œè´Ÿè´£ä½¿ç”¨Tensor T è¿›è¡Œè®¡ç®—ã€‚ç”±äºæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼ŒCPU åœ¨å‘èµ·æ‹·è´åç«‹åˆ»å°±å»æäº¤è®¡ç®—ä»»åŠ¡ï¼Œæ­¤æ—¶ Stream A çš„æ‹·è´æ“ä½œå¾ˆå¯èƒ½è¿˜æ²¡å®Œæˆã€‚å¦‚æœ Stream B å¼€å§‹è¯»å– Tï¼Œå®ƒå¯èƒ½ä¼šè¯»åˆ°ä¸å®Œæ•´æˆ–é”™è¯¯çš„æ•°æ®ï¼Œå¯¼è‡´è®¡ç®—ç»“æœé”™è¯¯ã€‚\nå†çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœCPUä¸‹å‘Tensorçš„ç›¸å…³è®¡ç®—æŒ‡ä»¤åï¼Œè®¤ä¸ºè¯¥Tensorå¯ä»¥åˆ é™¤ï¼Œæ‰§è¡Œäº†delå‘½ä»¤ï¼Œä½†ç›¸å…³kernelå†…çš„è®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œä¹Ÿæœ‰å¯èƒ½å¯¼è‡´è®¡ç®—è¿‡ç¨‹ä¸­è¯»å–åˆ°é”™è¯¯çš„æ•°æ®ï¼Œä»è€Œå¯¼è‡´è®¡ç®—é”™è¯¯ã€‚\nå½“ä½¿ç”¨T.record_streamæ—¶ï¼Œä¼šå¼ºåˆ¶ç­‰å¾…è¯¥æµï¼Œä¹‹åå†è¿›è¡Œå…¶ä»–æµçš„æ“ä½œï¼›å°è¯•é‡Šæ”¾æ—¶ï¼Œä¹Ÿä¼šç­‰å¾…è¿™ä¸ªæµï¼Œæ­¤æ—¶ç”³è¯·çš„å†…å­˜ä¸æ˜¯çœŸæ­£çš„è¢«é‡Šæ”¾äº†ï¼Œé˜²æ­¢å‡ºç°æ•°æ®è¸©è¸æˆ–è¯»å–è„æ•°æ®ã€‚\nT.record_stream(s1) record_streamåŸç† æŸ¥çœ‹record_streamæºç ï¼Œå‘ç°ä¸Caching Allocatorç›¸å…³\nvoid record_stream_cuda(Tensor\u0026amp; self, c10::Stream stream) { struct c10::StreamData3 data = stream.pack3(); c10::cuda::CUDACachingAllocator::recordStream(self.storage().data_ptr(), at::cuda::CUDAStream::unpack3(data.stream_id, data.device_index, data.device_type)); } record_streamä¼šåœ¨Tensorå¯¹åº”çš„Blockä¸­æ’å…¥ä¸€ä¸ªeventï¼Œå¹¶æ ¹æ®ä½¿ç”¨çš„æµæ•°å¢åŠ å¼•ç”¨è®¡æ•°ï¼Œå³ä½¿ CPU ç«¯å¼•ç”¨å½’é›¶ï¼ŒCaching Allocatorä¹Ÿä¼šæ‹¦æˆªé‡Šæ”¾è¯·æ±‚ï¼Œå°†è¯¥æ˜¾å­˜å—æ ‡è®°ä¸ºâ€œæµå ç”¨â€çŠ¶æ€å¹¶ç§»å…¥å¾…å›æ”¶é˜Ÿåˆ—ï¼ŒåŒæ—¶æŒç»­è½®è¯¢è¯¥æµçš„æ‰§è¡ŒçŠ¶æ€ã€‚åªæœ‰å½“ç¡®è®¤æµä¸­æ‰€æœ‰ä»»åŠ¡å®Œå…¨ç»“æŸæ—¶ï¼ŒCaching Allocatoræ‰ä¼šçœŸæ­£å°†æ˜¾å­˜é‡Šæ”¾ï¼ˆä¹Ÿä¸æ˜¯çœŸæ­£çš„é‡Šæ”¾ï¼ŒæŒ‡å¯ä»¥è¢«å¤ç”¨ï¼‰ã€‚\næŸ¥çœ‹Caching Allocatorä¸­çš„æºç ï¼š\nvoid recordStream(Block* block, cuda::CUDAStream stream) { std::lock_guard\u0026lt;std::recursive_mutex\u0026gt; lock(mutex); if (stream.stream() == block-\u0026gt;stream) { return; } block-\u0026gt;stream_uses.insert(stream); // è®°å½•ä½¿ç”¨è¿™å—blockçš„æµ if (C10_UNLIKELY(!captures_underway.empty())) { block_to_cudagraph_stream_uses[block].insert(stream); } } é€šè¿‡block-\u0026gt;stream_uses.insert(stream); è®°å½•ä¸‹æ‰€æœ‰ä½¿ç”¨è¿™å—blockçš„æµã€‚å•å•åªæ˜¯è®°å½•å¹¶ä¸èƒ½å¼ºåˆ¶åŒæ­¥ï¼Œæ‰¾ä¸€ä¸‹æ˜¯å“ªé‡Œæ’å…¥è¿™ä¸ªeventçš„ï¼Œå‘ç°Caching Allocatorä¸­é€šè¿‡insert_events å‡½æ•°å¯¹blockæ‰€åœ¨çš„æµæ’å…¥eventå¹¶å®ç°å¼•ç”¨è®¡æ•°ï¼š\nvoid insert_events(Block* block) { .... stream_set streams(std::move(block-\u0026gt;stream_uses)); // è¯»å–åˆšåˆšè®°å½•çš„æµ for (auto\u0026amp; stream : streams) { EventPool::Event event = create_event_internal(stream.device_index()); C10_CUDA_CHECK(cudaEventRecord(*event, stream.stream())); // æ’å…¥event block-\u0026gt;event_count++; // å¼•ç”¨è®¡æ•° cuda_events[stream].emplace_back(std::move(event), block); } .... } insert_eventsåœ¨ä¸€ä¸ªblockå°è¯•è¢«freeçš„æ—¶å€™æ‰ä¼šè¢«è°ƒç”¨ï¼Œæ„æ€æ˜¯è¯´å½“å°è¯•freeä¸€å—å†…å­˜æ—¶ï¼ŒCaching Allocatorä¼šå…ˆæŸ¥æ‰¾æ˜¯å¦è¢«å¤šä¸ªæµä½¿ç”¨ï¼Œä¸€æ—¦è¢«å¤šä¸ªæµä½¿ç”¨å°±ä¸èƒ½è¢«è½»æ˜“freeï¼Œéœ€è¦åœ¨å¯¹åº”çš„æµä¸Šæ’å…¥eventå¹¶è®°å½•ï¼š\nvoid free(Block* block) { .... if (!block-\u0026gt;stream_uses.empty()) { if (C10_UNLIKELY(!captures_underway.empty())) { .... // å°è¯•å¤ç”¨ï¼ˆæŒ–ä¸ªå‘ğŸ•³ï¸ï¼‰ } else { insert_events(block); // æ’å…¥event } .... å‚è€ƒç¤¾åŒºçš„è¿™å¼ å›¾ï¼Œdelçš„eventä¼šè¢«æ’å…¥æµå½“å‰æ‰§è¡Œçš„ä½ç½®ï¼Œä¸€æ—¦å‰é¢çš„æ“ä½œéƒ½æ‰§è¡Œç»“æŸäº†ï¼Œè¿™æ¡æµä¸Šçš„eventå°±å¯ä»¥è¢«è§¦å‘äº†ã€‚\næ€è€ƒå‡ ä¸ªé—®é¢˜ï¼š\næ€ä¹ˆåˆ¤æ–­eventè¢«è§¦å‘äº†ï¼Ÿ ç­”æ¡ˆæ˜¯è½®è¯¢ æ€ä¹ˆè§¦å‘è½®è¯¢ï¼Ÿ è½®è¯¢çš„é¢‘ç‡æ˜¯ä»€ä¹ˆï¼Ÿ å…ˆçœ‹ä¸€ä¸‹è½®è¯¢æ˜¯æ€ä¹ˆè¿›è¡Œçš„ï¼Œä»¥åŠè½®è¯¢åæ˜¯æ€ä¹ˆé‡Šæ”¾blockçš„ã€‚caching allocatorä¼šé€šè¿‡cudaEventQueryè½®è¯¢å¯¹åº”çš„eventæ˜¯å¦è¢«ç»“æŸï¼Œå¦‚æœç»“æŸï¼Œå¯¹åº”çš„è®¡æ•°-1ï¼Œç›´åˆ°è®¡æ•°ä¸º0ï¼Œæ ‡å¿—ç€blockç»ˆäºå¯ä»¥è¢«æ­£ç¡®çš„freeäº†ã€‚\nvoid process_events(const std::shared_ptr\u0026lt;GatheredContext\u0026gt;\u0026amp; context) { insert_events_deferred_until_no_capture(context); for (auto it = cuda_events.begin(); it != cuda_events.end();) { while (!it-\u0026gt;second.empty()) { auto\u0026amp; e = it-\u0026gt;second.front(); EventPool::Event event = std::move(e.first); Block* block = e.second; cudaError_t err = C10_CUDA_ERROR_HANDLED(cudaEventQuery(*event)); // è½®è¯¢ if (err == cudaErrorNotReady) { // ignore and clear the error if not ready (void)cudaGetLastError(); // Return the ownership of the Event (unique ptr) e.first = std::move(event); break; } else if (err != cudaSuccess) { C10_CUDA_CHECK(err); } block-\u0026gt;event_count--; // å¼•ç”¨è®¡æ•°-1 if (block-\u0026gt;event_count == 0) { free_block(block, context); // å¦‚æœå¼•ç”¨è®¡æ•°ä¸º0ï¼Œæ­£ç¡®free } it-\u0026gt;second.pop_front(); } if (it-\u0026gt;second.empty()) { it = cuda_events.erase(it); } else { it++; } } } çœ‹ä¸€ä¸‹è½®è¯¢æ˜¯åœ¨å“ªé‡Œè¢«è§¦å‘çš„ï¼Ÿç»“è®ºæ˜¯åœ¨æ¯æ¬¡mallocçš„æ—¶å€™ï¼Œé‚£ä¹ˆè½®è¯¢çš„é¢‘ç‡ä¹Ÿç¡®å®šäº†ï¼Œå°±æ˜¯æ¯å½“å°è¯•ç”³è¯·ä¸€å—æ–°çš„blockæ—¶ï¼Œç³»ç»Ÿä¼šå°è¯•æŸ¥çœ‹æ˜¯å¦å­˜åœ¨è¢«å¤šæµä½¿ç”¨çš„blockå·²ç»å¯ä»¥freeäº†ï¼Œå¦‚æœæ˜¯ï¼Œé‚£ä¹ˆä¼šå°è¯•å¤ç”¨å®ƒã€‚\nBlock* malloc( c10::DeviceIndex device, size_t orig_size, cudaStream_t stream) { .... if (C10_LIKELY(captures_underway.empty())) { process_events(context); //è½®è¯¢ record_streamçš„å±€é™æ€§ record_streamèƒ½å¾ˆå¥½çš„å®ç°åŒæ­¥ï¼Œé˜²æ­¢æ•°æ®è¸©è¸ï¼Œé‚£ä¹ˆå®ƒçš„æœºåˆ¶æ˜¯å¦éšå«äº†å®ƒçš„ä¸€äº›å±€é™æ€§å‘¢ï¼Ÿé¦–å…ˆè½®è¯¢ä¼šå¯¼è‡´ä¸€ç‚¹ç‚¹æ€§èƒ½ä¸‹é™ï¼Œå¦ä¸€æ–¹é¢ç”±äºé‡Šæ”¾å˜æ…¢äº†ï¼Œå¯¼è‡´blockçš„ç”Ÿå‘½å‘¨æœŸå˜é•¿äº†ï¼Œé‚£ä¹ˆå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æ˜¾å­˜çš„å³°å€¼ã€‚å¦‚å›¾æ‰€ç¤ºï¼š\næ˜¾å­˜å³°å€¼ è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆFSDP2ä¸­ä¸å†ç»§ç»­ä½¿ç”¨record_streamç®¡ç†ç”Ÿå‘½å‘¨æœŸçš„ä¸€éƒ¨åˆ†åŸå› ï¼Œå¯ä»¥å‚è€ƒä¸“æ çš„å¦ä¸€ç¯‡æ–‡ç« ï¼šPyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ï¼Œçœ‹å†…å­˜å¤ç”¨é—®é¢˜æ˜¯æ€ä¹ˆäº§ç”Ÿä»¥åŠæ€ä¹ˆè¢«ä¸€æ­¥æ­¥è§£å†³çš„ã€‚\nReference https://github.com/pytorch/pytorch/blob/main/c10/cuda/CUDACachingAllocator.cpp\nhttps://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-record_stream-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","summary":"\u003ch2 id=\"æ¦‚è¿°\"\u003eæ¦‚è¿°\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003erecord_stream\u003c/code\u003eæ˜¯ PyTorch ä¸­ç”¨äºç®¡ç† CUDA æµä¹‹é—´Tensorç”Ÿå‘½å‘¨æœŸå’Œä¾èµ–å…³ç³»çš„ä¸€ä¸ªå…³é”®æœºåˆ¶ã€‚å®ƒçš„æ ¸å¿ƒç›®çš„æ˜¯ç¡®ä¿åœ¨å¼‚æ­¥æ‰§è¡Œçš„ GPU æ“ä½œä¸­ï¼Œæ•°æ®çš„ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼Œé˜²æ­¢å› æ“ä½œé‡å å¯¼è‡´çš„æ•°æ®æŸåæˆ–è®¡ç®—é”™è¯¯ã€‚\u003c/p\u003e\n\u003ch3 id=\"ä¸ºä»€ä¹ˆéœ€è¦-record_stream\"\u003eä¸ºä»€ä¹ˆéœ€è¦ record_streamï¼Ÿ\u003c/h3\u003e\n\u003cp\u003eè¦ç†è§£ \u003ccode\u003erecord_stream\u003c/code\u003eï¼Œé¦–å…ˆçœ‹ä¸€ä¸‹ CUDA æµçš„ç›¸å…³æ¦‚å¿µï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCUDA æµï¼šå¯ä»¥çœ‹ä½œæ˜¯ GPU ä¸Šçš„ä¸€ä¸ªâ€œä»»åŠ¡é˜Ÿåˆ—â€ã€‚æäº¤ç»™åŒä¸€ä¸ªæµçš„æ“ä½œä¼šæŒ‰ç…§å…ˆè¿›å…ˆå‡ºçš„é¡ºåºå¼‚æ­¥æ‰§è¡Œï¼›\u003c/li\u003e\n\u003cli\u003eå¼‚æ­¥æ‰§è¡Œï¼šCPU å°†ä»»åŠ¡æäº¤ç»™ GPU çš„æŸä¸ªæµåï¼Œä¸ä¼šç­‰å¾… GPU å®Œæˆå°±ç›´æ¥è¿”å›ï¼Œç»§ç»­æ‰§è¡Œåç»­çš„ CPU ä»£ç ï¼›\u003c/li\u003e\n\u003cli\u003eå¤šæµå¹¶è¡Œï¼šå…è®¸åˆ›å»ºå¤šä¸ªæµï¼Œè®© GPU åŒæ—¶æ‰§è¡Œæ¥è‡ªä¸åŒæµçš„ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´é«˜å±‚æ¬¡çš„å¹¶è¡Œã€‚\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ä¸ºä»€ä¹ˆä¼šå‡ºç°è®¡ç®—é”™è¯¯ä¸¾ä¸ªä¾‹å­\"\u003eä¸ºä»€ä¹ˆä¼šå‡ºç°è®¡ç®—é”™è¯¯ï¼Ÿä¸¾ä¸ªä¾‹å­ï¼š\u003c/h3\u003e\n\u003cp\u003eå‡è®¾Stream Aæ˜¯ä¸€ä¸ªé€šä¿¡æµï¼Œè´Ÿè´£é€šè¿‡all-gatheræ”¶é›†ä¸€ä¸ªTensorTï¼ŒStream B æ˜¯ä¸€ä¸ªè®¡ç®—æµï¼Œè´Ÿè´£ä½¿ç”¨Tensor T è¿›è¡Œè®¡ç®—ã€‚ç”±äºæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼ŒCPU åœ¨å‘èµ·æ‹·è´åç«‹åˆ»å°±å»æäº¤è®¡ç®—ä»»åŠ¡ï¼Œæ­¤æ—¶ Stream A çš„æ‹·è´æ“ä½œå¾ˆå¯èƒ½è¿˜æ²¡å®Œæˆã€‚å¦‚æœ Stream B å¼€å§‹è¯»å– Tï¼Œå®ƒå¯èƒ½ä¼šè¯»åˆ°ä¸å®Œæ•´æˆ–é”™è¯¯çš„æ•°æ®ï¼Œå¯¼è‡´è®¡ç®—ç»“æœé”™è¯¯ã€‚\u003c/p\u003e\n\u003cp\u003eå†çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœCPUä¸‹å‘Tensorçš„ç›¸å…³è®¡ç®—æŒ‡ä»¤åï¼Œè®¤ä¸ºè¯¥Tensorå¯ä»¥åˆ é™¤ï¼Œæ‰§è¡Œäº†delå‘½ä»¤ï¼Œä½†ç›¸å…³kernelå†…çš„è®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œä¹Ÿæœ‰å¯èƒ½å¯¼è‡´è®¡ç®—è¿‡ç¨‹ä¸­è¯»å–åˆ°é”™è¯¯çš„æ•°æ®ï¼Œä»è€Œå¯¼è‡´è®¡ç®—é”™è¯¯ã€‚\u003c/p\u003e\n\u003cp\u003eå½“ä½¿ç”¨T.record_streamæ—¶ï¼Œä¼šå¼ºåˆ¶ç­‰å¾…è¯¥æµï¼Œä¹‹åå†è¿›è¡Œå…¶ä»–æµçš„æ“ä½œï¼›å°è¯•é‡Šæ”¾æ—¶ï¼Œä¹Ÿä¼šç­‰å¾…è¿™ä¸ªæµï¼Œæ­¤æ—¶ç”³è¯·çš„å†…å­˜ä¸æ˜¯çœŸæ­£çš„è¢«é‡Šæ”¾äº†ï¼Œé˜²æ­¢å‡ºç°æ•°æ®è¸©è¸æˆ–è¯»å–è„æ•°æ®ã€‚\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erecord_stream\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003es1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"record_streamåŸç†\"\u003erecord_streamåŸç†\u003c/h2\u003e\n\u003cp\u003eæŸ¥çœ‹record_streamæºç ï¼Œå‘ç°ä¸Caching Allocatorç›¸å…³\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003erecord_stream_cuda\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTensor\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"n\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ec10\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eStream\u003c/span\u003e \u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"nc\"\u003ec10\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eStreamData3\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epack3\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"n\"\u003ec10\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003ecuda\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eCUDACachingAllocator\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003erecordStream\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estorage\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003edata_ptr\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e \u003cspan class=\"n\"\u003eat\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003ecuda\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eCUDAStream\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eunpack3\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estream_id\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edevice_index\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edevice_type\u003c/span\u003e\u003cspan class=\"p\"\u003e));\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003erecord_stream\u003c/code\u003eä¼šåœ¨Tensorå¯¹åº”çš„Blockä¸­æ’å…¥ä¸€ä¸ªeventï¼Œå¹¶æ ¹æ®ä½¿ç”¨çš„æµæ•°å¢åŠ å¼•ç”¨è®¡æ•°ï¼Œå³ä½¿ CPU ç«¯å¼•ç”¨å½’é›¶ï¼ŒCaching Allocatorä¹Ÿä¼šæ‹¦æˆªé‡Šæ”¾è¯·æ±‚ï¼Œå°†è¯¥æ˜¾å­˜å—æ ‡è®°ä¸ºâ€œæµå ç”¨â€çŠ¶æ€å¹¶ç§»å…¥å¾…å›æ”¶é˜Ÿåˆ—ï¼ŒåŒæ—¶æŒç»­è½®è¯¢è¯¥æµçš„æ‰§è¡ŒçŠ¶æ€ã€‚åªæœ‰å½“ç¡®è®¤æµä¸­æ‰€æœ‰ä»»åŠ¡å®Œå…¨ç»“æŸæ—¶ï¼ŒCaching Allocatoræ‰ä¼šçœŸæ­£å°†æ˜¾å­˜é‡Šæ”¾ï¼ˆä¹Ÿä¸æ˜¯çœŸæ­£çš„é‡Šæ”¾ï¼ŒæŒ‡å¯ä»¥è¢«å¤ç”¨ï¼‰ã€‚\u003c/p\u003e\n\u003cp\u003eæŸ¥çœ‹Caching Allocatorä¸­çš„æºç ï¼š\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003erecordStream\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eBlock\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecuda\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eCUDAStream\u003c/span\u003e \u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003elock_guard\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003erecursive_mutex\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003elock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emutex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003estream_uses\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// è®°å½•ä½¿ç”¨è¿™å—blockçš„æµ\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eC10_UNLIKELY\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ecaptures_underway\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eempty\u003c/span\u003e\u003cspan class=\"p\"\u003e()))\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003eblock_to_cudagraph_stream_uses\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eblock\u003c/span\u003e\u003cspan class=\"p\"\u003e].\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eé€šè¿‡\u003ccode\u003eblock-\u0026gt;stream_uses.insert(stream);\u003c/code\u003e è®°å½•ä¸‹æ‰€æœ‰ä½¿ç”¨è¿™å—blockçš„æµã€‚å•å•åªæ˜¯è®°å½•å¹¶ä¸èƒ½å¼ºåˆ¶åŒæ­¥ï¼Œæ‰¾ä¸€ä¸‹æ˜¯å“ªé‡Œæ’å…¥è¿™ä¸ªeventçš„ï¼Œå‘ç°Caching Allocatorä¸­é€šè¿‡\u003ccode\u003einsert_events\u003c/code\u003e å‡½æ•°å¯¹blockæ‰€åœ¨çš„æµæ’å…¥eventå¹¶å®ç°å¼•ç”¨è®¡æ•°ï¼š\u003c/p\u003e","title":"PyTorch record_stream æºç è§£è¯»"},{"content":"ç®€ä»‹ PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚\nDTensoråŸºæœ¬ç”¨æ³• åŸºæœ¬å±æ€§ DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial\nReplicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›\nShardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›\nPartialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€\nä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š\ntorchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š\nShard(dim) -\u0026gt; Replicate(): all_gather\nShard(src_dim) -\u0026gt; Shard(dst_dim): all_to_all\nReplicate() -\u0026gt; Shard(dim): local chunking (i.e. torch.chunk)\nPartial() -\u0026gt; Replicate(): all_reduce\nPartial() -\u0026gt; Shard(dim): reduce_scatter\nAPIï¼š ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š\ndistribute_tensor() æ¥å£ local_tensor = torch.randn(8, 16, device=f\u0026#34;cuda:{rank}\u0026#34;) dtensor_shard = distribute_tensor( local_tensor, device_mesh=mesh, placements=[Shard(0)], ) from local tensor local_tensor = torch.tensor([[1, 2], [3, 4]]) dtensor_shard = DTensor( local_tensor, device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, list(range(world_size))), placements=[dist.Placement(\u0026#34;shard\u0026#34;, dim=0)] # æ²¿ç¬¬0ç»´åˆ†ç‰‡ ) æˆ–è€… local_tensor = torch.tensor([1, 2, 3]) dtensor = local_tensor.to_dtensor( device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, [0,1]), placements=[dist.Placement(\u0026#34;replicate\u0026#34;)] ) å·¥å‚å‡½æ•° dtensor_ones = DTensor.ones( (2, 4), device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, list(range(world_size))), placements=[dist.Placement(\u0026#34;shard\u0026#34;, dim=1)] ) åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚\nåŸç† ä¸€ä¸ªæºå¸¦DTensorçš„ç®—å­è°ƒç”¨æ—¶ï¼Œä¼šå…ˆè¢«DTensorä¸­å®šä¹‰çš„__torch_dispatch__ç»™Hookä½ï¼Œä¼˜å…ˆæ‰§è¡ŒDTensorå®šä¹‰çš„Dispatchå†…çš„é€»è¾‘ï¼›\ndef __torch_dispatch__(cls, func, types, args=(), kwargs=None): # type: ignore[override] return DTensor._op_dispatcher.dispatch( func, args, kwargs or {}, ) è¿›å…¥Dispatheråï¼Œé€šè¿‡è¿™è¡Œä»£ç è§£åŒ…å‡ºop_infoï¼ŒåŒ…å«device_meshã€_local_tensorã€_specç­‰åˆ†å¸ƒå¼ä¿¡æ¯\nop_info = self.unwrap_to_op_info(op_call, args, kwargs) åŸºäºåˆ†å¸ƒå¼ä¿¡æ¯ï¼Œæ¨å¯¼shardè§„åˆ™ï¼Œå³ä»¥å½“å‰è¾“å…¥ï¼Œè¾“å‡ºçš„placementä¼šæ˜¯ä»€ä¹ˆï¼Œè¿™ä¸ªæ¨å¯¼æœ‰ä¸€å®šçš„ä¼˜å…ˆçº§ï¼Œæºç ä¸­æ˜¯è¿™æ ·æè¿°çš„ï¼š\nMain dispatching logic. Follows precedence order: (1) custom_op_handler (2) registered sharding strategy, then rule (3) composite implicit autograd decomposition é™¤äº†è‡ªå®šä¹‰handlerï¼Œä¼˜å…ˆçº§æ˜¯strategyç„¶åæ˜¯ruleï¼Œå–å†³äºç®—å­çš„åˆ†å¸ƒå¼æ³¨å†Œæ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å›é€€åˆ°compositeå®ç°ã€‚\npytorché€šè¿‡@register_op_strategyæ³¨å†Œç”Ÿæˆçš„æ˜¯strategyï¼Œä¼šè®¡ç®—é€šä¿¡costï¼Œä¼˜å…ˆçº§æ›´é«˜ï¼Œé€šè¿‡@register_prop_ruleæ³¨å†Œçš„æ˜¯ruleï¼Œä¼˜å…ˆçº§è¾ƒä½ã€‚\nåŸºäºåˆ†å¸ƒå¼ä¿¡æ¯å’Œplacementè¿›è¡Œè®¡ç®—ï¼Œå¹¶åœ¨è®¡ç®—å‰å®Œæˆç›¸åº”çš„é€šä¿¡æ“ä½œï¼Œå¦‚all-gatherç­‰ã€‚åœ¨å®Œæˆè®¡ç®—åï¼Œä¼šæ ¹æ®æ˜¯å¦ä»…ä¿ç•™local_tensorå†æ¬¡è¿›è¡Œç›¸åº”çš„é€šä¿¡æ“ä½œã€‚\né¢˜å¤–è¯ ä¸ºä»€ä¹ˆ__torch_dispatch__å¯ä»¥å®ç°hookï¼Œåœ¨DTensoræ‰§è¡Œå‰é¡ºåˆ©æ¥ç®¡ï¼Ÿ pytorchä¼šcheckè¾“å…¥çš„å¯¹è±¡ä¸­æ˜¯å¦æœ‰__torch_dispatch__è¿™ä¸ªattr\nstatic bool check_has_torch_dispatch(PyObject* obj) { PyTypeObject* tp = Py_TYPE(obj); if (THPVariable_CheckTypeExact(tp)) { return false; } py::object attr = PyObject_FastGetAttrString(obj, \u0026#34;__torch_dispatch__\u0026#34;); return ( attr.ptr() != nullptr \u0026amp;\u0026amp; attr.ptr() != torch::disabled_torch_dispatch_impl()); } å¦‚æœæœ‰å°±ä¼šå°è¯•å»ä¸ºå…¶å¢åŠ pythonè¿™ä¸ªdispatch key\nif (has_torch_dispatch_if_known.has_value() ? *has_torch_dispatch_if_known : check_has_torch_dispatch(obj)) { var.unsafeGetTensorImpl()-\u0026gt;set_python_dispatch(true); è€Œpython keyçš„ä¼˜å…ˆçº§æ¯”cudaç­‰dence keyçš„ä¼˜å…ˆçº§æ›´é«˜ï¼Œä¼šå…ˆè¢«è°ƒç”¨ï¼Œå› æ­¤å¯ä»¥å®ç°è¿™ä¸ªhook æŒ–ä¸ªå‘ğŸ•³ï¸ï¼Œåé¢æœ‰ç©ºæ¢³ç†ä¸‹Dispatchä¼˜å…ˆçº§çš„å†…å®¹\nReference https://docs.pytorch.org/docs/stable/distributed.tensor.html\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","summary":"\u003ch2 id=\"ç®€ä»‹\"\u003eç®€ä»‹\u003c/h2\u003e\n\u003cp\u003ePyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚\u003c/p\u003e\n\u003ch2 id=\"dtensoråŸºæœ¬ç”¨æ³•\"\u003eDTensoråŸºæœ¬ç”¨æ³•\u003c/h2\u003e\n\u003ch3 id=\"åŸºæœ¬å±æ€§\"\u003eåŸºæœ¬å±æ€§\u003c/h3\u003e\n\u003cp\u003eDTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eReplicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePartialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Image Alt Text\" loading=\"lazy\" src=\"/DTensor/DTensor_01.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003etorchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eShard(dim) -\u0026gt; Replicate(): all_gather\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShard(src_dim) -\u0026gt; Shard(dst_dim): all_to_all\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReplicate() -\u0026gt; Shard(dim): local chunking (i.e. torch.chunk)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePartial() -\u0026gt; Replicate(): all_reduce\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePartial() -\u0026gt; Shard(dim): reduce_scatter\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"api\"\u003eAPIï¼š\u003c/h3\u003e\n\u003cp\u003eä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003edistribute_tensor()\u003c/code\u003e æ¥å£\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erandn\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e16\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edevice\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;cuda:\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003erank\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edtensor_shard\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edistribute_tensor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edevice_mesh\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003emesh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eplacements\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eShard\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003e\u003ccode\u003efrom local tensor\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etensor\u003c/span\u003e\u003cspan class=\"p\"\u003e([[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e]])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edtensor_shard\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edevice_mesh\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eDeviceMesh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eworld_size\u003c/span\u003e\u003cspan class=\"p\"\u003e))),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eplacements\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ePlacement\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;shard\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# æ²¿ç¬¬0ç»´åˆ†ç‰‡\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eæˆ–è€…\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etensor\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edtensor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elocal_tensor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto_dtensor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edevice_mesh\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eDeviceMesh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eplacements\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ePlacement\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;replicate\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eå·¥å‚å‡½æ•°\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edtensor_ones\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDTensor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eones\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edevice_mesh\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eDeviceMesh\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eworld_size\u003c/span\u003e\u003cspan class=\"p\"\u003e))),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eplacements\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edist\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ePlacement\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;shard\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eåˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚\u003c/p\u003e","title":"PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»"},{"content":"æ¦‚è¿° FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\nFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹ å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\nComparing with FSDP1, FSDP2 has following advantages: Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow. Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc) Mixing frozen and non-frozen parameters can in the same communication group without using extra memory. ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\nAPIæ¥å£ FSDP2çš„æ¥å£ä¸æ˜¯å‘åå…¼å®¹çš„ï¼Œç›¸åº”çš„æ¥å£åç§°ä¹Ÿä»FSDPå˜æˆäº†fully_shardï¼Œä»¥ä¸€ä¸ªmodelä¸ºä¾‹ï¼Œä¸å†åƒFSDP1ä¸€æ ·å¯ä»¥ç›´æ¥é€šè¿‡ä¸€ä¸ªæ¥å£åŒ…è£¹æ•´ä¸ªæ¨¡å‹ï¼Œè€Œè½¬å˜æˆé€å±‚è¿›è¡Œæ¨¡å‹çš„åŒ…è£¹ã€‚\nfor module in model.modules(); if isinstance(module, TransformerBlock): fully_shard(module, **fsdp_kwargs) fully_shard(model, **fsdp_kwargs) åŸå› æ˜¯åœ¨FSDP2ä¸­ï¼Œunitå˜æˆäº†æœ€å°çš„å¤„ç†å•ä½ï¼Œunitå¯ä»¥æ˜¯ä¸€ä¸ªlayerï¼Œä¸€ä¸ªnnModuleï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ•´ä¸ªæ¨¡å‹ï¼Œåˆ†å—èƒ½æ›´åˆç†åœ°å»ç®¡ç†æ¯ä¸ªåˆ†å—å‚æ•°çš„ç”Ÿå‘½å‘¨æœŸã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªé¢œè‰²ä»£è¡¨ä¸€ä¸ªunitçš„å‚æ•°ã€‚\nç›¸æ¯”ä¹‹ä¸‹ï¼ŒFSDP1çš„å¤„ç†æ–¹å¼ä¸ºï¼š\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP # defnition of model ...... model = FSDP(model) åˆ†å¸ƒå¼ä¸DTensor Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.\nDTensoræ˜¯pytorchçš„ä¸€ç§åˆ†å¸ƒå¼æ•°æ®ç»“æ„ï¼Œæ˜¯torch.Tensorçš„å­ç±»ã€‚DTensoré€šè¿‡Devicemeshæè¿°äº†å¼ é‡åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä»¥ä»€ä¹ˆåˆ’åˆ†æ–¹å¼åˆ†å¸ƒã€‚Placementå‚æ•°æè¿°äº†æ¯ä¸ªå‚æ•°ä¸Šçš„å‚¨å­˜æ–¹å¼ã€‚FSDP2ä½¿ç”¨äº†DTensorä½œä¸ºå‚æ•°çš„åˆ†å¸ƒå¼è¡¨è¾¾ï¼Œæ¯ä¸ªrankåªæŒæœ‰å®Œæ•´Tensorçš„å±€éƒ¨shardã€‚ DTensorç›¸å…³ä»‹ç»è§ä¸“æ æ–‡ç«  PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯» DTensorå–ä»£çš„æ˜¯FSDP1 ä½¿ç”¨çš„FlatParameterå‚æ•°åˆ‡å‰²æ–¹å¼ï¼Œä¼šæŠŠæ‰€æœ‰è¦åˆ†å—çš„å‚æ•°æ‹¼æ¥æˆä¸€ä¸ªå·¨å¤§çš„Tensorï¼Œæ ¹æ®è¦åˆ†å—çš„å¤§å°å¯¹å‚æ•°è¿›è¡Œpaddingï¼Œä¹‹åæŒ‰å¤§å°åˆ†é…ç»™å„ä¸ªrankï¼ŒåŒæ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šéšå‚æ•°åˆ‡åˆ†è€Œåˆ‡åˆ†ã€‚\nTo shard the FlatParameter, FSDP divides it into equal-sized chunks, where the number of chunks equals the sharding factor, and assigns one chunk per rank.\nFSDP1 FlatParam FSDPçš„FlatParamåˆ†å—å¯¼è‡´æŸäº›Tensorå¯èƒ½ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒçš„rankï¼Œè€ŒæŸäº›Tensorå®Œæ•´çš„ä¿ç•™åœ¨æŸäº›rankä¸­ã€‚åŸºäºDTensorçš„åˆ‡åˆ†æ–¹å¼é€šå¸¸æ˜¯æ²¿Dim0åˆ‡åˆ†ï¼ŒFSDP2é‡‡ç”¨è¿™ç§åˆ‡åˆ†æ–¹å¼ä¿è¯æ¯ä¸ªrankéƒ½è·å¾—æ¯ä¸ªparamçš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹éƒ¨ä»½Tensorè¿›è¡Œpaddingã€‚\nç»è¿‡FSDP2çš„init_device_meshï¼Œç”Ÿæˆåˆ†å¸ƒå¼çš„åˆ†å—ä¿¡æ¯ã€‚DTensoråœ¨è¿›è¡Œshardingæ“ä½œæ—¶ä¼šä¿ç•™è¿™äº›åˆ†å—ä¿¡æ¯ï¼ŒFSDP2ä¸­ï¼Œåœ¨FSDPParamä¸­è¿›è¡Œç»´æŠ¤ã€‚åœ¨è¿›è¡Œé€šä¿¡æ“ä½œï¼Œå¦‚All-reduceæˆ–reduce-scatteræ—¶ï¼ŒDTensorä¸­å‚¨å­˜çš„åˆ†å—ä¿¡æ¯ä¼šè¢«è¯»å–ï¼Œä»¥å®Œæˆå‚æ•°çš„shardå’Œunshardæ“ä½œã€‚ æ‰“å°fsdpå‰åçš„å‚æ•°ä¿¡æ¯ï¼Œå¯ä»¥å¾—çŸ¥shardingå‰gradå±äº\u0026lt;class \u0026rsquo;torch.Tensor\u0026rsquo;\u0026gt;ï¼Œè€Œshardingåè½¬å˜æˆäº†\u0026lt;class \u0026rsquo;torch.distributed.tensor.Tensor\u0026rsquo;\u0026gt; Streamã€Eventæ§åˆ¶\nImproving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. é€šè¿‡é¿å…recordStreamï¼Œæ— éœ€CPUåŒæ­¥é™ä½äº†GPUæ˜¾å­˜å ç”¨\nrecordStreamæ˜¯torchç”¨æ¥ç®¡ç†CUDAå¼ é‡ç”Ÿå‘½å‘¨æœŸçš„æ–¹æ³•ï¼Œå› ä¸ºGPUçš„æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæ¯”å¦‚ä¸‹å‘ä¸€ä¸ªè®¡ç®—æˆ–è€…é€šä¿¡ç®—å­æ—¶ï¼ŒCPUæ“ä½œä¼šç«‹å³è¿”å›ï¼Œè€Œå®é™…çš„æ•°æ®è®¡ç®—æˆ–è€…é€šä¿¡ä»ç„¶åœ¨GPUä¸Šè¿›è¡Œï¼Œé‚£ä¹ˆæ­¤æ—¶å¦‚æœå°è¯•è¯»å†™è¿™å—æ­£åœ¨æ“ä½œçš„bufferï¼Œå°±æœ‰å¯èƒ½å¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚\nCUDA tensorçš„ç”Ÿå‘½å‘¨æœŸæ˜¯ç”±GPUä¸Šå†…å­˜çš„åˆ†é…åˆé‡Šæ”¾ç®¡ç†çš„ï¼ŒrecordStreamä¼šè®°å½•tensorè¢«å“ªäº›æµç»‘å®šï¼Œå¦‚æœè¦é‡Šæ”¾ä¸€ä¸ªtensorï¼Œå°±å¿…é¡»ä¿è¯ä¾èµ–è¯¥tensorçš„æµå…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼Œè€Œä¸æ˜¯ä»…ä¸æ“ä½œå®ƒçš„æµç›¸å…³ã€‚\nFSDP1çš„å±€é™æ€§ï¼š FSDP1çš„prefetchå®ç°äº†åœ¨è®¡ç®—å½“å‰å±‚å‚æ•°æ—¶ï¼Œé¢„å–ä¸‹ä¸€å±‚å‚æ•°ï¼Œä»¥å®ç°é€šä¿¡/è®¡ç®—çš„overlapï¼Œä½†æ˜¯é€šå¸¸è®¡ç®—æµå’Œé€šä¿¡æµæ˜¯ä¸¤æ¡streamï¼Œä¸”ä¸åŒstreamé—´æ˜¯ç›¸äº’å¹¶è¡Œçš„ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§æƒ…å†µï¼šå³å¦‚æœæŸæ¬¡è®¡ç®—ç”±äºè€—æ—¶è¾ƒä¹…ï¼Œä¹…åˆ°è¿ç»­ä¸‹å‘çš„å‡ ä¸ªall-gatheræ“ä½œå…¨éƒ¨å®Œæˆäº†ï¼Œè®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œæ­¤æ—¶æ˜¾å­˜å°±ä¼šè¢«é€šä¿¡åçš„å‚æ•°ä¸æ–­å æ®ï¼Œç”šè‡³ä¼šå‡ºç°oomã€‚\nFSDP1å¯¹è¿™ä¸ªé—®é¢˜çš„è§£æ³•ä¹Ÿååˆ†ç²—æš´ï¼Œå³ï¼Œè®¾ç½®äº†ä¸€ä¸ªrate_limiter,å³é€Ÿç‡æ§åˆ¶å™¨ï¼Œåœ¨æ¯ä¸ªTransformerBlockåé˜»å¡CPUï¼Œrate_limiterä¸­çš„é™åˆ¶é»˜è®¤ä¸º2ï¼Œä¿è¯åªæœ‰å½“ä¸Šä¸€å±‚ç»“æŸæ—¶ï¼Œæ‰ä¼šå¯¹ä¸‹ä¸€å±‚è¿›è¡Œall-gatheræ“ä½œã€‚ æºç ï¼š\nclass _FreeEventQueue: \u0026#34;\u0026#34;\u0026#34; This tracks all pending frees corresponding to inflight all-gathers. The queueing pattern is iterative enqueues with a single dequeue per iteration once the limit ``_max_num_inflight_all_gathers`` is reached. \u0026#34;\u0026#34;\u0026#34; def __init__(self) -\u0026gt; None: self._queue: collections.deque[torch.Event] = collections.deque() self._max_num_inflight_all_gathers = 2 # empirically chosen å…³äºFSDP1è¿˜æœ‰ä¸€ä¸ªè®¨è®ºï¼Œé€šè¿‡recordStreamæ§åˆ¶ï¼Œå‡è®¾å­˜åœ¨ä¸¤ä¸ªæµstreamAå’ŒstreamBï¼Œå½“streamAæŒæœ‰æŸä¸ªTensoræ—¶ï¼Œç›´åˆ°è¿™æ¡steamA å®Œå…¨ç»“æŸæ—¶ï¼Œç›¸åº”çš„Tensoræ‰ä¼šè¢«é‡Šæ”¾ï¼Œå› æ­¤ï¼Œå³ä½¿è¿™ä¸ªTensorç”Ÿå‘½å‘¨æœŸç†è®ºå·²ç»ç»“æŸäº†ï¼Œä½†streamAæ²¡æœ‰é€€å‡ºï¼Œæ­¤æ—¶å¦‚æœåœ¨streamBä¸Šå°è¯•mallocä¸€å—å†…å­˜ï¼ŒcachingAllocatorä¼šåˆ¤æ–­æ­¤æ—¶æ²¡æœ‰ç©ºé—²å†…å­˜ï¼Œè€Œå»é‡æ–°åˆ†é…ç©ºé—´ã€‚è¿™å°±å¯¼è‡´äº†1. å†…å­˜å­˜åœ¨å³°å€¼ 2. ç”±äºåå¤mallocï¼Œæ€§èƒ½å˜å·®ã€‚ç¤¾åŒºè®¨è®ºçš„å†…å­˜å³°å€¼å¦‚å›¾ï¼š\nåŸºæœ¬åŸç†å¦‚å›¾æ‰€ç¤ºï¼šå½“åœ¨å°è¯•delä¸€ä¸ªæ­£åœ¨ä½¿ç”¨çš„tensoråï¼Œå¦‚æœä½¿ç”¨è¿‡è¯¥tensorçš„æµè¿˜æœªç»“æŸï¼Œé‚£ä¹ˆæ­¤æ—¶çš„mallocæ“ä½œæ— æ³•å¤ç”¨å†…å­˜ï¼Œå¦‚å›¾ä¸Šçš„malloc Cï¼Œå¦‚æœåœ¨æµç»“æŸä¹‹åï¼Œå¦‚malloc Eï¼Œæ‰å¯ä»¥é¡ºåˆ©å¤ç”¨Açš„å†…å­˜ã€‚\næ”¹è¿›çš„ç‚¹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒæ­¥ä¸è¦å†å‘ç”Ÿåœ¨CPUä¸Šäº†ï¼Œè€Œé€šè¿‡åœ¨streamåæ’å…¥æ ‡è®°æ¥è®°å½•æ“ä½œæ˜¯å¦å®Œæˆï¼Œé€šä¿¡æµä¼šç­‰å¾…ä¾èµ–çš„è®¡ç®—æµï¼Œè€Œè®¡ç®—æµä¹Ÿä¼šç­‰å¾…ä¾èµ–çš„é€šä¿¡æµã€‚ç†è§£ä¸€ä¸‹è¿™å¼ å›¾ï¼Œä»¥iä¸ºä¾‹ï¼Œè®¡ç®—layer iæ—¶ï¼Œé¢„å–äº†i+1ï¼Œi+1çš„é€šä¿¡ä¼šäº§ç”Ÿä¸€ä¸ªevent iï¼Œè®¡ç®—æµä¼šå»waitè¿™ä¸ªeventï¼Œç›´åˆ°ç»“æŸåæ‰ä¼šå»è¿›è¡Œlayer i+1çš„è®¡ç®—ã€‚\nç°åœ¨CPUä¸éœ€è¦å†è¿›è¡Œç­‰å¾…ï¼Œä¹Ÿä¸ä¼šè¢«rate_limiteré˜»å¡ï¼Œç›¸åº”çš„é˜»å¡è¢«è½¬ç§»åˆ°äº†å¯¹åº”çš„æµä¸Šã€‚\nReference https://github.com/pytorch/pytorch/issues/114299\nhttps://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486\nhttps://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html\nhttps://arxiv.org/pdf/2304.11277\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/","summary":"\u003ch2 id=\"æ¦‚è¿°\"\u003eæ¦‚è¿°\u003c/h2\u003e\n\u003cp\u003eFSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\u003c/p\u003e\n\u003ch2 id=\"fsdp2æ¢³ç†åŠæ›´æ–°ç‚¹\"\u003eFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹\u003c/h2\u003e\n\u003cp\u003eå®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eComparing with FSDP1, FSDP2 has following advantages:\nRepresenting sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.\nImproving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.\nOffering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)\nMixing frozen and non-frozen parameters can in the same communication group without using extra memory.\nä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\u003c/p\u003e","title":"PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§"},{"content":"Add unified memory APIs for torch.accelerator #152932 https://github.com/pytorch/pytorch/pull/152932\nä¸ºtorch.acceleratorå¢åŠ äº†å¦‚ä¸‹APIï¼š\nempty_cache max_memory_allocated max_memory_reserved memory_allocated memory_reserved memory_stats reset_accumulated_memory_stats reset_peak_memory_stats è¿™ä¸ªprè¿›ä¸€æ­¥è¯´æ˜äº†PyTorchæ­£åœ¨å»CUDAåŒ–ï¼Œä»è€Œè½¬å˜ä¸ºä¸€ç³»åˆ—åŠ é€Ÿå™¨æä¾›è®¡ç®—æ¡†æ¶ï¼Œå³ä½¿ä»–ä»¬çš„githubä¸»é¡µä»ç„¶å†™ç€ï¼š\nTensors and Dynamic neural networks in Python with strong GPU acceleration\nè¿™äº›ç”¨æ³•å¾ˆé•¿ä¸€æ®µæ—¶é—´æ˜¯ä¸ºcudaæä¾›çš„ï¼Œè°ƒç”¨æ–¹å¼ä¸»è¦ä¸ºtorch.cuda.empty_cacheç­‰ï¼Œæ¥å£ä¸»è¦ä½œç”¨æ˜¯æŸ¥çœ‹deviceä¸Šçš„æ˜¾å­˜å ç”¨å¦‚memory_allocatedå’Œmemory_reservedï¼Œä»¥åŠæ¸…ç†å’Œé‡ç½®ç›¸å…³çŠ¶æ€ã€‚ ä¸€èµ·æ¥çœ‹ä¸‹ç›¸å…³å‡½æ•°çš„è°ƒç”¨æµç¨‹ï¼Œä»¥cudaä¸ºä¾‹ï¼Œä»æºç åˆ†æè¿™ä¸ªæ¥å£æ˜¯æ€ä¹ˆä¸€æ­¥ä¸€æ­¥è·å–åº•å±‚çš„çŠ¶æ€çš„ã€‚é¦–å…ˆæ‰€æœ‰memoryç›¸å…³çš„æ•°æ®éƒ½æ˜¯ä»memory_statsè¿™ä¸ªå­—å…¸å†…æå–å¯¹äºçš„value\ndef memory_reserved(device_index: _device_t = None, /) -\u0026gt; int: r\u0026#34;\u0026#34;\u0026#34;Return the current :ref:`accelerator\u0026lt;accelerators\u0026gt;` device memory managed by the caching allocator in bytes for a given device index. Args: device_index (:class:`torch.device`, str, int, optional): the index of the device to target. If not given, use :func:`torch.accelerator.current_device_index` by default. If a :class:`torch.device` or str is provided, its type must match the current :ref:`accelerator\u0026lt;accelerators\u0026gt;` device type. \u0026#34;\u0026#34;\u0026#34; return memory_stats(device_index).get(\u0026#34;reserved_bytes.all.current\u0026#34;, 0) è€Œmemory_stasé€šè¿‡åº•å±‚_accelerator_getDeviceStatsæ¥å£è·å–ç›¸å…³deviceçš„çŠ¶æ€\nstats = torch._C._accelerator_getDeviceStats(device_index) torch._C._accelerator_getDeviceStatsæ˜¯ä¸€ä¸ªpybindç»‘å®šçš„Cæ¥å£å‡½æ•° at::accelerator::getDeviceStats(device_index); ç»§ç»­çœ‹è°ƒç”¨æ ˆ\nTORCH_API inline at::CachingDeviceAllocator::DeviceStats getDeviceStats( c10::DeviceIndex device_index) { const auto device_type = getAccelerator(true).value(); return at::getDeviceAllocator(device_type)-\u0026gt;getDeviceStats(device_index); } ä»¥cudaä¸ºä¾‹\ninline c10::CachingDeviceAllocator::DeviceStats getDeviceStats( c10::DeviceIndex device) { return get()-\u0026gt;getDeviceStats(device); } å…¶ä¸­getæ–¹æ³•æ¥è‡ªallocatorï¼Œallocatoræ˜¯CUDA Cacing Allocatorä¸­çš„å•ä¾‹å®ç°ï¼Œæ‰€æœ‰æ–¹æ³•éƒ½éœ€è¦é€šè¿‡è¯¥æ¥å£å®ç°\ninline CUDAAllocator* get() { return allocator.load(); } cudaçš„getDeviceStatsé€šè¿‡ä¸‹é¢ä»£ç å®ç°\nDeviceStats getDeviceStats(c10::DeviceIndex device) override { assertValidDevice(device); return device_allocator[device]-\u0026gt;getStats(); } æ€»ä¹‹ï¼Œç°åœ¨ï¼Œæ‰€æœ‰åŠ é€Ÿå™¨éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„æ¥å£è¿”å›ä»PyTorchç”³è¯·çš„å†…å­˜ï¼Œä»¥åŠå¯¹å…¶çŠ¶æ€è¿›è¡Œç®¡ç†ï¼Œå¤§ä¸€ç»Ÿè¿›è¡Œä¸­â€¦â€¦\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/","summary":"\u003ch2 id=\"add-unified-memory-apis-for-torchaccelerator-152932\"\u003eAdd unified memory APIs for torch.accelerator #152932\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/pytorch/pytorch/pull/152932\"\u003ehttps://github.com/pytorch/pytorch/pull/152932\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eä¸ºtorch.acceleratorå¢åŠ äº†å¦‚ä¸‹APIï¼š\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eempty_cache\u003c/li\u003e\n\u003cli\u003emax_memory_allocated\u003c/li\u003e\n\u003cli\u003emax_memory_reserved\u003c/li\u003e\n\u003cli\u003ememory_allocated\u003c/li\u003e\n\u003cli\u003ememory_reserved\u003c/li\u003e\n\u003cli\u003ememory_stats\u003c/li\u003e\n\u003cli\u003ereset_accumulated_memory_stats\u003c/li\u003e\n\u003cli\u003ereset_peak_memory_stats\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eè¿™ä¸ªprè¿›ä¸€æ­¥è¯´æ˜äº†PyTorchæ­£åœ¨å»CUDAåŒ–ï¼Œä»è€Œè½¬å˜ä¸ºä¸€ç³»åˆ—åŠ é€Ÿå™¨æä¾›è®¡ç®—æ¡†æ¶ï¼Œå³ä½¿ä»–ä»¬çš„githubä¸»é¡µä»ç„¶å†™ç€ï¼š\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTensors and Dynamic neural networks in Python with strong GPU acceleration\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eè¿™äº›ç”¨æ³•å¾ˆé•¿ä¸€æ®µæ—¶é—´æ˜¯ä¸ºcudaæä¾›çš„ï¼Œè°ƒç”¨æ–¹å¼ä¸»è¦ä¸º\u003ccode\u003etorch.cuda.empty_cache\u003c/code\u003eç­‰ï¼Œæ¥å£ä¸»è¦ä½œç”¨æ˜¯æŸ¥çœ‹deviceä¸Šçš„æ˜¾å­˜å ç”¨å¦‚\u003ccode\u003ememory_allocated\u003c/code\u003eå’Œ\u003ccode\u003ememory_reserved\u003c/code\u003eï¼Œä»¥åŠæ¸…ç†å’Œé‡ç½®ç›¸å…³çŠ¶æ€ã€‚\nä¸€èµ·æ¥çœ‹ä¸‹ç›¸å…³å‡½æ•°çš„è°ƒç”¨æµç¨‹ï¼Œä»¥cudaä¸ºä¾‹ï¼Œä»æºç åˆ†æè¿™ä¸ªæ¥å£æ˜¯æ€ä¹ˆä¸€æ­¥ä¸€æ­¥è·å–åº•å±‚çš„çŠ¶æ€çš„ã€‚é¦–å…ˆæ‰€æœ‰memoryç›¸å…³çš„æ•°æ®éƒ½æ˜¯ä»\u003ccode\u003ememory_stats\u003c/code\u003eè¿™ä¸ªå­—å…¸å†…æå–å¯¹äºçš„value\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ememory_reserved\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edevice_index\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003e_device_t\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Return the current :ref:`accelerator\u0026lt;accelerators\u0026gt;` device memory managed by the caching allocator\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    in bytes for a given device index.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Args:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        device_index (:class:`torch.device`, str, int, optional): the index of the device to target.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            If not given, use :func:`torch.accelerator.current_device_index` by default.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            If a :class:`torch.device` or str is provided, its type must match the current\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            :ref:`accelerator\u0026lt;accelerators\u0026gt;` device type.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ememory_stats\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edevice_index\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;reserved_bytes.all.current\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eè€Œ\u003ccode\u003ememory_stas\u003c/code\u003eé€šè¿‡åº•å±‚\u003ccode\u003e_accelerator_getDeviceStats\u003c/code\u003eæ¥å£è·å–ç›¸å…³deviceçš„çŠ¶æ€\u003c/p\u003e","title":"PyTorch pr(1)ä¹‹torch.accelerator"},{"content":"å…³äºæˆ‘ æˆ‘æƒ³ä»æŸä¸€æ—¶åˆ»èµ·ï¼Œæœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥è®°å½•å‘¨é­çš„å˜åŒ–ï¼Œä¸è®ºæ˜¯å‘ç”Ÿåœ¨è‡ªå·±èº«ä¸Šçš„ï¼Œè¿˜æ˜¯å¯¹è¿™ä¸ªä¸–ç•Œçš„æ„ŸçŸ¥ã€‚\nè¿™ä¸ªblogä¼šåˆ†äº«æŠ€æœ¯ã€ç”Ÿæ´»ã€æƒ³æ³•ä»¥åŠæˆ‘çš„æ€è€ƒï¼Œæˆ‘ä¼šå°½é‡æŠŠå®ƒä»¬å½’ç±»ï¼Œä¸å®šæœŸä¼šåŒæ­¥ä¸€äº›åˆ°ç¤¾äº¤åª’ä½“ä¸Šã€‚\nå»ºè®®ã€åˆ†äº«ã€äº¤æµæ¬¢è¿è¯„è®ºæˆ–é€šè¿‡ç¤¾åª’è”ç³»æˆ‘\nğŸ“Shanghai\næˆ‘çš„å…´è¶£ ","permalink":"https://zhouyeyu.github.io/about/","summary":"about YuZhouye","title":"About"}]