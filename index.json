[{"content":"To be doneğŸ˜ ","permalink":"https://zhouyeyu.github.io/posts/test/","summary":"\u003ch1 id=\"to-be-done\"\u003eTo be doneğŸ˜\u003c/h1\u003e","title":"Hugoéƒ¨ç½²github Pages"},{"content":"æ¦‚è¿° FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\nFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹ å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\nComparing with FSDP1, FSDP2 has following advantages: Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow. Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc) Mixing frozen and non-frozen parameters can in the same communication group without using extra memory. ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\nAPIæ¥å£ FSDP2çš„æ¥å£ä¸æ˜¯å‘åå…¼å®¹çš„ï¼Œç›¸åº”çš„æ¥å£åç§°ä¹Ÿä»FSDPå˜æˆäº†fully_shardï¼Œä»¥ä¸€ä¸ªmodelä¸ºä¾‹ï¼Œä¸å†åƒFSDP1ä¸€æ ·å¯ä»¥ç›´æ¥é€šè¿‡ä¸€ä¸ªæ¥å£åŒ…è£¹æ•´ä¸ªæ¨¡å‹ï¼Œè€Œè½¬å˜æˆé€å±‚è¿›è¡Œæ¨¡å‹çš„åŒ…è£¹ã€‚\nfor module in model.modules(); if isinstance(module, TransformerBlock): fully_shard(module, **fsdp_kwargs) fully_shard(model, **fsdp_kwargs) åŸå› æ˜¯åœ¨FSDP2ä¸­ï¼Œunitå˜æˆäº†æœ€å°çš„å¤„ç†å•ä½ï¼Œunitå¯ä»¥æ˜¯ä¸€ä¸ªlayerï¼Œä¸€ä¸ªnnModuleï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ•´ä¸ªæ¨¡å‹ï¼Œåˆ†å—èƒ½æ›´åˆç†åœ°å»ç®¡ç†æ¯ä¸ªåˆ†å—å‚æ•°çš„ç”Ÿå‘½å‘¨æœŸã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªé¢œè‰²ä»£è¡¨ä¸€ä¸ªunitçš„å‚æ•°ã€‚ https://pica.zhimg.com/80/v2-dc0c66f50b3c3aad15864a9bc0ace3a6_1440w.png?source=ccfced1a ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFSDP1çš„å¤„ç†æ–¹å¼ä¸ºï¼š\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP # defnition of model ...... model = FSDP(model) åˆ†å¸ƒå¼ä¸DTensor Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.\nDTensoræ˜¯pytorchçš„ä¸€ç§åˆ†å¸ƒå¼æ•°æ®ç»“æ„ï¼Œæ˜¯torch.Tensorçš„å­ç±»ã€‚DTensoré€šè¿‡Devicemeshæè¿°äº†å¼ é‡åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä»¥ä»€ä¹ˆåˆ’åˆ†æ–¹å¼åˆ†å¸ƒã€‚Placementå‚æ•°æè¿°äº†æ¯ä¸ªå‚æ•°ä¸Šçš„å‚¨å­˜æ–¹å¼ã€‚FSDP2ä½¿ç”¨äº†DTensorä½œä¸ºå‚æ•°çš„åˆ†å¸ƒå¼è¡¨è¾¾ï¼Œæ¯ä¸ªrankåªæŒæœ‰å®Œæ•´Tensorçš„å±€éƒ¨shardã€‚ DTensorç›¸å…³ä»‹ç»è§ä¸“æ æ–‡ç«  PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯» DTensorå–ä»£çš„æ˜¯FSDP1 ä½¿ç”¨çš„FlatParameterå‚æ•°åˆ‡å‰²æ–¹å¼ï¼Œä¼šæŠŠæ‰€æœ‰è¦åˆ†å—çš„å‚æ•°æ‹¼æ¥æˆä¸€ä¸ªå·¨å¤§çš„Tensorï¼Œæ ¹æ®è¦åˆ†å—çš„å¤§å°å¯¹å‚æ•°è¿›è¡Œpaddingï¼Œä¹‹åæŒ‰å¤§å°åˆ†é…ç»™å„ä¸ªrankï¼ŒåŒæ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šéšå‚æ•°åˆ‡åˆ†è€Œåˆ‡åˆ†ã€‚\nTo shard the FlatParameter, FSDP divides it into equal-sized chunks, where the number of chunks equals the sharding factor, and assigns one chunk per rank.\nFSDP1 FlatParam FSDPçš„FlatParamåˆ†å—å¯¼è‡´æŸäº›Tensorå¯èƒ½ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒçš„rankï¼Œè€ŒæŸäº›Tensorå®Œæ•´çš„ä¿ç•™åœ¨æŸäº›rankä¸­ã€‚åŸºäºDTensorçš„åˆ‡åˆ†æ–¹å¼é€šå¸¸æ˜¯æ²¿Dim0åˆ‡åˆ†ï¼ŒFSDP2é‡‡ç”¨è¿™ç§åˆ‡åˆ†æ–¹å¼ä¿è¯æ¯ä¸ªrankéƒ½è·å¾—æ¯ä¸ªparamçš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹éƒ¨ä»½Tensorè¿›è¡Œpaddingã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ ç»è¿‡FSDP2çš„init_device_meshï¼Œç”Ÿæˆåˆ†å¸ƒå¼çš„åˆ†å—ä¿¡æ¯ã€‚DTensoråœ¨è¿›è¡Œshardingæ“ä½œæ—¶ä¼šä¿ç•™è¿™äº›åˆ†å—ä¿¡æ¯ï¼ŒFSDP2ä¸­ï¼Œåœ¨FSDPParamä¸­è¿›è¡Œç»´æŠ¤ã€‚åœ¨è¿›è¡Œé€šä¿¡æ“ä½œï¼Œå¦‚All-reduceæˆ–reduce-scatteræ—¶ï¼ŒDTensorä¸­å‚¨å­˜çš„åˆ†å—ä¿¡æ¯ä¼šè¢«è¯»å–ï¼Œä»¥å®Œæˆå‚æ•°çš„shardå’Œunshardæ“ä½œã€‚ æ‰“å°fsdpå‰åçš„å‚æ•°ä¿¡æ¯ï¼Œå¯ä»¥å¾—çŸ¥shardingå‰gradå±äº\u0026lt;class \u0026rsquo;torch.Tensor\u0026rsquo;\u0026gt;ï¼Œè€Œshardingåè½¬å˜æˆäº†\u0026lt;class \u0026rsquo;torch.distributed.tensor.Tensor\u0026rsquo;\u0026gt; Streamã€Eventæ§åˆ¶\nImproving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. é€šè¿‡é¿å…recordStreamï¼Œæ— éœ€CPUåŒæ­¥é™ä½äº†GPUæ˜¾å­˜å ç”¨\nrecordStreamæ˜¯torchç”¨æ¥ç®¡ç†CUDAå¼ é‡ç”Ÿå‘½å‘¨æœŸçš„æ–¹æ³•ï¼Œå› ä¸ºGPUçš„æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæ¯”å¦‚ä¸‹å‘ä¸€ä¸ªè®¡ç®—æˆ–è€…é€šä¿¡ç®—å­æ—¶ï¼ŒCPUæ“ä½œä¼šç«‹å³è¿”å›ï¼Œè€Œå®é™…çš„æ•°æ®è®¡ç®—æˆ–è€…é€šä¿¡ä»ç„¶åœ¨GPUä¸Šè¿›è¡Œï¼Œé‚£ä¹ˆæ­¤æ—¶å¦‚æœå°è¯•è¯»å†™è¿™å—æ­£åœ¨æ“ä½œçš„bufferï¼Œå°±æœ‰å¯èƒ½å¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚\nupdate: ä¸“æ å†…å®¹recordStreamæºç è§£è¯»ä¸Šçº¿å•¦ï¼š CUDA tensorçš„ç”Ÿå‘½å‘¨æœŸæ˜¯ç”±GPUä¸Šå†…å­˜çš„åˆ†é…åˆé‡Šæ”¾ç®¡ç†çš„ï¼ŒrecordStreamä¼šè®°å½•tensorè¢«å“ªäº›æµç»‘å®šï¼Œå¦‚æœè¦é‡Šæ”¾ä¸€ä¸ªtensorï¼Œå°±å¿…é¡»ä¿è¯ä¾èµ–è¯¥tensorçš„æµå…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼Œè€Œä¸æ˜¯ä»…ä¸æ“ä½œå®ƒçš„æµç›¸å…³ã€‚\nFSDP1çš„å±€é™æ€§ï¼š FSDP1çš„prefetchå®ç°äº†åœ¨è®¡ç®—å½“å‰å±‚å‚æ•°æ—¶ï¼Œé¢„å–ä¸‹ä¸€å±‚å‚æ•°ï¼Œä»¥å®ç°é€šä¿¡/è®¡ç®—çš„overlapï¼Œä½†æ˜¯é€šå¸¸è®¡ç®—æµå’Œé€šä¿¡æµæ˜¯ä¸¤æ¡streamï¼Œä¸”ä¸åŒstreamé—´æ˜¯ç›¸äº’å¹¶è¡Œçš„ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§æƒ…å†µï¼šå³å¦‚æœæŸæ¬¡è®¡ç®—ç”±äºè€—æ—¶è¾ƒä¹…ï¼Œä¹…åˆ°è¿ç»­ä¸‹å‘çš„å‡ ä¸ªall-gatheræ“ä½œå…¨éƒ¨å®Œæˆäº†ï¼Œè®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œæ­¤æ—¶æ˜¾å­˜å°±ä¼šè¢«é€šä¿¡åçš„å‚æ•°ä¸æ–­å æ®ï¼Œç”šè‡³ä¼šå‡ºç°oomã€‚\nFSDP1å¯¹è¿™ä¸ªé—®é¢˜çš„è§£æ³•ä¹Ÿååˆ†ç²—æš´ï¼Œå³ï¼Œè®¾ç½®äº†ä¸€ä¸ªrate_limiter,å³é€Ÿç‡æ§åˆ¶å™¨ï¼Œåœ¨æ¯ä¸ªTransformerBlockåé˜»å¡CPUï¼Œrate_limiterä¸­çš„é™åˆ¶é»˜è®¤ä¸º2ï¼Œä¿è¯åªæœ‰å½“ä¸Šä¸€å±‚ç»“æŸæ—¶ï¼Œæ‰ä¼šå¯¹ä¸‹ä¸€å±‚è¿›è¡Œall-gatheræ“ä½œã€‚ æºç ï¼š\nclass _FreeEventQueue: \u0026#34;\u0026#34;\u0026#34; This tracks all pending frees corresponding to inflight all-gathers. The queueing pattern is iterative enqueues with a single dequeue per iteration once the limit ``_max_num_inflight_all_gathers`` is reached. \u0026#34;\u0026#34;\u0026#34; def __init__(self) -\u0026gt; None: self._queue: collections.deque[torch.Event] = collections.deque() self._max_num_inflight_all_gathers = 2 # empirically chosen å…³äºFSDP1è¿˜æœ‰ä¸€ä¸ªè®¨è®ºï¼Œé€šè¿‡recordStreamæ§åˆ¶ï¼Œå‡è®¾å­˜åœ¨ä¸¤ä¸ªæµstreamAå’ŒstreamBï¼Œå½“streamAæŒæœ‰æŸä¸ªTensoræ—¶ï¼Œç›´åˆ°è¿™æ¡steamA å®Œå…¨ç»“æŸæ—¶ï¼Œç›¸åº”çš„Tensoræ‰ä¼šè¢«é‡Šæ”¾ï¼Œå› æ­¤ï¼Œå³ä½¿è¿™ä¸ªTensorç”Ÿå‘½å‘¨æœŸç†è®ºå·²ç»ç»“æŸäº†ï¼Œä½†streamAæ²¡æœ‰é€€å‡ºï¼Œæ­¤æ—¶å¦‚æœåœ¨streamBä¸Šå°è¯•mallocä¸€å—å†…å­˜ï¼ŒcachingAllocatorä¼šåˆ¤æ–­æ­¤æ—¶æ²¡æœ‰ç©ºé—²å†…å­˜ï¼Œè€Œå»é‡æ–°åˆ†é…ç©ºé—´ã€‚è¿™å°±å¯¼è‡´äº†1. å†…å­˜å­˜åœ¨å³°å€¼ 2. ç”±äºåå¤mallocï¼Œæ€§èƒ½å˜å·®ã€‚ç¤¾åŒºè®¨è®ºçš„å†…å­˜å³°å€¼å¦‚å›¾ï¼š\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ åŸºæœ¬åŸç†å¦‚å›¾æ‰€ç¤ºï¼šå½“åœ¨å°è¯•delä¸€ä¸ªæ­£åœ¨ä½¿ç”¨çš„tensoråï¼Œå¦‚æœä½¿ç”¨è¿‡è¯¥tensorçš„æµè¿˜æœªç»“æŸï¼Œé‚£ä¹ˆæ­¤æ—¶çš„mallocæ“ä½œæ— æ³•å¤ç”¨å†…å­˜ï¼Œå¦‚å›¾ä¸Šçš„malloc Cï¼Œå¦‚æœåœ¨æµç»“æŸä¹‹åï¼Œå¦‚malloc Eï¼Œæ‰å¯ä»¥é¡ºåˆ©å¤ç”¨Açš„å†…å­˜ã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ æ”¹è¿›çš„ç‚¹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒæ­¥ä¸è¦å†å‘ç”Ÿåœ¨CPUä¸Šäº†ï¼Œè€Œé€šè¿‡åœ¨streamåæ’å…¥æ ‡è®°æ¥è®°å½•æ“ä½œæ˜¯å¦å®Œæˆï¼Œé€šä¿¡æµä¼šç­‰å¾…ä¾èµ–çš„è®¡ç®—æµï¼Œè€Œè®¡ç®—æµä¹Ÿä¼šç­‰å¾…ä¾èµ–çš„é€šä¿¡æµã€‚ç†è§£ä¸€ä¸‹è¿™å¼ å›¾ï¼Œä»¥iä¸ºä¾‹ï¼Œè®¡ç®—layer iæ—¶ï¼Œé¢„å–äº†i+1ï¼Œi+1çš„é€šä¿¡ä¼šäº§ç”Ÿä¸€ä¸ªevent iï¼Œè®¡ç®—æµä¼šå»waitè¿™ä¸ªeventï¼Œç›´åˆ°ç»“æŸåæ‰ä¼šå»è¿›è¡Œlayer i+1çš„è®¡ç®—ã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ ç°åœ¨CPUä¸éœ€è¦å†è¿›è¡Œç­‰å¾…ï¼Œä¹Ÿä¸ä¼šè¢«rate_limiteré˜»å¡ï¼Œç›¸åº”çš„é˜»å¡è¢«è½¬ç§»åˆ°äº†å¯¹åº”çš„æµä¸Šã€‚ è§‰å¾—æœ‰ç”¨ç‚¹ä¸ªèµå’Œå…³æ³¨ï¼Œtorchæœºåˆ¶æŒç»­æ›´æ–°ä¸­ğŸ’•\nReference\nhttps://github.com/pytorch/pytorch/issues/114299 https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486 https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html https://arxiv.org/pdf/2304.11277\n","permalink":"https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/","summary":"\u003ch2 id=\"æ¦‚è¿°\"\u003eæ¦‚è¿°\u003c/h2\u003e\n\u003cp\u003eFSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\u003c/p\u003e\n\u003ch2 id=\"fsdp2æ¢³ç†åŠæ›´æ–°ç‚¹\"\u003eFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹\u003c/h2\u003e\n\u003cp\u003eå®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eComparing with FSDP1, FSDP2 has following advantages:\nRepresenting sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.\nImproving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.\nOffering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)\nMixing frozen and non-frozen parameters can in the same communication group without using extra memory.\nä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\u003c/p\u003e","title":"PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§"},{"content":"å…³äºæˆ‘ æ¯•ä¸šäº2025å¹´çš„ä¸€æšå°å°ç¡•å£«ç”Ÿï¼Œå·¥ä½œæ–¹å‘æ˜¯å¤§æ¨¡å‹è®­ç»ƒï¼ŒåŒ…æ‹¬ä½†ä¸é™äºPyTorchã€åˆ†å¸ƒå¼ã€é«˜æ€§èƒ½è®¡ç®—ç­‰ç­‰ã€‚è¿™ä¸ªblogä¼šåˆ†äº«æŠ€æœ¯ã€ç”Ÿæ´»ã€æƒ³æ³•ä»¥åŠæˆ‘çš„æ€è€ƒï¼Œæˆ‘ä¼šå°½é‡æŠŠå®ƒä»¬å½’ç±»ï¼Œä¸å®šæœŸä¼šåŒæ­¥ä¸€äº›åˆ°ç¤¾äº¤åª’ä½“ä¸Šã€‚\nè‡³äºæˆ‘ä¸ºä»€ä¹ˆè¦å¼€é€šè¿™ä¸ªblogï¼Œå·¥ä½œä¸€æ®µæ—¶é—´ä¹‹åï¼Œæˆ‘å‘ç°å·²ç»å¾ˆå°‘æŠŠæ—¶é—´ç•™ç»™è‡ªå·±äº†ï¼Œä¸ºæ•°ä¸å¤šçš„ç©ºé—²æ—¶é—´ä¼šè¢«å„ç§æ‚äº‹è¿…é€Ÿå æ»¡ã€‚ æˆ‘æƒ³ä»æŸä¸€æ—¶åˆ»èµ·ï¼Œæœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥è®°å½•è‡ªå·±å˜åŒ–ï¼Œä¸è®ºæ˜¯å‘ç”Ÿåœ¨è‡ªå·±èº«ä¸Šçš„ï¼Œè¿˜æ˜¯å¯¹è¿™ä¸ªä¸–ç•Œçš„æ„ŸçŸ¥ã€‚\næˆ‘çš„æµ…å°è¾„æ­¢çš„å…´è¶£ä»¬ TODOï¼šåŠ ä¸ªå›¾ç‰‡å§\næ¸¸æˆï¼šå®ˆæœ›å…ˆé”‹ è‹±é›„è”ç›Ÿ ç‹è€…è£è€€ è”šè“\néŸ³ä¹ï¼šä¸‡èƒ½é’å¹´æ—…åº— å®‹å†¬é‡ èµµé›· é™ˆå¥•è¿…\nä¹å™¨ï¼šå‰ä»– é’¢ç´\n","permalink":"https://zhouyeyu.github.io/about/","summary":"about YuZhouye","title":"About"}]