<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PyTorch pr(2) cudaGraph下多流显存复用 | YuZhouye's Blog</title><meta name=keywords content="Tech,Pytorch,Pr解读"><meta name=description content="[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352
https://github.com/pytorch/pytorch/pull/158352
专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。
背景：CUDA Graph 下显存管理限制
在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：
    if (!block->stream_uses.empty()) {
      if (C10_UNLIKELY(!captures_underway.empty())) {
        // It's forbidden to cudaEventQuery an event recorded during CUDA graph
        // capture. We conservatively defer recording end-of-life events until
        // the next call to process_events() (which won't happen until no
        // captures are underway)
        needs_events_deferred_until_no_capture.push_back(block);
cudaGraph下安全的多流复用
为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：

Free marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。
Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。

内存块可重用性判断规则
cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则："><meta name=author content="YuZhouye"><link rel=canonical href=https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://zhouyeyu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zhouyeyu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zhouyeyu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zhouyeyu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zhouyeyu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/"><meta property="og:site_name" content="YuZhouye's Blog"><meta property="og:title" content="PyTorch pr(2) cudaGraph下多流显存复用"><meta property="og:description" content="[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352 https://github.com/pytorch/pytorch/pull/158352
专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。
背景：CUDA Graph 下显存管理限制 在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：
if (!block->stream_uses.empty()) { if (C10_UNLIKELY(!captures_underway.empty())) { // It's forbidden to cudaEventQuery an event recorded during CUDA graph // capture. We conservatively defer recording end-of-life events until // the next call to process_events() (which won't happen until no // captures are underway) needs_events_deferred_until_no_capture.push_back(block); cudaGraph下安全的多流复用 为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：
Free marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。 Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。 内存块可重用性判断规则 cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则："><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-05T23:11:11+08:00"><meta property="article:modified_time" content="2026-01-05T23:11:11+08:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Pr解读"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch pr(2) cudaGraph下多流显存复用"><meta name=twitter:description content="[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352
https://github.com/pytorch/pytorch/pull/158352
专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。
背景：CUDA Graph 下显存管理限制
在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：
    if (!block->stream_uses.empty()) {
      if (C10_UNLIKELY(!captures_underway.empty())) {
        // It's forbidden to cudaEventQuery an event recorded during CUDA graph
        // capture. We conservatively defer recording end-of-life events until
        // the next call to process_events() (which won't happen until no
        // captures are underway)
        needs_events_deferred_until_no_capture.push_back(block);
cudaGraph下安全的多流复用
为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：

Free marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。
Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。

内存块可重用性判断规则
cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zhouyeyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch pr(2) cudaGraph下多流显存复用","item":"https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch pr(2) cudaGraph下多流显存复用","name":"PyTorch pr(2) cudaGraph下多流显存复用","description":"[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352 https://github.com/pytorch/pytorch/pull/158352\n专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。\n背景：CUDA Graph 下显存管理限制 在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：\nif (!block-\u0026gt;stream_uses.empty()) { if (C10_UNLIKELY(!captures_underway.empty())) { // It\u0026#39;s forbidden to cudaEventQuery an event recorded during CUDA graph // capture. We conservatively defer recording end-of-life events until // the next call to process_events() (which won\u0026#39;t happen until no // captures are underway) needs_events_deferred_until_no_capture.push_back(block); cudaGraph下安全的多流复用 为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：\nFree marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。 Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。 内存块可重用性判断规则 cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则：\n","keywords":["Tech","Pytorch","Pr解读"],"articleBody":"[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352 https://github.com/pytorch/pytorch/pull/158352\n专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。\n背景：CUDA Graph 下显存管理限制 在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：\nif (!block-\u003estream_uses.empty()) { if (C10_UNLIKELY(!captures_underway.empty())) { // It's forbidden to cudaEventQuery an event recorded during CUDA graph // capture. We conservatively defer recording end-of-life events until // the next call to process_events() (which won't happen until no // captures are underway) needs_events_deferred_until_no_capture.push_back(block); cudaGraph下安全的多流复用 为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：\nFree marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。 Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。 内存块可重用性判断规则 cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则：\nStrong Rule (Graph-Wide Safety)：从全局保障安全，规定若内存块的所有空闲标记均为所有活跃流的终端节点的 “前驱节点”，则该块可安全重用，通过严格的全局执行顺序避免重放时的生命周期重叠； Per-stream Rule (A Practical Optimization)： 仅验证单流，对于流 S 上的分配请求，若内存块的所有空闲标记是流 S 的终端节点的前驱节点，即可在流 S 上重用，因此无需验证全图安全性。 核心实现流程 捕获期间执行free 对block-\u003estream_uses中的每个流及 “分配流”，插入free marker，关联对应的block和free marker所对应的空node if (CUDAAllocatorConfig::graph_capture_record_stream_reuse()) { deferred_blocks.emplace(block, insert_free_marker(block)); // 插入free marker } else { deferred_blocks.emplace(block, std::vector\u003ccudaGraphNode_t\u003e{}); // 捕获结束后访问 } 捕获期间malloc if (CUDAAllocatorConfig::graph_capture_record_stream_reuse()) { free_safe_blocks_in_capture(context, stream); } free_safe_blocks_in_capture函数内容： get_reusable_empty_nodes通过获取当前流的终端节点，通过反向 DFS 遍历每个终端节点的依赖链，统计空闲节点能到达的终端数量；最终筛选出 “能到达所有终端节点” 的空闲节点，构成当前流可复用的空闲节点集合，从而判断哪些free marker属于当前流可复用空闲节点，之后，判断free时载入过的block中哪些可以被正确复用，如果可复用则释放：\nauto reusable_empty_nodes = get_reusable_empty_nodes(stream); if (reusable_empty_nodes.empty()) { return; } std::vector\u003cBlock*\u003e blocks_to_erase; // 记录待删除的块 for (auto\u0026 [block, inserted_empty_nodes] : deferred_blocks) { // 跳过两类块：1. 无空闲标记的块（无法判断安全）；2. 分配流≠当前流的块 if (inserted_empty_nodes.empty() || block-\u003estream != stream) { continue; } // 该块的所有空闲标记是否都在reusable_empty_nodes中 bool is_reusable = true; for (const auto\u0026 node : inserted_empty_nodes) { if (reusable_empty_nodes.find(node) == reusable_empty_nodes.end()) { is_reusable = false; break; } } // 若所有标记都安全 → 该块可回收 if (is_reusable) { // 清除stream_uses：Graph已通过依赖保证同步，无需再跟踪多流使用 block-\u003estream_uses.clear(); // 将块回收到空闲列表 free_block(block, context); // 记录该块，后续从deferred_blocks中删除 blocks_to_erase.push_back(block); } } 现在，只要开启graph_capture_record_stream_reuse，pytorch会自动尝试在cuda_graph中通过插入free marker来提前释放一些block，使其生命周期变短，而不再是等到捕获完全结束时一次性释放。\n","wordCount":"219","inLanguage":"en","datePublished":"2026-01-05T23:11:11+08:00","dateModified":"2026-01-05T23:11:11+08:00","author":{"@type":"Person","name":"YuZhouye"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/"},"publisher":{"@type":"Organization","name":"YuZhouye's Blog","logo":{"@type":"ImageObject","url":"https://zhouyeyu.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://zhouyeyu.github.io/ accesskey=h title="YuZhouye's Blog (Alt + H)">YuZhouye's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://zhouyeyu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://zhouyeyu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://zhouyeyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://zhouyeyu.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">PyTorch pr(2) cudaGraph下多流显存复用</h1><div class=post-meta><span title='2026-01-05 23:11:11 +0800 CST'>January 5, 2026</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>219 words</span>&nbsp;·&nbsp;<span>YuZhouye</span></div></header><div class=post-content><h2 id=cuda-reuse-blocks-with-record_stream-during-cuda-graph-capture-in-the-cudacachingallocator-158352>[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352<a hidden class=anchor aria-hidden=true href=#cuda-reuse-blocks-with-record_stream-during-cuda-graph-capture-in-the-cudacachingallocator-158352>#</a></h2><p><a href=https://github.com/pytorch/pytorch/pull/158352>https://github.com/pytorch/pytorch/pull/158352</a></p><p>专栏另一篇文章解读了record_stream相关内容，但CUDACachingAllocator源码中存在一个分支，会根据CUDAAllocatorConfig::graph_capture_record_stream_reuse()判断是否free和malloc某个block，这里填个坑，找一下对应的pr看是怎么复用的。</p><h2 id=背景cuda-graph-下显存管理限制>背景：CUDA Graph 下显存管理限制<a hidden class=anchor aria-hidden=true href=#背景cuda-graph-下显存管理限制>#</a></h2><p>在 CUDA Graph 捕获过程中，CUDACachingAllocator对内存的管理存在一个关键限制：必须等到捕获结束后才能回收内存块。这一限制源于 CUDA 的底层约束 —— 捕获阶段不允许查询event状态（因为此时 CUDA 操作尚未执行），而CUDACachingAllocator依赖事件驱动逻辑判断内存块是否可回收。而这会导致本该被释放的block未能被正确释放，直到捕获完全结束，比如free中的代码片段：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>block</span><span class=o>-&gt;</span><span class=n>stream_uses</span><span class=p>.</span><span class=n>empty</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>C10_UNLIKELY</span><span class=p>(</span><span class=o>!</span><span class=n>captures_underway</span><span class=p>.</span><span class=n>empty</span><span class=p>()))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// It&#39;s forbidden to cudaEventQuery an event recorded during CUDA graph
</span></span></span><span class=line><span class=cl>        <span class=c1>// capture. We conservatively defer recording end-of-life events until
</span></span></span><span class=line><span class=cl>        <span class=c1>// the next call to process_events() (which won&#39;t happen until no
</span></span></span><span class=line><span class=cl>        <span class=c1>// captures are underway)
</span></span></span><span class=line><span class=cl>        <span class=n>needs_events_deferred_until_no_capture</span><span class=p>.</span><span class=n>push_back</span><span class=p>(</span><span class=n>block</span><span class=p>);</span>
</span></span></code></pre></div><h2 id=cudagraph下安全的多流复用>cudaGraph下安全的多流复用<a hidden class=anchor aria-hidden=true href=#cudagraph下安全的多流复用>#</a></h2><p>为实现安全重用，PR 首先明确了两个核心术语，作为后续判断逻辑的基础：</p><ul><li>Free marker：通过cudaGraphAddEmptyNode创建的 “捕获合法” 空节点，插入到每个使用过该内存块的stream中，且位于该块最后一次被使用的操作之后，用于标记 “内存块已空闲”。</li><li>Terminal Node：流或捕获图中 “最新操作” 的集合，新捕获的操作会附加在终端节点之后。对于正在捕获的流，可通过cudaStreamGetCaptureInfo的dependencies_out参数获取终端节点集合。</li></ul><h3 id=内存块可重用性判断规则>内存块可重用性判断规则<a hidden class=anchor aria-hidden=true href=#内存块可重用性判断规则>#</a></h3><p>cudaGraph生成一个DAG，因此作者提出了两种平衡 “安全性” 与 “灵活性” 的判断规则：</p><ul><li>Strong Rule (Graph-Wide Safety)：从全局保障安全，规定若内存块的所有空闲标记均为所有活跃流的终端节点的 “前驱节点”，则该块可安全重用，通过严格的全局执行顺序避免重放时的生命周期重叠；</li><li>Per-stream Rule (A Practical Optimization)： 仅验证单流，对于流 S 上的分配请求，若内存块的所有空闲标记是流 S 的终端节点的前驱节点，即可在流 S 上重用，因此无需验证全图安全性。</li></ul><p><img alt="Image Alt Text" loading=lazy src=/PRs/PRs_01.jpg></p><h2 id=核心实现流程>核心实现流程<a hidden class=anchor aria-hidden=true href=#核心实现流程>#</a></h2><h3 id=捕获期间执行free>捕获期间执行free<a hidden class=anchor aria-hidden=true href=#捕获期间执行free>#</a></h3><ol><li>对block->stream_uses中的每个流及 “分配流”，插入free marker，关联对应的block和free marker所对应的空node</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>CUDAAllocatorConfig</span><span class=o>::</span><span class=n>graph_capture_record_stream_reuse</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>deferred_blocks</span><span class=p>.</span><span class=n>emplace</span><span class=p>(</span><span class=n>block</span><span class=p>,</span> <span class=n>insert_free_marker</span><span class=p>(</span><span class=n>block</span><span class=p>));</span> <span class=c1>// 插入free marker
</span></span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>deferred_blocks</span><span class=p>.</span><span class=n>emplace</span><span class=p>(</span><span class=n>block</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=n>cudaGraphNode_t</span><span class=o>&gt;</span><span class=p>{});</span> <span class=c1>// 捕获结束后访问
</span></span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span></code></pre></div><ol start=2><li>捕获期间malloc</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>CUDAAllocatorConfig</span><span class=o>::</span><span class=n>graph_capture_record_stream_reuse</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>free_safe_blocks_in_capture</span><span class=p>(</span><span class=n>context</span><span class=p>,</span> <span class=n>stream</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span></code></pre></div><h4 id=free_safe_blocks_in_capture函数内容><code>free_safe_blocks_in_capture</code>函数内容：<a hidden class=anchor aria-hidden=true href=#free_safe_blocks_in_capture函数内容>#</a></h4><p>get_reusable_empty_nodes通过获取当前流的终端节点，通过反向 DFS 遍历每个终端节点的依赖链，统计空闲节点能到达的终端数量；最终筛选出 “能到达所有终端节点” 的空闲节点，构成当前流可复用的空闲节点集合，从而判断哪些free marker属于当前流可复用空闲节点，之后，判断free时载入过的block中哪些可以被正确复用，如果可复用则释放：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=n>reusable_empty_nodes</span> <span class=o>=</span> <span class=n>get_reusable_empty_nodes</span><span class=p>(</span><span class=n>stream</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>reusable_empty_nodes</span><span class=p>.</span><span class=n>empty</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=n>Block</span><span class=o>*&gt;</span> <span class=n>blocks_to_erase</span><span class=p>;</span> <span class=c1>// 记录待删除的块
</span></span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=k>auto</span><span class=o>&amp;</span> <span class=p>[</span><span class=n>block</span><span class=p>,</span> <span class=n>inserted_empty_nodes</span><span class=p>]</span> <span class=o>:</span> <span class=n>deferred_blocks</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>// 跳过两类块：1. 无空闲标记的块（无法判断安全）；2. 分配流≠当前流的块
</span></span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>inserted_empty_nodes</span><span class=p>.</span><span class=n>empty</span><span class=p>()</span> <span class=o>||</span> <span class=n>block</span><span class=o>-&gt;</span><span class=n>stream</span> <span class=o>!=</span> <span class=n>stream</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>continue</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// 该块的所有空闲标记是否都在reusable_empty_nodes中
</span></span></span><span class=line><span class=cl>  <span class=kt>bool</span> <span class=n>is_reusable</span> <span class=o>=</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=k>const</span> <span class=k>auto</span><span class=o>&amp;</span> <span class=nl>node</span> <span class=p>:</span> <span class=n>inserted_empty_nodes</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>reusable_empty_nodes</span><span class=p>.</span><span class=n>find</span><span class=p>(</span><span class=n>node</span><span class=p>)</span> <span class=o>==</span> <span class=n>reusable_empty_nodes</span><span class=p>.</span><span class=n>end</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>is_reusable</span> <span class=o>=</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// 若所有标记都安全 → 该块可回收
</span></span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>is_reusable</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 清除stream_uses：Graph已通过依赖保证同步，无需再跟踪多流使用
</span></span></span><span class=line><span class=cl>    <span class=n>block</span><span class=o>-&gt;</span><span class=n>stream_uses</span><span class=p>.</span><span class=n>clear</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 将块回收到空闲列表
</span></span></span><span class=line><span class=cl>    <span class=n>free_block</span><span class=p>(</span><span class=n>block</span><span class=p>,</span> <span class=n>context</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 记录该块，后续从deferred_blocks中删除
</span></span></span><span class=line><span class=cl>    <span class=n>blocks_to_erase</span><span class=p>.</span><span class=n>push_back</span><span class=p>(</span><span class=n>block</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>现在，只要开启<code>graph_capture_record_stream_reuse</code>，pytorch会自动尝试在cuda_graph中通过插入free marker来提前释放一些block，使其生命周期变短，而不再是等到捕获完全结束时一次性释放。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zhouyeyu.github.io/tags/tech/>Tech</a></li><li><a href=https://zhouyeyu.github.io/tags/pytorch/>Pytorch</a></li><li><a href=https://zhouyeyu.github.io/tags/pr%E8%A7%A3%E8%AF%BB/>Pr解读</a></li></ul><nav class=paginav><a class=prev href=https://zhouyeyu.github.io/posts/test/><span class=title>« Prev</span><br><span>Hugo部署github Pages</span>
</a><a class=next href=https://zhouyeyu.github.io/posts/pytorch-record_stream-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/><span class=title>Next »</span><br><span>PyTorch record_stream 源码解读</span></a></nav></footer><div class=comments><script>let giscusTheme=document.body.className.includes("dark")?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","zhouyeyu/zhouyeyu.github.io"),s.setAttribute("data-repo-id","R_kgDOQ7G1cA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOQ7G1cM4C1Z0P"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","top"),s.setAttribute("data-lang","zh-CN"),s.setAttribute("data-theme",giscusTheme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector(".comments").appendChild(s),document.getElementById("theme-toggle").addEventListener("click",()=>{const t=document.body.className.includes("dark")?"light":"dark",n={giscus:{setConfig:{theme:t}}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage(n,"https://giscus.app")})</script></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zhouyeyu.github.io/>YuZhouye's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>