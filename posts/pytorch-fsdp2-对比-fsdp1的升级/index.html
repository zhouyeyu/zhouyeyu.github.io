<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ | YuZhouye's Blog</title><meta name=keywords content="Tech,Pytorch,FSDP2"><meta name=description content="æ¦‚è¿°
FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚
FSDP2æ¢³ç†åŠæ›´æ–°ç‚¹
å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š

Comparing with FSDP1, FSDP2 has following advantages:
Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.
Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.
Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)
Mixing frozen and non-frozen parameters can in the same communication group without using extra memory.
ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚"><meta name=author content="YuZhouye"><link rel=canonical href=http://localhost:1313/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="http://localhost:1313/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/"><meta property="og:site_name" content="YuZhouye's Blog"><meta property="og:title" content="PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§"><meta property="og:description" content="æ¦‚è¿° FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚
FSDP2æ¢³ç†åŠæ›´æ–°ç‚¹ å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š
Comparing with FSDP1, FSDP2 has following advantages: Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow. Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc) Mixing frozen and non-frozen parameters can in the same communication group without using extra memory. ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-18T21:37:02+08:00"><meta property="article:modified_time" content="2025-12-18T21:37:02+08:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="FSDP2"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§"><meta name=twitter:description content="æ¦‚è¿°
FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚
FSDP2æ¢³ç†åŠæ›´æ–°ç‚¹
å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š

Comparing with FSDP1, FSDP2 has following advantages:
Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.
Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.
Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)
Mixing frozen and non-frozen parameters can in the same communication group without using extra memory.
ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§","item":"http://localhost:1313/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§","name":"PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§","description":"æ¦‚è¿° FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\nFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹ å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\nComparing with FSDP1, FSDP2 has following advantages: Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow. Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc) Mixing frozen and non-frozen parameters can in the same communication group without using extra memory. ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\n","keywords":["Tech","Pytorch","FSDP2"],"articleBody":"æ¦‚è¿° FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚\nFSDP2æ¢³ç†åŠæ›´æ–°ç‚¹ å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š\nComparing with FSDP1, FSDP2 has following advantages: Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow. Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc) Mixing frozen and non-frozen parameters can in the same communication group without using extra memory. ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚\nAPIæ¥å£ FSDP2çš„æ¥å£ä¸æ˜¯å‘åå…¼å®¹çš„ï¼Œç›¸åº”çš„æ¥å£åç§°ä¹Ÿä»FSDPå˜æˆäº†fully_shardï¼Œä»¥ä¸€ä¸ªmodelä¸ºä¾‹ï¼Œä¸å†åƒFSDP1ä¸€æ ·å¯ä»¥ç›´æ¥é€šè¿‡ä¸€ä¸ªæ¥å£åŒ…è£¹æ•´ä¸ªæ¨¡å‹ï¼Œè€Œè½¬å˜æˆé€å±‚è¿›è¡Œæ¨¡å‹çš„åŒ…è£¹ã€‚\nfor module in model.modules(); if isinstance(module, TransformerBlock): fully_shard(module, **fsdp_kwargs) fully_shard(model, **fsdp_kwargs) åŸå› æ˜¯åœ¨FSDP2ä¸­ï¼Œunitå˜æˆäº†æœ€å°çš„å¤„ç†å•ä½ï¼Œunitå¯ä»¥æ˜¯ä¸€ä¸ªlayerï¼Œä¸€ä¸ªnnModuleï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ•´ä¸ªæ¨¡å‹ï¼Œåˆ†å—èƒ½æ›´åˆç†åœ°å»ç®¡ç†æ¯ä¸ªåˆ†å—å‚æ•°çš„ç”Ÿå‘½å‘¨æœŸã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªé¢œè‰²ä»£è¡¨ä¸€ä¸ªunitçš„å‚æ•°ã€‚ https://pica.zhimg.com/80/v2-dc0c66f50b3c3aad15864a9bc0ace3a6_1440w.png?source=ccfced1a ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFSDP1çš„å¤„ç†æ–¹å¼ä¸ºï¼š\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP # defnition of model ...... model = FSDP(model) åˆ†å¸ƒå¼ä¸DTensor Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.\nDTensoræ˜¯pytorchçš„ä¸€ç§åˆ†å¸ƒå¼æ•°æ®ç»“æ„ï¼Œæ˜¯torch.Tensorçš„å­ç±»ã€‚DTensoré€šè¿‡Devicemeshæè¿°äº†å¼ é‡åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä»¥ä»€ä¹ˆåˆ’åˆ†æ–¹å¼åˆ†å¸ƒã€‚Placementå‚æ•°æè¿°äº†æ¯ä¸ªå‚æ•°ä¸Šçš„å‚¨å­˜æ–¹å¼ã€‚FSDP2ä½¿ç”¨äº†DTensorä½œä¸ºå‚æ•°çš„åˆ†å¸ƒå¼è¡¨è¾¾ï¼Œæ¯ä¸ªrankåªæŒæœ‰å®Œæ•´Tensorçš„å±€éƒ¨shardã€‚ DTensorç›¸å…³ä»‹ç»è§ä¸“æ æ–‡ç«  PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯» DTensorå–ä»£çš„æ˜¯FSDP1 ä½¿ç”¨çš„FlatParameterå‚æ•°åˆ‡å‰²æ–¹å¼ï¼Œä¼šæŠŠæ‰€æœ‰è¦åˆ†å—çš„å‚æ•°æ‹¼æ¥æˆä¸€ä¸ªå·¨å¤§çš„Tensorï¼Œæ ¹æ®è¦åˆ†å—çš„å¤§å°å¯¹å‚æ•°è¿›è¡Œpaddingï¼Œä¹‹åæŒ‰å¤§å°åˆ†é…ç»™å„ä¸ªrankï¼ŒåŒæ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šéšå‚æ•°åˆ‡åˆ†è€Œåˆ‡åˆ†ã€‚\nTo shard the FlatParameter, FSDP divides it into equal-sized chunks, where the number of chunks equals the sharding factor, and assigns one chunk per rank.\nFSDP1 FlatParam FSDPçš„FlatParamåˆ†å—å¯¼è‡´æŸäº›Tensorå¯èƒ½ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒçš„rankï¼Œè€ŒæŸäº›Tensorå®Œæ•´çš„ä¿ç•™åœ¨æŸäº›rankä¸­ã€‚åŸºäºDTensorçš„åˆ‡åˆ†æ–¹å¼é€šå¸¸æ˜¯æ²¿Dim0åˆ‡åˆ†ï¼ŒFSDP2é‡‡ç”¨è¿™ç§åˆ‡åˆ†æ–¹å¼ä¿è¯æ¯ä¸ªrankéƒ½è·å¾—æ¯ä¸ªparamçš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹éƒ¨ä»½Tensorè¿›è¡Œpaddingã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ ç»è¿‡FSDP2çš„init_device_meshï¼Œç”Ÿæˆåˆ†å¸ƒå¼çš„åˆ†å—ä¿¡æ¯ã€‚DTensoråœ¨è¿›è¡Œshardingæ“ä½œæ—¶ä¼šä¿ç•™è¿™äº›åˆ†å—ä¿¡æ¯ï¼ŒFSDP2ä¸­ï¼Œåœ¨FSDPParamä¸­è¿›è¡Œç»´æŠ¤ã€‚åœ¨è¿›è¡Œé€šä¿¡æ“ä½œï¼Œå¦‚All-reduceæˆ–reduce-scatteræ—¶ï¼ŒDTensorä¸­å‚¨å­˜çš„åˆ†å—ä¿¡æ¯ä¼šè¢«è¯»å–ï¼Œä»¥å®Œæˆå‚æ•°çš„shardå’Œunshardæ“ä½œã€‚ æ‰“å°fsdpå‰åçš„å‚æ•°ä¿¡æ¯ï¼Œå¯ä»¥å¾—çŸ¥shardingå‰gradå±äºï¼Œè€Œshardingåè½¬å˜æˆäº† Streamã€Eventæ§åˆ¶\nImproving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization. é€šè¿‡é¿å…recordStreamï¼Œæ— éœ€CPUåŒæ­¥é™ä½äº†GPUæ˜¾å­˜å ç”¨\nrecordStreamæ˜¯torchç”¨æ¥ç®¡ç†CUDAå¼ é‡ç”Ÿå‘½å‘¨æœŸçš„æ–¹æ³•ï¼Œå› ä¸ºGPUçš„æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæ¯”å¦‚ä¸‹å‘ä¸€ä¸ªè®¡ç®—æˆ–è€…é€šä¿¡ç®—å­æ—¶ï¼ŒCPUæ“ä½œä¼šç«‹å³è¿”å›ï¼Œè€Œå®é™…çš„æ•°æ®è®¡ç®—æˆ–è€…é€šä¿¡ä»ç„¶åœ¨GPUä¸Šè¿›è¡Œï¼Œé‚£ä¹ˆæ­¤æ—¶å¦‚æœå°è¯•è¯»å†™è¿™å—æ­£åœ¨æ“ä½œçš„bufferï¼Œå°±æœ‰å¯èƒ½å¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚\nupdate: ä¸“æ å†…å®¹recordStreamæºç è§£è¯»ä¸Šçº¿å•¦ï¼š CUDA tensorçš„ç”Ÿå‘½å‘¨æœŸæ˜¯ç”±GPUä¸Šå†…å­˜çš„åˆ†é…åˆé‡Šæ”¾ç®¡ç†çš„ï¼ŒrecordStreamä¼šè®°å½•tensorè¢«å“ªäº›æµç»‘å®šï¼Œå¦‚æœè¦é‡Šæ”¾ä¸€ä¸ªtensorï¼Œå°±å¿…é¡»ä¿è¯ä¾èµ–è¯¥tensorçš„æµå…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼Œè€Œä¸æ˜¯ä»…ä¸æ“ä½œå®ƒçš„æµç›¸å…³ã€‚\nFSDP1çš„å±€é™æ€§ï¼š FSDP1çš„prefetchå®ç°äº†åœ¨è®¡ç®—å½“å‰å±‚å‚æ•°æ—¶ï¼Œé¢„å–ä¸‹ä¸€å±‚å‚æ•°ï¼Œä»¥å®ç°é€šä¿¡/è®¡ç®—çš„overlapï¼Œä½†æ˜¯é€šå¸¸è®¡ç®—æµå’Œé€šä¿¡æµæ˜¯ä¸¤æ¡streamï¼Œä¸”ä¸åŒstreamé—´æ˜¯ç›¸äº’å¹¶è¡Œçš„ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§æƒ…å†µï¼šå³å¦‚æœæŸæ¬¡è®¡ç®—ç”±äºè€—æ—¶è¾ƒä¹…ï¼Œä¹…åˆ°è¿ç»­ä¸‹å‘çš„å‡ ä¸ªall-gatheræ“ä½œå…¨éƒ¨å®Œæˆäº†ï¼Œè®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œæ­¤æ—¶æ˜¾å­˜å°±ä¼šè¢«é€šä¿¡åçš„å‚æ•°ä¸æ–­å æ®ï¼Œç”šè‡³ä¼šå‡ºç°oomã€‚\nFSDP1å¯¹è¿™ä¸ªé—®é¢˜çš„è§£æ³•ä¹Ÿååˆ†ç²—æš´ï¼Œå³ï¼Œè®¾ç½®äº†ä¸€ä¸ªrate_limiter,å³é€Ÿç‡æ§åˆ¶å™¨ï¼Œåœ¨æ¯ä¸ªTransformerBlockåé˜»å¡CPUï¼Œrate_limiterä¸­çš„é™åˆ¶é»˜è®¤ä¸º2ï¼Œä¿è¯åªæœ‰å½“ä¸Šä¸€å±‚ç»“æŸæ—¶ï¼Œæ‰ä¼šå¯¹ä¸‹ä¸€å±‚è¿›è¡Œall-gatheræ“ä½œã€‚ æºç ï¼š\nclass _FreeEventQueue: \"\"\" This tracks all pending frees corresponding to inflight all-gathers. The queueing pattern is iterative enqueues with a single dequeue per iteration once the limit ``_max_num_inflight_all_gathers`` is reached. \"\"\" def __init__(self) -\u003e None: self._queue: collections.deque[torch.Event] = collections.deque() self._max_num_inflight_all_gathers = 2 # empirically chosen å…³äºFSDP1è¿˜æœ‰ä¸€ä¸ªè®¨è®ºï¼Œé€šè¿‡recordStreamæ§åˆ¶ï¼Œå‡è®¾å­˜åœ¨ä¸¤ä¸ªæµstreamAå’ŒstreamBï¼Œå½“streamAæŒæœ‰æŸä¸ªTensoræ—¶ï¼Œç›´åˆ°è¿™æ¡steamA å®Œå…¨ç»“æŸæ—¶ï¼Œç›¸åº”çš„Tensoræ‰ä¼šè¢«é‡Šæ”¾ï¼Œå› æ­¤ï¼Œå³ä½¿è¿™ä¸ªTensorç”Ÿå‘½å‘¨æœŸç†è®ºå·²ç»ç»“æŸäº†ï¼Œä½†streamAæ²¡æœ‰é€€å‡ºï¼Œæ­¤æ—¶å¦‚æœåœ¨streamBä¸Šå°è¯•mallocä¸€å—å†…å­˜ï¼ŒcachingAllocatorä¼šåˆ¤æ–­æ­¤æ—¶æ²¡æœ‰ç©ºé—²å†…å­˜ï¼Œè€Œå»é‡æ–°åˆ†é…ç©ºé—´ã€‚è¿™å°±å¯¼è‡´äº†1. å†…å­˜å­˜åœ¨å³°å€¼ 2. ç”±äºåå¤mallocï¼Œæ€§èƒ½å˜å·®ã€‚ç¤¾åŒºè®¨è®ºçš„å†…å­˜å³°å€¼å¦‚å›¾ï¼š\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ åŸºæœ¬åŸç†å¦‚å›¾æ‰€ç¤ºï¼šå½“åœ¨å°è¯•delä¸€ä¸ªæ­£åœ¨ä½¿ç”¨çš„tensoråï¼Œå¦‚æœä½¿ç”¨è¿‡è¯¥tensorçš„æµè¿˜æœªç»“æŸï¼Œé‚£ä¹ˆæ­¤æ—¶çš„mallocæ“ä½œæ— æ³•å¤ç”¨å†…å­˜ï¼Œå¦‚å›¾ä¸Šçš„malloc Cï¼Œå¦‚æœåœ¨æµç»“æŸä¹‹åï¼Œå¦‚malloc Eï¼Œæ‰å¯ä»¥é¡ºåˆ©å¤ç”¨Açš„å†…å­˜ã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ æ”¹è¿›çš„ç‚¹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒæ­¥ä¸è¦å†å‘ç”Ÿåœ¨CPUä¸Šäº†ï¼Œè€Œé€šè¿‡åœ¨streamåæ’å…¥æ ‡è®°æ¥è®°å½•æ“ä½œæ˜¯å¦å®Œæˆï¼Œé€šä¿¡æµä¼šç­‰å¾…ä¾èµ–çš„è®¡ç®—æµï¼Œè€Œè®¡ç®—æµä¹Ÿä¼šç­‰å¾…ä¾èµ–çš„é€šä¿¡æµã€‚ç†è§£ä¸€ä¸‹è¿™å¼ å›¾ï¼Œä»¥iä¸ºä¾‹ï¼Œè®¡ç®—layer iæ—¶ï¼Œé¢„å–äº†i+1ï¼Œi+1çš„é€šä¿¡ä¼šäº§ç”Ÿä¸€ä¸ªevent iï¼Œè®¡ç®—æµä¼šå»waitè¿™ä¸ªeventï¼Œç›´åˆ°ç»“æŸåæ‰ä¼šå»è¿›è¡Œlayer i+1çš„è®¡ç®—ã€‚\næ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰ ç°åœ¨CPUä¸éœ€è¦å†è¿›è¡Œç­‰å¾…ï¼Œä¹Ÿä¸ä¼šè¢«rate_limiteré˜»å¡ï¼Œç›¸åº”çš„é˜»å¡è¢«è½¬ç§»åˆ°äº†å¯¹åº”çš„æµä¸Šã€‚ è§‰å¾—æœ‰ç”¨ç‚¹ä¸ªèµå’Œå…³æ³¨ï¼Œtorchæœºåˆ¶æŒç»­æ›´æ–°ä¸­ğŸ’•\nReference\nhttps://github.com/pytorch/pytorch/issues/114299 https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486 https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html https://arxiv.org/pdf/2304.11277\n","wordCount":"306","inLanguage":"en","datePublished":"2025-12-18T21:37:02+08:00","dateModified":"2025-12-18T21:37:02+08:00","author":{"@type":"Person","name":"YuZhouye"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/"},"publisher":{"@type":"Organization","name":"YuZhouye's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="YuZhouye's Blog (Alt + H)">YuZhouye's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§</h1><div class=post-meta><span title='2025-12-18 21:37:02 +0800 CST'>December 18, 2025</span>&nbsp;Â·&nbsp;<span>2 min</span>&nbsp;Â·&nbsp;<span>306 words</span>&nbsp;Â·&nbsp;<span>YuZhouye</span></div></header><div class=post-content><h2 id=æ¦‚è¿°>æ¦‚è¿°<a hidden class=anchor aria-hidden=true href=#æ¦‚è¿°>#</a></h2><p>FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚</p><h2 id=fsdp2æ¢³ç†åŠæ›´æ–°ç‚¹>FSDP2æ¢³ç†åŠæ›´æ–°ç‚¹<a hidden class=anchor aria-hidden=true href=#fsdp2æ¢³ç†åŠæ›´æ–°ç‚¹>#</a></h2><p>å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š</p><blockquote><p>Comparing with FSDP1, FSDP2 has following advantages:
Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.
Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.
Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)
Mixing frozen and non-frozen parameters can in the same communication group without using extra memory.
ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚</p></blockquote><h2 id=apiæ¥å£>APIæ¥å£<a hidden class=anchor aria-hidden=true href=#apiæ¥å£>#</a></h2><p>FSDP2çš„æ¥å£ä¸æ˜¯å‘åå…¼å®¹çš„ï¼Œç›¸åº”çš„æ¥å£åç§°ä¹Ÿä»FSDPå˜æˆäº†fully_shardï¼Œä»¥ä¸€ä¸ªmodelä¸ºä¾‹ï¼Œä¸å†åƒFSDP1ä¸€æ ·å¯ä»¥ç›´æ¥é€šè¿‡ä¸€ä¸ªæ¥å£åŒ…è£¹æ•´ä¸ªæ¨¡å‹ï¼Œè€Œè½¬å˜æˆé€å±‚è¿›è¡Œæ¨¡å‹çš„åŒ…è£¹ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>module</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>modules</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>TransformerBlock</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>fully_shard</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=o>**</span><span class=n>fsdp_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fully_shard</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=o>**</span><span class=n>fsdp_kwargs</span><span class=p>)</span>
</span></span></code></pre></div><p>åŸå› æ˜¯åœ¨FSDP2ä¸­ï¼Œunitå˜æˆäº†æœ€å°çš„å¤„ç†å•ä½ï¼Œunitå¯ä»¥æ˜¯ä¸€ä¸ªlayerï¼Œä¸€ä¸ªnnModuleï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ•´ä¸ªæ¨¡å‹ï¼Œåˆ†å—èƒ½æ›´åˆç†åœ°å»ç®¡ç†æ¯ä¸ªåˆ†å—å‚æ•°çš„ç”Ÿå‘½å‘¨æœŸã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªé¢œè‰²ä»£è¡¨ä¸€ä¸ªunitçš„å‚æ•°ã€‚
<a href="https://pica.zhimg.com/80/v2-dc0c66f50b3c3aad15864a9bc0ace3a6_1440w.png?source=ccfced1a">https://pica.zhimg.com/80/v2-dc0c66f50b3c3aad15864a9bc0ace3a6_1440w.png?source=ccfced1a</a>
ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFSDP1çš„å¤„ç†æ–¹å¼ä¸ºï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.distributed.fsdp</span> <span class=kn>import</span> <span class=n>FullyShardedDataParallel</span> <span class=k>as</span> <span class=n>FSDP</span>
</span></span><span class=line><span class=cl><span class=c1># defnition of model</span>
</span></span><span class=line><span class=cl><span class=o>......</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>FSDP</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=åˆ†å¸ƒå¼ä¸dtensor>åˆ†å¸ƒå¼ä¸DTensor<a hidden class=anchor aria-hidden=true href=#åˆ†å¸ƒå¼ä¸dtensor>#</a></h2><blockquote><p>Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.</p></blockquote><p>DTensoræ˜¯pytorchçš„ä¸€ç§åˆ†å¸ƒå¼æ•°æ®ç»“æ„ï¼Œæ˜¯torch.Tensorçš„å­ç±»ã€‚DTensoré€šè¿‡Devicemeshæè¿°äº†å¼ é‡åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä»¥ä»€ä¹ˆåˆ’åˆ†æ–¹å¼åˆ†å¸ƒã€‚Placementå‚æ•°æè¿°äº†æ¯ä¸ªå‚æ•°ä¸Šçš„å‚¨å­˜æ–¹å¼ã€‚FSDP2ä½¿ç”¨äº†DTensorä½œä¸ºå‚æ•°çš„åˆ†å¸ƒå¼è¡¨è¾¾ï¼Œæ¯ä¸ªrankåªæŒæœ‰å®Œæ•´Tensorçš„å±€éƒ¨shardã€‚
DTensorç›¸å…³ä»‹ç»è§ä¸“æ æ–‡ç«  PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»
DTensorå–ä»£çš„æ˜¯FSDP1 ä½¿ç”¨çš„FlatParameterå‚æ•°åˆ‡å‰²æ–¹å¼ï¼Œä¼šæŠŠæ‰€æœ‰è¦åˆ†å—çš„å‚æ•°æ‹¼æ¥æˆä¸€ä¸ªå·¨å¤§çš„Tensorï¼Œæ ¹æ®è¦åˆ†å—çš„å¤§å°å¯¹å‚æ•°è¿›è¡Œpaddingï¼Œä¹‹åæŒ‰å¤§å°åˆ†é…ç»™å„ä¸ªrankï¼ŒåŒæ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šéšå‚æ•°åˆ‡åˆ†è€Œåˆ‡åˆ†ã€‚</p><blockquote><p>To shard the FlatParameter, FSDP divides it into equal-sized chunks, where the number of chunks equals the sharding factor, and assigns one chunk per rank.</p></blockquote><h2 id=fsdp1-flatparam>FSDP1 FlatParam<a hidden class=anchor aria-hidden=true href=#fsdp1-flatparam>#</a></h2><p>FSDPçš„FlatParamåˆ†å—å¯¼è‡´æŸäº›Tensorå¯èƒ½ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒçš„rankï¼Œè€ŒæŸäº›Tensorå®Œæ•´çš„ä¿ç•™åœ¨æŸäº›rankä¸­ã€‚åŸºäºDTensorçš„åˆ‡åˆ†æ–¹å¼é€šå¸¸æ˜¯æ²¿Dim0åˆ‡åˆ†ï¼ŒFSDP2é‡‡ç”¨è¿™ç§åˆ‡åˆ†æ–¹å¼ä¿è¯æ¯ä¸ªrankéƒ½è·å¾—æ¯ä¸ªparamçš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹éƒ¨ä»½Tensorè¿›è¡Œpaddingã€‚</p><p>æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰
ç»è¿‡FSDP2çš„init_device_meshï¼Œç”Ÿæˆåˆ†å¸ƒå¼çš„åˆ†å—ä¿¡æ¯ã€‚DTensoråœ¨è¿›è¡Œshardingæ“ä½œæ—¶ä¼šä¿ç•™è¿™äº›åˆ†å—ä¿¡æ¯ï¼ŒFSDP2ä¸­ï¼Œåœ¨FSDPParamä¸­è¿›è¡Œç»´æŠ¤ã€‚åœ¨è¿›è¡Œé€šä¿¡æ“ä½œï¼Œå¦‚All-reduceæˆ–reduce-scatteræ—¶ï¼ŒDTensorä¸­å‚¨å­˜çš„åˆ†å—ä¿¡æ¯ä¼šè¢«è¯»å–ï¼Œä»¥å®Œæˆå‚æ•°çš„shardå’Œunshardæ“ä½œã€‚
æ‰“å°fsdpå‰åçš„å‚æ•°ä¿¡æ¯ï¼Œå¯ä»¥å¾—çŸ¥shardingå‰gradå±äº&lt;class &rsquo;torch.Tensor&rsquo;>ï¼Œè€Œshardingåè½¬å˜æˆäº†&lt;class &rsquo;torch.distributed.tensor.Tensor&rsquo;>
Streamã€Eventæ§åˆ¶</p><blockquote><p>Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.
é€šè¿‡é¿å…recordStreamï¼Œæ— éœ€CPUåŒæ­¥é™ä½äº†GPUæ˜¾å­˜å ç”¨</p></blockquote><p>recordStreamæ˜¯torchç”¨æ¥ç®¡ç†CUDAå¼ é‡ç”Ÿå‘½å‘¨æœŸçš„æ–¹æ³•ï¼Œå› ä¸ºGPUçš„æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæ¯”å¦‚ä¸‹å‘ä¸€ä¸ªè®¡ç®—æˆ–è€…é€šä¿¡ç®—å­æ—¶ï¼ŒCPUæ“ä½œä¼šç«‹å³è¿”å›ï¼Œè€Œå®é™…çš„æ•°æ®è®¡ç®—æˆ–è€…é€šä¿¡ä»ç„¶åœ¨GPUä¸Šè¿›è¡Œï¼Œé‚£ä¹ˆæ­¤æ—¶å¦‚æœå°è¯•è¯»å†™è¿™å—æ­£åœ¨æ“ä½œçš„bufferï¼Œå°±æœ‰å¯èƒ½å¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚</p><p>update: ä¸“æ å†…å®¹recordStreamæºç è§£è¯»ä¸Šçº¿å•¦ï¼š
CUDA tensorçš„ç”Ÿå‘½å‘¨æœŸæ˜¯ç”±GPUä¸Šå†…å­˜çš„åˆ†é…åˆé‡Šæ”¾ç®¡ç†çš„ï¼ŒrecordStreamä¼šè®°å½•tensorè¢«å“ªäº›æµç»‘å®šï¼Œå¦‚æœè¦é‡Šæ”¾ä¸€ä¸ªtensorï¼Œå°±å¿…é¡»ä¿è¯ä¾èµ–è¯¥tensorçš„æµå…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼Œè€Œä¸æ˜¯ä»…ä¸æ“ä½œå®ƒçš„æµç›¸å…³ã€‚</p><p>FSDP1çš„å±€é™æ€§ï¼š
FSDP1çš„prefetchå®ç°äº†åœ¨è®¡ç®—å½“å‰å±‚å‚æ•°æ—¶ï¼Œé¢„å–ä¸‹ä¸€å±‚å‚æ•°ï¼Œä»¥å®ç°é€šä¿¡/è®¡ç®—çš„overlapï¼Œä½†æ˜¯é€šå¸¸è®¡ç®—æµå’Œé€šä¿¡æµæ˜¯ä¸¤æ¡streamï¼Œä¸”ä¸åŒstreamé—´æ˜¯ç›¸äº’å¹¶è¡Œçš„ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§æƒ…å†µï¼šå³å¦‚æœæŸæ¬¡è®¡ç®—ç”±äºè€—æ—¶è¾ƒä¹…ï¼Œä¹…åˆ°è¿ç»­ä¸‹å‘çš„å‡ ä¸ªall-gatheræ“ä½œå…¨éƒ¨å®Œæˆäº†ï¼Œè®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œæ­¤æ—¶æ˜¾å­˜å°±ä¼šè¢«é€šä¿¡åçš„å‚æ•°ä¸æ–­å æ®ï¼Œç”šè‡³ä¼šå‡ºç°oomã€‚</p><p>FSDP1å¯¹è¿™ä¸ªé—®é¢˜çš„è§£æ³•ä¹Ÿååˆ†ç²—æš´ï¼Œå³ï¼Œè®¾ç½®äº†ä¸€ä¸ªrate_limiter,å³é€Ÿç‡æ§åˆ¶å™¨ï¼Œåœ¨æ¯ä¸ªTransformerBlockåé˜»å¡CPUï¼Œrate_limiterä¸­çš„é™åˆ¶é»˜è®¤ä¸º2ï¼Œä¿è¯åªæœ‰å½“ä¸Šä¸€å±‚ç»“æŸæ—¶ï¼Œæ‰ä¼šå¯¹ä¸‹ä¸€å±‚è¿›è¡Œall-gatheræ“ä½œã€‚
æºç ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>_FreeEventQueue</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    This tracks all pending frees corresponding to inflight all-gathers. The
</span></span></span><span class=line><span class=cl><span class=s2>    queueing pattern is iterative enqueues with a single dequeue per iteration
</span></span></span><span class=line><span class=cl><span class=s2>    once the limit ``_max_num_inflight_all_gathers`` is reached.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_queue</span><span class=p>:</span> <span class=n>collections</span><span class=o>.</span><span class=n>deque</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Event</span><span class=p>]</span> <span class=o>=</span> <span class=n>collections</span><span class=o>.</span><span class=n>deque</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_max_num_inflight_all_gathers</span> <span class=o>=</span> <span class=mi>2</span>  <span class=c1># empirically chosen</span>
</span></span></code></pre></div><p>å…³äºFSDP1è¿˜æœ‰ä¸€ä¸ªè®¨è®ºï¼Œé€šè¿‡recordStreamæ§åˆ¶ï¼Œå‡è®¾å­˜åœ¨ä¸¤ä¸ªæµstreamAå’ŒstreamBï¼Œå½“streamAæŒæœ‰æŸä¸ªTensoræ—¶ï¼Œç›´åˆ°è¿™æ¡steamA å®Œå…¨ç»“æŸæ—¶ï¼Œç›¸åº”çš„Tensoræ‰ä¼šè¢«é‡Šæ”¾ï¼Œå› æ­¤ï¼Œå³ä½¿è¿™ä¸ªTensorç”Ÿå‘½å‘¨æœŸç†è®ºå·²ç»ç»“æŸäº†ï¼Œä½†streamAæ²¡æœ‰é€€å‡ºï¼Œæ­¤æ—¶å¦‚æœåœ¨streamBä¸Šå°è¯•mallocä¸€å—å†…å­˜ï¼ŒcachingAllocatorä¼šåˆ¤æ–­æ­¤æ—¶æ²¡æœ‰ç©ºé—²å†…å­˜ï¼Œè€Œå»é‡æ–°åˆ†é…ç©ºé—´ã€‚è¿™å°±å¯¼è‡´äº†1. å†…å­˜å­˜åœ¨å³°å€¼ 2. ç”±äºåå¤mallocï¼Œæ€§èƒ½å˜å·®ã€‚ç¤¾åŒºè®¨è®ºçš„å†…å­˜å³°å€¼å¦‚å›¾ï¼š</p><p>æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰
åŸºæœ¬åŸç†å¦‚å›¾æ‰€ç¤ºï¼šå½“åœ¨å°è¯•delä¸€ä¸ªæ­£åœ¨ä½¿ç”¨çš„tensoråï¼Œå¦‚æœä½¿ç”¨è¿‡è¯¥tensorçš„æµè¿˜æœªç»“æŸï¼Œé‚£ä¹ˆæ­¤æ—¶çš„mallocæ“ä½œæ— æ³•å¤ç”¨å†…å­˜ï¼Œå¦‚å›¾ä¸Šçš„malloc Cï¼Œå¦‚æœåœ¨æµç»“æŸä¹‹åï¼Œå¦‚malloc Eï¼Œæ‰å¯ä»¥é¡ºåˆ©å¤ç”¨Açš„å†…å­˜ã€‚</p><p>æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰
æ”¹è¿›çš„ç‚¹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒæ­¥ä¸è¦å†å‘ç”Ÿåœ¨CPUä¸Šäº†ï¼Œè€Œé€šè¿‡åœ¨streamåæ’å…¥æ ‡è®°æ¥è®°å½•æ“ä½œæ˜¯å¦å®Œæˆï¼Œé€šä¿¡æµä¼šç­‰å¾…ä¾èµ–çš„è®¡ç®—æµï¼Œè€Œè®¡ç®—æµä¹Ÿä¼šç­‰å¾…ä¾èµ–çš„é€šä¿¡æµã€‚ç†è§£ä¸€ä¸‹è¿™å¼ å›¾ï¼Œä»¥iä¸ºä¾‹ï¼Œè®¡ç®—layer iæ—¶ï¼Œé¢„å–äº†i+1ï¼Œi+1çš„é€šä¿¡ä¼šäº§ç”Ÿä¸€ä¸ªevent iï¼Œè®¡ç®—æµä¼šå»waitè¿™ä¸ªeventï¼Œç›´åˆ°ç»“æŸåæ‰ä¼šå»è¿›è¡Œlayer i+1çš„è®¡ç®—ã€‚</p><p>æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰
ç°åœ¨CPUä¸éœ€è¦å†è¿›è¡Œç­‰å¾…ï¼Œä¹Ÿä¸ä¼šè¢«rate_limiteré˜»å¡ï¼Œç›¸åº”çš„é˜»å¡è¢«è½¬ç§»åˆ°äº†å¯¹åº”çš„æµä¸Šã€‚
è§‰å¾—æœ‰ç”¨ç‚¹ä¸ªèµå’Œå…³æ³¨ï¼Œtorchæœºåˆ¶æŒç»­æ›´æ–°ä¸­ğŸ’•</p><p>Reference</p><p><a href=https://github.com/pytorch/pytorch/issues/114299>https://github.com/pytorch/pytorch/issues/114299</a>
<a href=https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486>https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486</a>
<a href=https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html>https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html</a>
<a href=https://arxiv.org/pdf/2304.11277>https://arxiv.org/pdf/2304.11277</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/tech/>Tech</a></li><li><a href=http://localhost:1313/tags/pytorch/>Pytorch</a></li><li><a href=http://localhost:1313/tags/fsdp2/>FSDP2</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/test/><span class=title>Â« Prev</span><br><span>Hugoéƒ¨ç½²github Pages</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>YuZhouye's Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>