<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PyTorch pr(1)之torch.accelerator | YuZhouye's Blog</title><meta name=keywords content="Tech,Pytorch,Pr解读"><meta name=description content='Add unified memory APIs for torch.accelerator #152932
https://github.com/pytorch/pytorch/pull/152932
为torch.accelerator增加了如下API：

empty_cache
max_memory_allocated
max_memory_reserved
memory_allocated
memory_reserved
memory_stats
reset_accumulated_memory_stats
reset_peak_memory_stats

这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：

Tensors and Dynamic neural networks in Python with strong GPU acceleration

这些用法很长一段时间是为cuda提供的，调用方式主要为torch.cuda.empty_cache等，接口主要作用是查看device上的显存占用如memory_allocated和memory_reserved，以及清理和重置相关状态。
一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从memory_stats这个字典内提取对于的value
def memory_reserved(device_index: _device_t = None, /) -> int:
    r"""Return the current :ref:`accelerator<accelerators>` device memory managed by the caching allocator
    in bytes for a given device index.
    Args:
        device_index (:class:`torch.device`, str, int, optional): the index of the device to target.
            If not given, use :func:`torch.accelerator.current_device_index` by default.
            If a :class:`torch.device` or str is provided, its type must match the current
            :ref:`accelerator<accelerators>` device type.
    """
    return memory_stats(device_index).get("reserved_bytes.all.current", 0)
而memory_stas通过底层_accelerator_getDeviceStats接口获取相关device的状态'><meta name=author content="YuZhouye"><link rel=canonical href=https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://zhouyeyu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zhouyeyu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zhouyeyu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zhouyeyu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zhouyeyu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/"><meta property="og:site_name" content="YuZhouye's Blog"><meta property="og:title" content="PyTorch pr(1)之torch.accelerator"><meta property="og:description" content='Add unified memory APIs for torch.accelerator #152932 https://github.com/pytorch/pytorch/pull/152932
为torch.accelerator增加了如下API：
empty_cache max_memory_allocated max_memory_reserved memory_allocated memory_reserved memory_stats reset_accumulated_memory_stats reset_peak_memory_stats 这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：
Tensors and Dynamic neural networks in Python with strong GPU acceleration
这些用法很长一段时间是为cuda提供的，调用方式主要为torch.cuda.empty_cache等，接口主要作用是查看device上的显存占用如memory_allocated和memory_reserved，以及清理和重置相关状态。 一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从memory_stats这个字典内提取对于的value
def memory_reserved(device_index: _device_t = None, /) -> int: r"""Return the current :ref:`accelerator<accelerators>` device memory managed by the caching allocator in bytes for a given device index. Args: device_index (:class:`torch.device`, str, int, optional): the index of the device to target. If not given, use :func:`torch.accelerator.current_device_index` by default. If a :class:`torch.device` or str is provided, its type must match the current :ref:`accelerator<accelerators>` device type. """ return memory_stats(device_index).get("reserved_bytes.all.current", 0) 而memory_stas通过底层_accelerator_getDeviceStats接口获取相关device的状态'><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-07T20:44:02+08:00"><meta property="article:modified_time" content="2025-12-07T20:44:02+08:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Pr解读"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch pr(1)之torch.accelerator"><meta name=twitter:description content='Add unified memory APIs for torch.accelerator #152932
https://github.com/pytorch/pytorch/pull/152932
为torch.accelerator增加了如下API：

empty_cache
max_memory_allocated
max_memory_reserved
memory_allocated
memory_reserved
memory_stats
reset_accumulated_memory_stats
reset_peak_memory_stats

这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：

Tensors and Dynamic neural networks in Python with strong GPU acceleration

这些用法很长一段时间是为cuda提供的，调用方式主要为torch.cuda.empty_cache等，接口主要作用是查看device上的显存占用如memory_allocated和memory_reserved，以及清理和重置相关状态。
一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从memory_stats这个字典内提取对于的value
def memory_reserved(device_index: _device_t = None, /) -> int:
    r"""Return the current :ref:`accelerator<accelerators>` device memory managed by the caching allocator
    in bytes for a given device index.
    Args:
        device_index (:class:`torch.device`, str, int, optional): the index of the device to target.
            If not given, use :func:`torch.accelerator.current_device_index` by default.
            If a :class:`torch.device` or str is provided, its type must match the current
            :ref:`accelerator<accelerators>` device type.
    """
    return memory_stats(device_index).get("reserved_bytes.all.current", 0)
而memory_stas通过底层_accelerator_getDeviceStats接口获取相关device的状态'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zhouyeyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch pr(1)之torch.accelerator","item":"https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch pr(1)之torch.accelerator","name":"PyTorch pr(1)之torch.accelerator","description":"Add unified memory APIs for torch.accelerator #152932 https://github.com/pytorch/pytorch/pull/152932\n为torch.accelerator增加了如下API：\nempty_cache max_memory_allocated max_memory_reserved memory_allocated memory_reserved memory_stats reset_accumulated_memory_stats reset_peak_memory_stats 这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：\nTensors and Dynamic neural networks in Python with strong GPU acceleration\n这些用法很长一段时间是为cuda提供的，调用方式主要为torch.cuda.empty_cache等，接口主要作用是查看device上的显存占用如memory_allocated和memory_reserved，以及清理和重置相关状态。 一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从memory_stats这个字典内提取对于的value\ndef memory_reserved(device_index: _device_t = None, /) -\u0026gt; int: r\u0026#34;\u0026#34;\u0026#34;Return the current :ref:`accelerator\u0026lt;accelerators\u0026gt;` device memory managed by the caching allocator in bytes for a given device index. Args: device_index (:class:`torch.device`, str, int, optional): the index of the device to target. If not given, use :func:`torch.accelerator.current_device_index` by default. If a :class:`torch.device` or str is provided, its type must match the current :ref:`accelerator\u0026lt;accelerators\u0026gt;` device type. \u0026#34;\u0026#34;\u0026#34; return memory_stats(device_index).get(\u0026#34;reserved_bytes.all.current\u0026#34;, 0) 而memory_stas通过底层_accelerator_getDeviceStats接口获取相关device的状态\n","keywords":["Tech","Pytorch","Pr解读"],"articleBody":"Add unified memory APIs for torch.accelerator #152932 https://github.com/pytorch/pytorch/pull/152932\n为torch.accelerator增加了如下API：\nempty_cache max_memory_allocated max_memory_reserved memory_allocated memory_reserved memory_stats reset_accumulated_memory_stats reset_peak_memory_stats 这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：\nTensors and Dynamic neural networks in Python with strong GPU acceleration\n这些用法很长一段时间是为cuda提供的，调用方式主要为torch.cuda.empty_cache等，接口主要作用是查看device上的显存占用如memory_allocated和memory_reserved，以及清理和重置相关状态。 一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从memory_stats这个字典内提取对于的value\ndef memory_reserved(device_index: _device_t = None, /) -\u003e int: r\"\"\"Return the current :ref:`accelerator` device memory managed by the caching allocator in bytes for a given device index. Args: device_index (:class:`torch.device`, str, int, optional): the index of the device to target. If not given, use :func:`torch.accelerator.current_device_index` by default. If a :class:`torch.device` or str is provided, its type must match the current :ref:`accelerator` device type. \"\"\" return memory_stats(device_index).get(\"reserved_bytes.all.current\", 0) 而memory_stas通过底层_accelerator_getDeviceStats接口获取相关device的状态\nstats = torch._C._accelerator_getDeviceStats(device_index) torch._C._accelerator_getDeviceStats是一个pybind绑定的C接口函数 at::accelerator::getDeviceStats(device_index); 继续看调用栈\nTORCH_API inline at::CachingDeviceAllocator::DeviceStats getDeviceStats( c10::DeviceIndex device_index) { const auto device_type = getAccelerator(true).value(); return at::getDeviceAllocator(device_type)-\u003egetDeviceStats(device_index); } 以cuda为例\ninline c10::CachingDeviceAllocator::DeviceStats getDeviceStats( c10::DeviceIndex device) { return get()-\u003egetDeviceStats(device); } 其中get方法来自allocator，allocator是CUDA Cacing Allocator中的单例实现，所有方法都需要通过该接口实现\ninline CUDAAllocator* get() { return allocator.load(); } cuda的getDeviceStats通过下面代码实现\nDeviceStats getDeviceStats(c10::DeviceIndex device) override { assertValidDevice(device); return device_allocator[device]-\u003egetStats(); } 总之，现在，所有加速器都可以通过统一的接口返回从PyTorch申请的内存，以及对其状态进行管理，大一统进行中……\n","wordCount":"150","inLanguage":"en","datePublished":"2025-12-07T20:44:02+08:00","dateModified":"2025-12-07T20:44:02+08:00","author":{"@type":"Person","name":"YuZhouye"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/"},"publisher":{"@type":"Organization","name":"YuZhouye's Blog","logo":{"@type":"ImageObject","url":"https://zhouyeyu.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://zhouyeyu.github.io/ accesskey=h title="YuZhouye's Blog (Alt + H)">YuZhouye's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://zhouyeyu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://zhouyeyu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zhouyeyu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://zhouyeyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://zhouyeyu.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">PyTorch pr(1)之torch.accelerator</h1><div class=post-meta><span title='2025-12-07 20:44:02 +0800 CST'>December 7, 2025</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>150 words</span>&nbsp;·&nbsp;<span>YuZhouye</span></div></header><div class=post-content><h2 id=add-unified-memory-apis-for-torchaccelerator-152932>Add unified memory APIs for torch.accelerator #152932<a hidden class=anchor aria-hidden=true href=#add-unified-memory-apis-for-torchaccelerator-152932>#</a></h2><p><a href=https://github.com/pytorch/pytorch/pull/152932>https://github.com/pytorch/pytorch/pull/152932</a></p><p>为torch.accelerator增加了如下API：</p><ul><li>empty_cache</li><li>max_memory_allocated</li><li>max_memory_reserved</li><li>memory_allocated</li><li>memory_reserved</li><li>memory_stats</li><li>reset_accumulated_memory_stats</li><li>reset_peak_memory_stats</li></ul><p>这个pr进一步说明了PyTorch正在去CUDA化，从而转变为一系列加速器提供计算框架，即使他们的github主页仍然写着：</p><blockquote><p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p></blockquote><p>这些用法很长一段时间是为cuda提供的，调用方式主要为<code>torch.cuda.empty_cache</code>等，接口主要作用是查看device上的显存占用如<code>memory_allocated</code>和<code>memory_reserved</code>，以及清理和重置相关状态。
一起来看下相关函数的调用流程，以cuda为例，从源码分析这个接口是怎么一步一步获取底层的状态的。首先所有memory相关的数据都是从<code>memory_stats</code>这个字典内提取对于的value</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>memory_reserved</span><span class=p>(</span><span class=n>device_index</span><span class=p>:</span> <span class=n>_device_t</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=o>/</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=sa>r</span><span class=s2>&#34;&#34;&#34;Return the current :ref:`accelerator&lt;accelerators&gt;` device memory managed by the caching allocator
</span></span></span><span class=line><span class=cl><span class=s2>    in bytes for a given device index.
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        device_index (:class:`torch.device`, str, int, optional): the index of the device to target.
</span></span></span><span class=line><span class=cl><span class=s2>            If not given, use :func:`torch.accelerator.current_device_index` by default.
</span></span></span><span class=line><span class=cl><span class=s2>            If a :class:`torch.device` or str is provided, its type must match the current
</span></span></span><span class=line><span class=cl><span class=s2>            :ref:`accelerator&lt;accelerators&gt;` device type.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>memory_stats</span><span class=p>(</span><span class=n>device_index</span><span class=p>)</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reserved_bytes.all.current&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></div><p>而<code>memory_stas</code>通过底层<code>_accelerator_getDeviceStats</code>接口获取相关device的状态</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>stats</span> <span class=o>=</span> <span class=n>torch</span><span class=p>.</span><span class=n>_C</span><span class=p>.</span><span class=n>_accelerator_getDeviceStats</span><span class=p>(</span><span class=n>device_index</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=p>.</span><span class=n>_C</span><span class=p>.</span><span class=n>_accelerator_getDeviceStats是一个pybind绑定的C接口函数</span>
</span></span><span class=line><span class=cl><span class=n>at</span><span class=o>::</span><span class=n>accelerator</span><span class=o>::</span><span class=n>getDeviceStats</span><span class=p>(</span><span class=n>device_index</span><span class=p>);</span>
</span></span></code></pre></div><p>继续看调用栈</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>TORCH_API</span> <span class=kr>inline</span> <span class=n>at</span><span class=o>::</span><span class=n>CachingDeviceAllocator</span><span class=o>::</span><span class=n>DeviceStats</span> <span class=n>getDeviceStats</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>c10</span><span class=o>::</span><span class=n>DeviceIndex</span> <span class=n>device_index</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=k>auto</span> <span class=n>device_type</span> <span class=o>=</span> <span class=n>getAccelerator</span><span class=p>(</span><span class=nb>true</span><span class=p>).</span><span class=n>value</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>at</span><span class=o>::</span><span class=n>getDeviceAllocator</span><span class=p>(</span><span class=n>device_type</span><span class=p>)</span><span class=o>-&gt;</span><span class=n>getDeviceStats</span><span class=p>(</span><span class=n>device_index</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>以cuda为例</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kr>inline</span> <span class=n>c10</span><span class=o>::</span><span class=n>CachingDeviceAllocator</span><span class=o>::</span><span class=n>DeviceStats</span> <span class=n>getDeviceStats</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>c10</span><span class=o>::</span><span class=n>DeviceIndex</span> <span class=n>device</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nf>get</span><span class=p>()</span><span class=o>-&gt;</span><span class=n>getDeviceStats</span><span class=p>(</span><span class=n>device</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>其中get方法来自allocator，allocator是CUDA Cacing Allocator中的单例实现，所有方法都需要通过该接口实现</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kr>inline</span> <span class=n>CUDAAllocator</span><span class=o>*</span> <span class=nf>get</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>allocator</span><span class=p>.</span><span class=n>load</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>cuda的getDeviceStats通过下面代码实现</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=n>DeviceStats</span> <span class=nf>getDeviceStats</span><span class=p>(</span><span class=n>c10</span><span class=o>::</span><span class=n>DeviceIndex</span> <span class=n>device</span><span class=p>)</span> <span class=k>override</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>assertValidDevice</span><span class=p>(</span><span class=n>device</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>device_allocator</span><span class=p>[</span><span class=n>device</span><span class=p>]</span><span class=o>-&gt;</span><span class=n>getStats</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>总之，现在，所有加速器都可以通过统一的接口返回从PyTorch申请的内存，以及对其状态进行管理，大一统进行中……</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zhouyeyu.github.io/tags/tech/>Tech</a></li><li><a href=https://zhouyeyu.github.io/tags/pytorch/>Pytorch</a></li><li><a href=https://zhouyeyu.github.io/tags/pr%E8%A7%A3%E8%AF%BB/>Pr解读</a></li></ul><nav class=paginav><a class=prev href=https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/><span class=title>« Prev</span><br><span>PyTorch FSDP2 对比 FSDP1的升级</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://zhouyeyu.github.io/>YuZhouye's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>