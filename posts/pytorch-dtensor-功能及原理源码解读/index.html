<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯» | YuZhouye's Blog</title><meta name=keywords content="Tech,Pytorch,DTensor"><meta name=description content='ç®€ä»‹
PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚
DTensoråŸºæœ¬ç”¨æ³•
åŸºæœ¬å±æ€§
DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial


Replicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›


Shardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›


Partialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€


ä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š

torchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š


Shard(dim) -> Replicate(): all_gather


Shard(src_dim) -> Shard(dst_dim): all_to_all


Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)


Partial() -> Replicate(): all_reduce


Partial() -> Shard(dim): reduce_scatter


APIï¼š
ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š

distribute_tensor() æ¥å£

local_tensor = torch.randn(8, 16, device=f"cuda:{rank}")

dtensor_shard = distribute_tensor(
    local_tensor,
    device_mesh=mesh,
    placements=[Shard(0)],
)

from local tensor

local_tensor = torch.tensor([[1, 2], [3, 4]])
dtensor_shard = DTensor(
    local_tensor,
    device_mesh=dist.DeviceMesh("cuda", list(range(world_size))),
    placements=[dist.Placement("shard", dim=0)]  # æ²¿ç¬¬0ç»´åˆ†ç‰‡
)
æˆ–è€…
local_tensor = torch.tensor([1, 2, 3])
dtensor = local_tensor.to_dtensor(
    device_mesh=dist.DeviceMesh("cuda", [0,1]),
    placements=[dist.Placement("replicate")]
)

å·¥å‚å‡½æ•°

dtensor_ones = DTensor.ones(
    (2, 4),
    device_mesh=dist.DeviceMesh("cuda", list(range(world_size))),
    placements=[dist.Placement("shard", dim=1)]
)
åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚'><meta name=author content="YuZhouye"><link rel=canonical href=https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://zhouyeyu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zhouyeyu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zhouyeyu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zhouyeyu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zhouyeyu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"><meta property="og:site_name" content="YuZhouye's Blog"><meta property="og:title" content="PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»"><meta property="og:description" content='ç®€ä»‹ PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚
DTensoråŸºæœ¬ç”¨æ³• åŸºæœ¬å±æ€§ DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial
Replicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›
Shardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›
Partialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€
ä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š
torchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š
Shard(dim) -> Replicate(): all_gather
Shard(src_dim) -> Shard(dst_dim): all_to_all
Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)
Partial() -> Replicate(): all_reduce
Partial() -> Shard(dim): reduce_scatter
APIï¼š ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š
distribute_tensor() æ¥å£ local_tensor = torch.randn(8, 16, device=f"cuda:{rank}") dtensor_shard = distribute_tensor( local_tensor, device_mesh=mesh, placements=[Shard(0)], ) from local tensor local_tensor = torch.tensor([[1, 2], [3, 4]]) dtensor_shard = DTensor( local_tensor, device_mesh=dist.DeviceMesh("cuda", list(range(world_size))), placements=[dist.Placement("shard", dim=0)] # æ²¿ç¬¬0ç»´åˆ†ç‰‡ ) æˆ–è€… local_tensor = torch.tensor([1, 2, 3]) dtensor = local_tensor.to_dtensor( device_mesh=dist.DeviceMesh("cuda", [0,1]), placements=[dist.Placement("replicate")] ) å·¥å‚å‡½æ•° dtensor_ones = DTensor.ones( (2, 4), device_mesh=dist.DeviceMesh("cuda", list(range(world_size))), placements=[dist.Placement("shard", dim=1)] ) åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚'><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-23T22:37:00+08:00"><meta property="article:modified_time" content="2025-12-23T22:37:00+08:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="DTensor"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»"><meta name=twitter:description content='ç®€ä»‹
PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚
DTensoråŸºæœ¬ç”¨æ³•
åŸºæœ¬å±æ€§
DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial


Replicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›


Shardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›


Partialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€


ä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š

torchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š


Shard(dim) -> Replicate(): all_gather


Shard(src_dim) -> Shard(dst_dim): all_to_all


Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)


Partial() -> Replicate(): all_reduce


Partial() -> Shard(dim): reduce_scatter


APIï¼š
ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š

distribute_tensor() æ¥å£

local_tensor = torch.randn(8, 16, device=f"cuda:{rank}")

dtensor_shard = distribute_tensor(
    local_tensor,
    device_mesh=mesh,
    placements=[Shard(0)],
)

from local tensor

local_tensor = torch.tensor([[1, 2], [3, 4]])
dtensor_shard = DTensor(
    local_tensor,
    device_mesh=dist.DeviceMesh("cuda", list(range(world_size))),
    placements=[dist.Placement("shard", dim=0)]  # æ²¿ç¬¬0ç»´åˆ†ç‰‡
)
æˆ–è€…
local_tensor = torch.tensor([1, 2, 3])
dtensor = local_tensor.to_dtensor(
    device_mesh=dist.DeviceMesh("cuda", [0,1]),
    placements=[dist.Placement("replicate")]
)

å·¥å‚å‡½æ•°

dtensor_ones = DTensor.ones(
    (2, 4),
    device_mesh=dist.DeviceMesh("cuda", list(range(world_size))),
    placements=[dist.Placement("shard", dim=1)]
)
åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zhouyeyu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»","item":"https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»","name":"PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»","description":"ç®€ä»‹ PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚\nDTensoråŸºæœ¬ç”¨æ³• åŸºæœ¬å±æ€§ DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial\nReplicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›\nShardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›\nPartialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€\nä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š\ntorchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š\nShard(dim) -\u0026gt; Replicate(): all_gather\nShard(src_dim) -\u0026gt; Shard(dst_dim): all_to_all\nReplicate() -\u0026gt; Shard(dim): local chunking (i.e. torch.chunk)\nPartial() -\u0026gt; Replicate(): all_reduce\nPartial() -\u0026gt; Shard(dim): reduce_scatter\nAPIï¼š ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š\ndistribute_tensor() æ¥å£ local_tensor = torch.randn(8, 16, device=f\u0026#34;cuda:{rank}\u0026#34;) dtensor_shard = distribute_tensor( local_tensor, device_mesh=mesh, placements=[Shard(0)], ) from local tensor local_tensor = torch.tensor([[1, 2], [3, 4]]) dtensor_shard = DTensor( local_tensor, device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, list(range(world_size))), placements=[dist.Placement(\u0026#34;shard\u0026#34;, dim=0)] # æ²¿ç¬¬0ç»´åˆ†ç‰‡ ) æˆ–è€… local_tensor = torch.tensor([1, 2, 3]) dtensor = local_tensor.to_dtensor( device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, [0,1]), placements=[dist.Placement(\u0026#34;replicate\u0026#34;)] ) å·¥å‚å‡½æ•° dtensor_ones = DTensor.ones( (2, 4), device_mesh=dist.DeviceMesh(\u0026#34;cuda\u0026#34;, list(range(world_size))), placements=[dist.Placement(\u0026#34;shard\u0026#34;, dim=1)] ) åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚\n","keywords":["Tech","Pytorch","DTensor"],"articleBody":"ç®€ä»‹ PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚\nDTensoråŸºæœ¬ç”¨æ³• åŸºæœ¬å±æ€§ DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial\nReplicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›\nShardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›\nPartialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€\nä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š\ntorchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š\nShard(dim) -\u003e Replicate(): all_gather\nShard(src_dim) -\u003e Shard(dst_dim): all_to_all\nReplicate() -\u003e Shard(dim): local chunking (i.e. torch.chunk)\nPartial() -\u003e Replicate(): all_reduce\nPartial() -\u003e Shard(dim): reduce_scatter\nAPIï¼š ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š\ndistribute_tensor() æ¥å£ local_tensor = torch.randn(8, 16, device=f\"cuda:{rank}\") dtensor_shard = distribute_tensor( local_tensor, device_mesh=mesh, placements=[Shard(0)], ) from local tensor local_tensor = torch.tensor([[1, 2], [3, 4]]) dtensor_shard = DTensor( local_tensor, device_mesh=dist.DeviceMesh(\"cuda\", list(range(world_size))), placements=[dist.Placement(\"shard\", dim=0)] # æ²¿ç¬¬0ç»´åˆ†ç‰‡ ) æˆ–è€… local_tensor = torch.tensor([1, 2, 3]) dtensor = local_tensor.to_dtensor( device_mesh=dist.DeviceMesh(\"cuda\", [0,1]), placements=[dist.Placement(\"replicate\")] ) å·¥å‚å‡½æ•° dtensor_ones = DTensor.ones( (2, 4), device_mesh=dist.DeviceMesh(\"cuda\", list(range(world_size))), placements=[dist.Placement(\"shard\", dim=1)] ) åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚\nåŸç† ä¸€ä¸ªæºå¸¦DTensorçš„ç®—å­è°ƒç”¨æ—¶ï¼Œä¼šå…ˆè¢«DTensorä¸­å®šä¹‰çš„__torch_dispatch__ç»™Hookä½ï¼Œä¼˜å…ˆæ‰§è¡ŒDTensorå®šä¹‰çš„Dispatchå†…çš„é€»è¾‘ï¼›\ndef __torch_dispatch__(cls, func, types, args=(), kwargs=None): # type: ignore[override] return DTensor._op_dispatcher.dispatch( func, args, kwargs or {}, ) è¿›å…¥Dispatheråï¼Œé€šè¿‡è¿™è¡Œä»£ç è§£åŒ…å‡ºop_infoï¼ŒåŒ…å«device_meshã€_local_tensorã€_specç­‰åˆ†å¸ƒå¼ä¿¡æ¯\nop_info = self.unwrap_to_op_info(op_call, args, kwargs) åŸºäºåˆ†å¸ƒå¼ä¿¡æ¯ï¼Œæ¨å¯¼shardè§„åˆ™ï¼Œå³ä»¥å½“å‰è¾“å…¥ï¼Œè¾“å‡ºçš„placementä¼šæ˜¯ä»€ä¹ˆï¼Œè¿™ä¸ªæ¨å¯¼æœ‰ä¸€å®šçš„ä¼˜å…ˆçº§ï¼Œæºç ä¸­æ˜¯è¿™æ ·æè¿°çš„ï¼š\nMain dispatching logic. Follows precedence order: (1) custom_op_handler (2) registered sharding strategy, then rule (3) composite implicit autograd decomposition é™¤äº†è‡ªå®šä¹‰handlerï¼Œä¼˜å…ˆçº§æ˜¯strategyç„¶åæ˜¯ruleï¼Œå–å†³äºç®—å­çš„åˆ†å¸ƒå¼æ³¨å†Œæ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å›é€€åˆ°compositeå®ç°ã€‚\npytorché€šè¿‡@register_op_strategyæ³¨å†Œç”Ÿæˆçš„æ˜¯strategyï¼Œä¼šè®¡ç®—é€šä¿¡costï¼Œä¼˜å…ˆçº§æ›´é«˜ï¼Œé€šè¿‡@register_prop_ruleæ³¨å†Œçš„æ˜¯ruleï¼Œä¼˜å…ˆçº§è¾ƒä½ã€‚\nåŸºäºåˆ†å¸ƒå¼ä¿¡æ¯å’Œplacementè¿›è¡Œè®¡ç®—ï¼Œå¹¶åœ¨è®¡ç®—å‰å®Œæˆç›¸åº”çš„é€šä¿¡æ“ä½œï¼Œå¦‚all-gatherç­‰ã€‚åœ¨å®Œæˆè®¡ç®—åï¼Œä¼šæ ¹æ®æ˜¯å¦ä»…ä¿ç•™local_tensorå†æ¬¡è¿›è¡Œç›¸åº”çš„é€šä¿¡æ“ä½œã€‚\né¢˜å¤–è¯ ä¸ºä»€ä¹ˆ__torch_dispatch__å¯ä»¥å®ç°hookï¼Œåœ¨DTensoræ‰§è¡Œå‰é¡ºåˆ©æ¥ç®¡ï¼Ÿ pytorchä¼šcheckè¾“å…¥çš„å¯¹è±¡ä¸­æ˜¯å¦æœ‰__torch_dispatch__è¿™ä¸ªattr\nstatic bool check_has_torch_dispatch(PyObject* obj) { PyTypeObject* tp = Py_TYPE(obj); if (THPVariable_CheckTypeExact(tp)) { return false; } py::object attr = PyObject_FastGetAttrString(obj, \"__torch_dispatch__\"); return ( attr.ptr() != nullptr \u0026\u0026 attr.ptr() != torch::disabled_torch_dispatch_impl()); } å¦‚æœæœ‰å°±ä¼šå°è¯•å»ä¸ºå…¶å¢åŠ pythonè¿™ä¸ªdispatch key\nif (has_torch_dispatch_if_known.has_value() ? *has_torch_dispatch_if_known : check_has_torch_dispatch(obj)) { var.unsafeGetTensorImpl()-\u003eset_python_dispatch(true); è€Œpython keyçš„ä¼˜å…ˆçº§æ¯”cudaç­‰dence keyçš„ä¼˜å…ˆçº§æ›´é«˜ï¼Œä¼šå…ˆè¢«è°ƒç”¨ï¼Œå› æ­¤å¯ä»¥å®ç°è¿™ä¸ªhook æŒ–ä¸ªå‘ğŸ•³ï¸ï¼Œåé¢æœ‰ç©ºæ¢³ç†ä¸‹Dispatchä¼˜å…ˆçº§çš„å†…å®¹\nReference https://docs.pytorch.org/docs/stable/distributed.tensor.html\n","wordCount":"199","inLanguage":"en","datePublished":"2025-12-23T22:37:00+08:00","dateModified":"2025-12-23T22:37:00+08:00","author":{"@type":"Person","name":"YuZhouye"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"},"publisher":{"@type":"Organization","name":"YuZhouye's Blog","logo":{"@type":"ImageObject","url":"https://zhouyeyu.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://zhouyeyu.github.io/ accesskey=h title="YuZhouye's Blog (Alt + H)">YuZhouye's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://zhouyeyu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://zhouyeyu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://zhouyeyu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://zhouyeyu.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»</h1><div class=post-meta><span title='2025-12-23 22:37:00 +0800 CST'>December 23, 2025</span>&nbsp;Â·&nbsp;<span>1 min</span>&nbsp;Â·&nbsp;<span>199 words</span>&nbsp;Â·&nbsp;<span>YuZhouye</span>&nbsp;Â·&nbsp;<span id=busuanzi_container_page_pv style=display:none>è®¿é—®é‡: <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-content><h2 id=ç®€ä»‹>ç®€ä»‹<a hidden class=anchor aria-hidden=true href=#ç®€ä»‹>#</a></h2><p>PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚</p><h2 id=dtensoråŸºæœ¬ç”¨æ³•>DTensoråŸºæœ¬ç”¨æ³•<a hidden class=anchor aria-hidden=true href=#dtensoråŸºæœ¬ç”¨æ³•>#</a></h2><h3 id=åŸºæœ¬å±æ€§>åŸºæœ¬å±æ€§<a hidden class=anchor aria-hidden=true href=#åŸºæœ¬å±æ€§>#</a></h3><p>DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial</p><ul><li><p>Replicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›</p></li><li><p>Shardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›</p></li><li><p>Partialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€</p></li></ul><p>ä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š</p><p><img alt="Image Alt Text" loading=lazy src=/DTensor/DTensor_01.jpg></p><p>torchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š</p><ul><li><p>Shard(dim) -> Replicate(): all_gather</p></li><li><p>Shard(src_dim) -> Shard(dst_dim): all_to_all</p></li><li><p>Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)</p></li><li><p>Partial() -> Replicate(): all_reduce</p></li><li><p>Partial() -> Shard(dim): reduce_scatter</p></li></ul><h3 id=api>APIï¼š<a hidden class=anchor aria-hidden=true href=#api>#</a></h3><p>ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š</p><ol><li><code>distribute_tensor()</code> æ¥å£</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>local_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;cuda:</span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dtensor_shard</span> <span class=o>=</span> <span class=n>distribute_tensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>local_tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_mesh</span><span class=o>=</span><span class=n>mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>placements</span><span class=o>=</span><span class=p>[</span><span class=n>Shard</span><span class=p>(</span><span class=mi>0</span><span class=p>)],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><ol start=2><li><code>from local tensor</code></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>local_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>dtensor_shard</span> <span class=o>=</span> <span class=n>DTensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>local_tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_mesh</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>world_size</span><span class=p>))),</span>
</span></span><span class=line><span class=cl>    <span class=n>placements</span><span class=o>=</span><span class=p>[</span><span class=n>dist</span><span class=o>.</span><span class=n>Placement</span><span class=p>(</span><span class=s2>&#34;shard&#34;</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)]</span>  <span class=c1># æ²¿ç¬¬0ç»´åˆ†ç‰‡</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>æˆ–è€…</span>
</span></span><span class=line><span class=cl><span class=n>local_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>dtensor</span> <span class=o>=</span> <span class=n>local_tensor</span><span class=o>.</span><span class=n>to_dtensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>device_mesh</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>placements</span><span class=o>=</span><span class=p>[</span><span class=n>dist</span><span class=o>.</span><span class=n>Placement</span><span class=p>(</span><span class=s2>&#34;replicate&#34;</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><ol start=3><li>å·¥å‚å‡½æ•°</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>dtensor_ones</span> <span class=o>=</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>device_mesh</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>world_size</span><span class=p>))),</span>
</span></span><span class=line><span class=cl>    <span class=n>placements</span><span class=o>=</span><span class=p>[</span><span class=n>dist</span><span class=o>.</span><span class=n>Placement</span><span class=p>(</span><span class=s2>&#34;shard&#34;</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚</p><h3 id=åŸç†>åŸç†<a hidden class=anchor aria-hidden=true href=#åŸç†>#</a></h3><p>ä¸€ä¸ªæºå¸¦DTensorçš„ç®—å­è°ƒç”¨æ—¶ï¼Œä¼šå…ˆè¢«DTensorä¸­å®šä¹‰çš„<code>__torch_dispatch__</code>ç»™Hookä½ï¼Œä¼˜å…ˆæ‰§è¡ŒDTensorå®šä¹‰çš„Dispatchå†…çš„é€»è¾‘ï¼›</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>__torch_dispatch__</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>func</span><span class=p>,</span> <span class=n>types</span><span class=p>,</span> <span class=n>args</span><span class=o>=</span><span class=p>(),</span> <span class=n>kwargs</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>  <span class=c1># type: ignore[override]</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>_op_dispatcher</span><span class=o>.</span><span class=n>dispatch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=n>func</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>kwargs</span> <span class=ow>or</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>       <span class=p>)</span>
</span></span></code></pre></div><p>è¿›å…¥Dispatheråï¼Œé€šè¿‡è¿™è¡Œä»£ç è§£åŒ…å‡º<code>op_info</code>ï¼ŒåŒ…å«<code>device_mesh</code>ã€<code>_local_tensor</code>ã€<code>_spec</code>ç­‰åˆ†å¸ƒå¼ä¿¡æ¯</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>op_info</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>unwrap_to_op_info</span><span class=p>(</span><span class=n>op_call</span><span class=p>,</span> <span class=n>args</span><span class=p>,</span> <span class=n>kwargs</span><span class=p>)</span>
</span></span></code></pre></div><p>åŸºäºåˆ†å¸ƒå¼ä¿¡æ¯ï¼Œæ¨å¯¼shardè§„åˆ™ï¼Œå³ä»¥å½“å‰è¾“å…¥ï¼Œè¾“å‡ºçš„placementä¼šæ˜¯ä»€ä¹ˆï¼Œè¿™ä¸ªæ¨å¯¼æœ‰ä¸€å®šçš„ä¼˜å…ˆçº§ï¼Œæºç ä¸­æ˜¯è¿™æ ·æè¿°çš„ï¼š</p><pre tabindex=0><code>        Main dispatching logic.  Follows precedence order:
        (1) custom_op_handler
        (2) registered sharding strategy, then rule
        (3) composite implicit autograd decomposition
</code></pre><p>é™¤äº†è‡ªå®šä¹‰handlerï¼Œä¼˜å…ˆçº§æ˜¯strategyç„¶åæ˜¯ruleï¼Œå–å†³äºç®—å­çš„åˆ†å¸ƒå¼æ³¨å†Œæ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å›é€€åˆ°compositeå®ç°ã€‚</p><p>pytorché€šè¿‡<code>@register_op_strategy</code>æ³¨å†Œç”Ÿæˆçš„æ˜¯strategyï¼Œä¼šè®¡ç®—é€šä¿¡costï¼Œä¼˜å…ˆçº§æ›´é«˜ï¼Œé€šè¿‡<code>@register_prop_rule</code>æ³¨å†Œçš„æ˜¯ruleï¼Œä¼˜å…ˆçº§è¾ƒä½ã€‚</p><p>åŸºäºåˆ†å¸ƒå¼ä¿¡æ¯å’Œplacementè¿›è¡Œè®¡ç®—ï¼Œå¹¶åœ¨è®¡ç®—å‰å®Œæˆç›¸åº”çš„é€šä¿¡æ“ä½œï¼Œå¦‚all-gatherç­‰ã€‚åœ¨å®Œæˆè®¡ç®—åï¼Œä¼šæ ¹æ®æ˜¯å¦ä»…ä¿ç•™local_tensorå†æ¬¡è¿›è¡Œç›¸åº”çš„é€šä¿¡æ“ä½œã€‚</p><h2 id=é¢˜å¤–è¯>é¢˜å¤–è¯<a hidden class=anchor aria-hidden=true href=#é¢˜å¤–è¯>#</a></h2><p>ä¸ºä»€ä¹ˆ<code>__torch_dispatch__</code>å¯ä»¥å®ç°hookï¼Œåœ¨DTensoræ‰§è¡Œå‰é¡ºåˆ©æ¥ç®¡ï¼Ÿ
pytorchä¼šcheckè¾“å…¥çš„å¯¹è±¡ä¸­æ˜¯å¦æœ‰<code>__torch_dispatch__</code>è¿™ä¸ªattr</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>static</span> <span class=kt>bool</span> <span class=nf>check_has_torch_dispatch</span><span class=p>(</span><span class=n>PyObject</span><span class=o>*</span> <span class=n>obj</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>PyTypeObject</span><span class=o>*</span> <span class=n>tp</span> <span class=o>=</span> <span class=n>Py_TYPE</span><span class=p>(</span><span class=n>obj</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>THPVariable_CheckTypeExact</span><span class=p>(</span><span class=n>tp</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>py</span><span class=o>::</span><span class=n>object</span> <span class=n>attr</span> <span class=o>=</span> <span class=n>PyObject_FastGetAttrString</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=s>&#34;__torch_dispatch__&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>attr</span><span class=p>.</span><span class=n>ptr</span><span class=p>()</span> <span class=o>!=</span> <span class=k>nullptr</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>      <span class=n>attr</span><span class=p>.</span><span class=n>ptr</span><span class=p>()</span> <span class=o>!=</span> <span class=n>torch</span><span class=o>::</span><span class=n>disabled_torch_dispatch_impl</span><span class=p>());</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>å¦‚æœæœ‰å°±ä¼šå°è¯•å»ä¸ºå…¶å¢åŠ pythonè¿™ä¸ªdispatch key</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>has_torch_dispatch_if_known</span><span class=p>.</span><span class=n>has_value</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=o>?</span> <span class=o>*</span><span class=nl>has_torch_dispatch_if_known</span>
</span></span><span class=line><span class=cl>              <span class=p>:</span> <span class=n>check_has_torch_dispatch</span><span class=p>(</span><span class=n>obj</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span><span class=p>.</span><span class=n>unsafeGetTensorImpl</span><span class=p>()</span><span class=o>-&gt;</span><span class=n>set_python_dispatch</span><span class=p>(</span><span class=nb>true</span><span class=p>);</span>
</span></span></code></pre></div><p>è€Œpython keyçš„ä¼˜å…ˆçº§æ¯”cudaç­‰dence keyçš„ä¼˜å…ˆçº§æ›´é«˜ï¼Œä¼šå…ˆè¢«è°ƒç”¨ï¼Œå› æ­¤å¯ä»¥å®ç°è¿™ä¸ªhook
æŒ–ä¸ªå‘ğŸ•³ï¸ï¼Œåé¢æœ‰ç©ºæ¢³ç†ä¸‹Dispatchä¼˜å…ˆçº§çš„å†…å®¹</p><h3 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h3><p><a href=https://docs.pytorch.org/docs/stable/distributed.tensor.html>https://docs.pytorch.org/docs/stable/distributed.tensor.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zhouyeyu.github.io/tags/tech/>Tech</a></li><li><a href=https://zhouyeyu.github.io/tags/pytorch/>Pytorch</a></li><li><a href=https://zhouyeyu.github.io/tags/dtensor/>DTensor</a></li></ul><nav class=paginav><a class=prev href=https://zhouyeyu.github.io/posts/pytorch-record_stream-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/><span class=title>Â« Prev</span><br><span>PyTorch record_stream æºç è§£è¯»</span>
</a><a class=next href=https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/><span class=title>Next Â»</span><br><span>PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§</span></a></nav></footer><div class=comments><script>let giscusTheme=document.body.className.includes("dark")?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","zhouyeyu/zhouyeyu.github.io"),s.setAttribute("data-repo-id","R_kgDOQ7G1cA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOQ7G1cM4C1Z0P"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","top"),s.setAttribute("data-lang","zh-CN"),s.setAttribute("data-theme",giscusTheme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector(".comments").appendChild(s),document.getElementById("theme-toggle").addEventListener("click",()=>{const t=document.body.className.includes("dark")?"light":"dark",n={giscus:{setConfig:{theme:t}}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage(n,"https://giscus.app")})</script></div></article></main><footer class=footer><div style=margin-bottom:5px;font-size:13px;color:var(--secondary)><span id=busuanzi_container_site_pv style=display:none>æ€»è®¿é—®é‡ <span id=busuanzi_value_site_pv></span> æ¬¡ Â·
</span><span id=sitetime></span></div><span>&copy; 2026 <a href=https://zhouyeyu.github.io/>YuZhouye's Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>function siteTime(){var e=new Date,t=new Date("2026-01-17T00:00:00"),n=e.getTime()-t.getTime(),s=Math.floor(n/(24*3600*1e3));document.getElementById("sitetime").innerHTML="æœ¬ç«™å·²è¿è¡Œ "+s+" å¤©"}siteTime();let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t)})</script></body></html>