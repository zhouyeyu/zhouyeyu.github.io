<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>YuZhouye&#39;s Blog</title>
    <link>https://zhouyeyu.github.io/</link>
    <description>Recent content on YuZhouye&#39;s Blog</description>
    <generator>Hugo -- 0.154.5</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 21:37:02 +0800</lastBuildDate>
    <atom:link href="https://zhouyeyu.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hugoéƒ¨ç½²github Pages</title>
      <link>https://zhouyeyu.github.io/posts/test/</link>
      <pubDate>Fri, 16 Jan 2026 21:37:02 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/test/</guid>
      <description>&lt;h1 id=&#34;to-be-done&#34;&gt;To be doneğŸ˜&lt;/h1&gt;</description>
    </item>
    <item>
      <title>PyTorch pr(2) cudaGraphä¸‹å¤šæµæ˜¾å­˜å¤ç”¨</title>
      <link>https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/</link>
      <pubDate>Mon, 05 Jan 2026 23:11:11 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/pytorch-pr2-cudagraph%E4%B8%8B%E5%A4%9A%E6%B5%81%E6%98%BE%E5%AD%98%E5%A4%8D%E7%94%A8/</guid>
      <description>&lt;h2 id=&#34;cuda-reuse-blocks-with-record_stream-during-cuda-graph-capture-in-the-cudacachingallocator-158352&#34;&gt;[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator #158352&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/pull/158352&#34;&gt;https://github.com/pytorch/pytorch/pull/158352&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä¸“æ å¦ä¸€ç¯‡æ–‡ç« è§£è¯»äº†record_streamç›¸å…³å†…å®¹ï¼Œä½†CUDACachingAllocatoræºç ä¸­å­˜åœ¨ä¸€ä¸ªåˆ†æ”¯ï¼Œä¼šæ ¹æ®CUDAAllocatorConfig::graph_capture_record_stream_reuse()åˆ¤æ–­æ˜¯å¦freeå’ŒmallocæŸä¸ªblockï¼Œè¿™é‡Œå¡«ä¸ªå‘ï¼Œæ‰¾ä¸€ä¸‹å¯¹åº”çš„prçœ‹æ˜¯æ€ä¹ˆå¤ç”¨çš„ã€‚&lt;/p&gt;
&lt;h2 id=&#34;èƒŒæ™¯cuda-graph-ä¸‹æ˜¾å­˜ç®¡ç†é™åˆ¶&#34;&gt;èƒŒæ™¯ï¼šCUDA Graph ä¸‹æ˜¾å­˜ç®¡ç†é™åˆ¶&lt;/h2&gt;
&lt;p&gt;åœ¨ CUDA Graph æ•è·è¿‡ç¨‹ä¸­ï¼ŒCUDACachingAllocatorå¯¹å†…å­˜çš„ç®¡ç†å­˜åœ¨ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå¿…é¡»ç­‰åˆ°æ•è·ç»“æŸåæ‰èƒ½å›æ”¶å†…å­˜å—ã€‚è¿™ä¸€é™åˆ¶æºäº CUDA çš„åº•å±‚çº¦æŸ â€”â€” æ•è·é˜¶æ®µä¸å…è®¸æŸ¥è¯¢eventçŠ¶æ€ï¼ˆå› ä¸ºæ­¤æ—¶ CUDA æ“ä½œå°šæœªæ‰§è¡Œï¼‰ï¼Œè€ŒCUDACachingAllocatorä¾èµ–äº‹ä»¶é©±åŠ¨é€»è¾‘åˆ¤æ–­å†…å­˜å—æ˜¯å¦å¯å›æ”¶ã€‚è€Œè¿™ä¼šå¯¼è‡´æœ¬è¯¥è¢«é‡Šæ”¾çš„blockæœªèƒ½è¢«æ­£ç¡®é‡Šæ”¾ï¼Œç›´åˆ°æ•è·å®Œå…¨ç»“æŸï¼Œæ¯”å¦‚freeä¸­çš„ä»£ç ç‰‡æ®µï¼š&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream_uses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C10_UNLIKELY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;captures_underway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// It&amp;#39;s forbidden to cudaEventQuery an event recorded during CUDA graph
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// capture. We conservatively defer recording end-of-life events until
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// the next call to process_events() (which won&amp;#39;t happen until no
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;// captures are underway)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;needs_events_deferred_until_no_capture&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;push_back&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cudagraphä¸‹å®‰å…¨çš„å¤šæµå¤ç”¨&#34;&gt;cudaGraphä¸‹å®‰å…¨çš„å¤šæµå¤ç”¨&lt;/h2&gt;
&lt;p&gt;ä¸ºå®ç°å®‰å…¨é‡ç”¨ï¼ŒPR é¦–å…ˆæ˜ç¡®äº†ä¸¤ä¸ªæ ¸å¿ƒæœ¯è¯­ï¼Œä½œä¸ºåç»­åˆ¤æ–­é€»è¾‘çš„åŸºç¡€ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Free markerï¼šé€šè¿‡cudaGraphAddEmptyNodeåˆ›å»ºçš„ â€œæ•è·åˆæ³•â€ ç©ºèŠ‚ç‚¹ï¼Œæ’å…¥åˆ°æ¯ä¸ªä½¿ç”¨è¿‡è¯¥å†…å­˜å—çš„streamä¸­ï¼Œä¸”ä½äºè¯¥å—æœ€åä¸€æ¬¡è¢«ä½¿ç”¨çš„æ“ä½œä¹‹åï¼Œç”¨äºæ ‡è®° â€œå†…å­˜å—å·²ç©ºé—²â€ã€‚&lt;/li&gt;
&lt;li&gt;Terminal Nodeï¼šæµæˆ–æ•è·å›¾ä¸­ â€œæœ€æ–°æ“ä½œâ€ çš„é›†åˆï¼Œæ–°æ•è·çš„æ“ä½œä¼šé™„åŠ åœ¨ç»ˆç«¯èŠ‚ç‚¹ä¹‹åã€‚å¯¹äºæ­£åœ¨æ•è·çš„æµï¼Œå¯é€šè¿‡cudaStreamGetCaptureInfoçš„dependencies_outå‚æ•°è·å–ç»ˆç«¯èŠ‚ç‚¹é›†åˆã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;å†…å­˜å—å¯é‡ç”¨æ€§åˆ¤æ–­è§„åˆ™&#34;&gt;å†…å­˜å—å¯é‡ç”¨æ€§åˆ¤æ–­è§„åˆ™&lt;/h3&gt;
&lt;p&gt;cudaGraphç”Ÿæˆä¸€ä¸ªDAGï¼Œå› æ­¤ä½œè€…æå‡ºäº†ä¸¤ç§å¹³è¡¡ â€œå®‰å…¨æ€§â€ ä¸ â€œçµæ´»æ€§â€ çš„åˆ¤æ–­è§„åˆ™ï¼š&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorch record_stream æºç è§£è¯»</title>
      <link>https://zhouyeyu.github.io/posts/pytorch-record_stream-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 30 Dec 2025 23:41:00 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/pytorch-record_stream-%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</guid>
      <description>&lt;h2 id=&#34;æ¦‚è¿°&#34;&gt;æ¦‚è¿°&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;record_stream&lt;/code&gt;æ˜¯ PyTorch ä¸­ç”¨äºç®¡ç† CUDA æµä¹‹é—´Tensorç”Ÿå‘½å‘¨æœŸå’Œä¾èµ–å…³ç³»çš„ä¸€ä¸ªå…³é”®æœºåˆ¶ã€‚å®ƒçš„æ ¸å¿ƒç›®çš„æ˜¯ç¡®ä¿åœ¨å¼‚æ­¥æ‰§è¡Œçš„ GPU æ“ä½œä¸­ï¼Œæ•°æ®çš„ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼Œé˜²æ­¢å› æ“ä½œé‡å å¯¼è‡´çš„æ•°æ®æŸåæˆ–è®¡ç®—é”™è¯¯ã€‚&lt;/p&gt;
&lt;h3 id=&#34;ä¸ºä»€ä¹ˆéœ€è¦-record_stream&#34;&gt;ä¸ºä»€ä¹ˆéœ€è¦ record_streamï¼Ÿ&lt;/h3&gt;
&lt;p&gt;è¦ç†è§£ &lt;code&gt;record_stream&lt;/code&gt;ï¼Œé¦–å…ˆçœ‹ä¸€ä¸‹ CUDA æµçš„ç›¸å…³æ¦‚å¿µï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CUDA æµï¼šå¯ä»¥çœ‹ä½œæ˜¯ GPU ä¸Šçš„ä¸€ä¸ªâ€œä»»åŠ¡é˜Ÿåˆ—â€ã€‚æäº¤ç»™åŒä¸€ä¸ªæµçš„æ“ä½œä¼šæŒ‰ç…§å…ˆè¿›å…ˆå‡ºçš„é¡ºåºå¼‚æ­¥æ‰§è¡Œï¼›&lt;/li&gt;
&lt;li&gt;å¼‚æ­¥æ‰§è¡Œï¼šCPU å°†ä»»åŠ¡æäº¤ç»™ GPU çš„æŸä¸ªæµåï¼Œä¸ä¼šç­‰å¾… GPU å®Œæˆå°±ç›´æ¥è¿”å›ï¼Œç»§ç»­æ‰§è¡Œåç»­çš„ CPU ä»£ç ï¼›&lt;/li&gt;
&lt;li&gt;å¤šæµå¹¶è¡Œï¼šå…è®¸åˆ›å»ºå¤šä¸ªæµï¼Œè®© GPU åŒæ—¶æ‰§è¡Œæ¥è‡ªä¸åŒæµçš„ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´é«˜å±‚æ¬¡çš„å¹¶è¡Œã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ä¸ºä»€ä¹ˆä¼šå‡ºç°è®¡ç®—é”™è¯¯ä¸¾ä¸ªä¾‹å­&#34;&gt;ä¸ºä»€ä¹ˆä¼šå‡ºç°è®¡ç®—é”™è¯¯ï¼Ÿä¸¾ä¸ªä¾‹å­ï¼š&lt;/h3&gt;
&lt;p&gt;å‡è®¾Stream Aæ˜¯ä¸€ä¸ªé€šä¿¡æµï¼Œè´Ÿè´£é€šè¿‡all-gatheræ”¶é›†ä¸€ä¸ªTensorTï¼ŒStream B æ˜¯ä¸€ä¸ªè®¡ç®—æµï¼Œè´Ÿè´£ä½¿ç”¨Tensor T è¿›è¡Œè®¡ç®—ã€‚ç”±äºæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼ŒCPU åœ¨å‘èµ·æ‹·è´åç«‹åˆ»å°±å»æäº¤è®¡ç®—ä»»åŠ¡ï¼Œæ­¤æ—¶ Stream A çš„æ‹·è´æ“ä½œå¾ˆå¯èƒ½è¿˜æ²¡å®Œæˆã€‚å¦‚æœ Stream B å¼€å§‹è¯»å– Tï¼Œå®ƒå¯èƒ½ä¼šè¯»åˆ°ä¸å®Œæ•´æˆ–é”™è¯¯çš„æ•°æ®ï¼Œå¯¼è‡´è®¡ç®—ç»“æœé”™è¯¯ã€‚&lt;/p&gt;
&lt;p&gt;å†çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœCPUä¸‹å‘Tensorçš„ç›¸å…³è®¡ç®—æŒ‡ä»¤åï¼Œè®¤ä¸ºè¯¥Tensorå¯ä»¥åˆ é™¤ï¼Œæ‰§è¡Œäº†delå‘½ä»¤ï¼Œä½†ç›¸å…³kernelå†…çš„è®¡ç®—è¿˜æ²¡æœ‰å®Œæˆï¼Œä¹Ÿæœ‰å¯èƒ½å¯¼è‡´è®¡ç®—è¿‡ç¨‹ä¸­è¯»å–åˆ°é”™è¯¯çš„æ•°æ®ï¼Œä»è€Œå¯¼è‡´è®¡ç®—é”™è¯¯ã€‚&lt;/p&gt;
&lt;p&gt;å½“ä½¿ç”¨T.record_streamæ—¶ï¼Œä¼šå¼ºåˆ¶ç­‰å¾…è¯¥æµï¼Œä¹‹åå†è¿›è¡Œå…¶ä»–æµçš„æ“ä½œï¼›å°è¯•é‡Šæ”¾æ—¶ï¼Œä¹Ÿä¼šç­‰å¾…è¿™ä¸ªæµï¼Œæ­¤æ—¶ç”³è¯·çš„å†…å­˜ä¸æ˜¯çœŸæ­£çš„è¢«é‡Šæ”¾äº†ï¼Œé˜²æ­¢å‡ºç°æ•°æ®è¸©è¸æˆ–è¯»å–è„æ•°æ®ã€‚&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;record_stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;record_streamåŸç†&#34;&gt;record_streamåŸç†&lt;/h2&gt;
&lt;p&gt;æŸ¥çœ‹record_streamæºç ï¼Œå‘ç°ä¸Caching Allocatorç›¸å…³&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;record_stream_cuda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tensor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c10&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Stream&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;c10&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;StreamData3&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pack3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;c10&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CUDACachingAllocator&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;recordStream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_ptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;at&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CUDAStream&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unpack3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;record_stream&lt;/code&gt;ä¼šåœ¨Tensorå¯¹åº”çš„Blockä¸­æ’å…¥ä¸€ä¸ªeventï¼Œå¹¶æ ¹æ®ä½¿ç”¨çš„æµæ•°å¢åŠ å¼•ç”¨è®¡æ•°ï¼Œå³ä½¿ CPU ç«¯å¼•ç”¨å½’é›¶ï¼ŒCaching Allocatorä¹Ÿä¼šæ‹¦æˆªé‡Šæ”¾è¯·æ±‚ï¼Œå°†è¯¥æ˜¾å­˜å—æ ‡è®°ä¸ºâ€œæµå ç”¨â€çŠ¶æ€å¹¶ç§»å…¥å¾…å›æ”¶é˜Ÿåˆ—ï¼ŒåŒæ—¶æŒç»­è½®è¯¢è¯¥æµçš„æ‰§è¡ŒçŠ¶æ€ã€‚åªæœ‰å½“ç¡®è®¤æµä¸­æ‰€æœ‰ä»»åŠ¡å®Œå…¨ç»“æŸæ—¶ï¼ŒCaching Allocatoræ‰ä¼šçœŸæ­£å°†æ˜¾å­˜é‡Šæ”¾ï¼ˆä¹Ÿä¸æ˜¯çœŸæ­£çš„é‡Šæ”¾ï¼ŒæŒ‡å¯ä»¥è¢«å¤ç”¨ï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;æŸ¥çœ‹Caching Allocatorä¸­çš„æºç ï¼š&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;recordStream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CUDAStream&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lock_guard&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;recursive_mutex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lock&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mutex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream_uses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// è®°å½•ä½¿ç”¨è¿™å—blockçš„æµ
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;C10_UNLIKELY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;captures_underway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;block_to_cudagraph_stream_uses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;insert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;é€šè¿‡&lt;code&gt;block-&amp;gt;stream_uses.insert(stream);&lt;/code&gt; è®°å½•ä¸‹æ‰€æœ‰ä½¿ç”¨è¿™å—blockçš„æµã€‚å•å•åªæ˜¯è®°å½•å¹¶ä¸èƒ½å¼ºåˆ¶åŒæ­¥ï¼Œæ‰¾ä¸€ä¸‹æ˜¯å“ªé‡Œæ’å…¥è¿™ä¸ªeventçš„ï¼Œå‘ç°Caching Allocatorä¸­é€šè¿‡&lt;code&gt;insert_events&lt;/code&gt; å‡½æ•°å¯¹blockæ‰€åœ¨çš„æµæ’å…¥eventå¹¶å®ç°å¼•ç”¨è®¡æ•°ï¼š&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorch DTensor åŠŸèƒ½åŠåŸç†æºç è§£è¯»</title>
      <link>https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 23 Dec 2025 22:37:00 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/pytorch-dtensor-%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</guid>
      <description>&lt;h2 id=&#34;ç®€ä»‹&#34;&gt;ç®€ä»‹&lt;/h2&gt;
&lt;p&gt;PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§ä¸Šä¸€ç¯‡æ–‡ç« æ¢³ç†äº†FSDP2ï¼Œäºæ˜¯é¡ºç€æ¢³ç†ä¸‹DTensorç›¸å…³çš„å†…å®¹ã€‚åœ¨DTensorå‡ºç°ä¹‹å‰ï¼ŒPytorch ä¸­çš„åˆ†å¸ƒå¼å¼ é‡ä¸»è¦ä¾èµ–äº torch.distributed æ¨¡å—ä¸‹çš„DDPã€ FSDPåŠç¬¬ä¸‰æ–¹åº“å¦‚DeepSpeedã€Megatron-LM æä¾›çš„å¹¶è¡Œèƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ¡†æ¶å¾€å¾€éœ€è¦åœ¨ä»£ç ä¸­ä¸æ–­åœ°æ’å…¥é€šä¿¡(å¦‚all-gatherã€all-reduceç­‰)ï¼Œæœ‰äº†DTensorï¼Œé™ä½ç”¨æˆ·æ‰‹åŠ¨ç®¡ç†é€šä¿¡æ“ä½œçš„æˆæœ¬ï¼Œè¿›ä¸€æ­¥æé«˜æ˜“ç”¨æ€§ï¼Œä¹Ÿæ˜¯ä¸torchçš„ç†å¿µç›¸åŒ¹é…çš„ã€‚&lt;/p&gt;
&lt;h2 id=&#34;dtensoråŸºæœ¬ç”¨æ³•&#34;&gt;DTensoråŸºæœ¬ç”¨æ³•&lt;/h2&gt;
&lt;h3 id=&#34;åŸºæœ¬å±æ€§&#34;&gt;åŸºæœ¬å±æ€§&lt;/h3&gt;
&lt;p&gt;DTensoræ˜¯torch.Tensorçš„å­ç±»ï¼Œå› æ­¤å¯ä»¥åƒæ™®é€štensorä¸€æ ·æ“ä½œå®ƒã€‚DTensoré€šè¿‡DeviceMeshå»ºç«‹å„ä¸ªrankä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡Placementæè¿°å½“å‰çš„çŠ¶æ€ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªPlacementï¼šShardã€Replicateå’ŒPartial&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Replicateï¼šæ¯ä¸ªrankæ‹¥æœ‰ç›¸åŒçš„å®Œæ•´å‚æ•°ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shardï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½åˆ†å—åçš„å‚æ•°ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partialï¼šæ¯ä¸ªrankåªæŒæœ‰éƒ¨ä»½æ•°æ®ï¼Œç›®å‰æ˜¯ç­‰å¾…all-reduceçŠ¶æ€&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä¸‰ä¸ªå±æ€§ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Image Alt Text&#34; loading=&#34;lazy&#34; src=&#34;https://zhouyeyu.github.io/DTensor/DTensor_01.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;torchå®˜æ–¹ç½—åˆ—äº†å‡ ä¸ªè½¬æ¢å…³ç³»ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shard(dim) -&amp;gt; Replicate(): all_gather&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shard(src_dim) -&amp;gt; Shard(dst_dim): all_to_all&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replicate() -&amp;gt; Shard(dim): local chunking (i.e. torch.chunk)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial() -&amp;gt; Replicate(): all_reduce&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partial() -&amp;gt; Shard(dim): reduce_scatter&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;api&#34;&gt;APIï¼š&lt;/h3&gt;
&lt;p&gt;ä¸€ä¸ªDTensorå¯ä»¥ç”±å¦‚ä¸‹APIåˆ›å»ºï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;distribute_tensor()&lt;/code&gt; æ¥å£&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cuda:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rank&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dtensor_shard&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;distribute_tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;device_mesh&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mesh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;placements&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Shard&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;code&gt;from local tensor&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dtensor_shard&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DTensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;device_mesh&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DeviceMesh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cuda&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;world_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;placements&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Placement&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;shard&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# æ²¿ç¬¬0ç»´åˆ†ç‰‡&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;æˆ–è€…&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dtensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;local_tensor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_dtensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;device_mesh&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DeviceMesh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cuda&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;placements&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Placement&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;replicate&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;å·¥å‚å‡½æ•°&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dtensor_ones&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DTensor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;device_mesh&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DeviceMesh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cuda&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;world_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;placements&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Placement&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;shard&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;åˆ›å»ºå¥½DTensorä¹‹åï¼Œå°±å¯ä»¥æŠŠDTensorå½“ä½œæ™®é€šçš„Tensorå¯¹è±¡å»æ“ä½œå’Œè¿ç®—ï¼ŒDTensorä¼šä¸ºä½ åœ¨å¿…è¦æ—¶æ’å…¥æ‰€éœ€è¦çš„é€šä¿¡æ“ä½œã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorch FSDP2 å¯¹æ¯” FSDP1çš„å‡çº§</title>
      <link>https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/</link>
      <pubDate>Thu, 18 Dec 2025 21:37:02 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/pytorch-fsdp2-%E5%AF%B9%E6%AF%94-fsdp1%E7%9A%84%E5%8D%87%E7%BA%A7/</guid>
      <description>&lt;h2 id=&#34;æ¦‚è¿°&#34;&gt;æ¦‚è¿°&lt;/h2&gt;
&lt;p&gt;FSDP2 ä¸FSDP1 ä¿æŒäº†ç›¸åŒçš„å¹¶è¡Œæ€è·¯ï¼Œå°†å‚æ•°ã€æ¢¯åº¦ç­‰åˆ†å—ï¼Œåœ¨éœ€è¦æ—¶all-gatherèšé›†åˆ°ä¸€èµ·ï¼Œç”¨å®Œå³åˆ»ä¸¢å¼ƒã€‚é€šè¿‡forward å’Œ backward hookè‡ªåŠ¨å¤„ç†ç›¸åº”çš„shardå’Œunshardæ“ä½œï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶é€šè¿‡prefetchè¿›ä¸€æ­¥æ©ç›–é€šä¿¡å’Œè®¡ç®—çš„è€—æ—¶ï¼Œæå‡æ€§èƒ½ã€‚ä½†æ˜¯FSDP2å’ŒFSDP1ä¹Ÿå­˜åœ¨ä¸€äº›diffï¼Œç”šè‡³æ¥å£ä¹Ÿä¸å†å‘åå…¼å®¹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;fsdp2æ¢³ç†åŠæ›´æ–°ç‚¹&#34;&gt;FSDP2æ¢³ç†åŠæ›´æ–°ç‚¹&lt;/h2&gt;
&lt;p&gt;å®˜æ–¹çš„æè¿°æ˜¯è¿™æ ·çš„ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Comparing with FSDP1, FSDP2 has following advantages:
Representing sharded parameters as DTensor sharded on dim-i, allowing for easy manipulation of individual parameters, communication-free sharded state dicts, and a simpler meta-device initialization flow.
Improving memory management system that achieves lower and deterministic GPU memory by avoiding recordStream (doc) and does so without any CPU synchronization.
Offering a tensor subclass extension point to customize the all-gather, e.g. for float8 all-gather for float8 linears (doc), and NF4 for QLoRA (doc)
Mixing frozen and non-frozen parameters can in the same communication group without using extra memory.
ä¸»è¦æ¢³ç†ä¸‹DTensorä»¥åŠrecordStreamç›¸å…³çš„æ”¹åŠ¨ã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorch pr(1)ä¹‹torch.accelerator</title>
      <link>https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/</link>
      <pubDate>Sun, 07 Dec 2025 20:44:02 +0800</pubDate>
      <guid>https://zhouyeyu.github.io/posts/pytorch-pr1%E4%B9%8Btorch.accelerator/</guid>
      <description>&lt;h2 id=&#34;add-unified-memory-apis-for-torchaccelerator-152932&#34;&gt;Add unified memory APIs for torch.accelerator #152932&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/pull/152932&#34;&gt;https://github.com/pytorch/pytorch/pull/152932&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä¸ºtorch.acceleratorå¢åŠ äº†å¦‚ä¸‹APIï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;empty_cache&lt;/li&gt;
&lt;li&gt;max_memory_allocated&lt;/li&gt;
&lt;li&gt;max_memory_reserved&lt;/li&gt;
&lt;li&gt;memory_allocated&lt;/li&gt;
&lt;li&gt;memory_reserved&lt;/li&gt;
&lt;li&gt;memory_stats&lt;/li&gt;
&lt;li&gt;reset_accumulated_memory_stats&lt;/li&gt;
&lt;li&gt;reset_peak_memory_stats&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;è¿™ä¸ªprè¿›ä¸€æ­¥è¯´æ˜äº†PyTorchæ­£åœ¨å»CUDAåŒ–ï¼Œä»è€Œè½¬å˜ä¸ºä¸€ç³»åˆ—åŠ é€Ÿå™¨æä¾›è®¡ç®—æ¡†æ¶ï¼Œå³ä½¿ä»–ä»¬çš„githubä¸»é¡µä»ç„¶å†™ç€ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿™äº›ç”¨æ³•å¾ˆé•¿ä¸€æ®µæ—¶é—´æ˜¯ä¸ºcudaæä¾›çš„ï¼Œè°ƒç”¨æ–¹å¼ä¸»è¦ä¸º&lt;code&gt;torch.cuda.empty_cache&lt;/code&gt;ç­‰ï¼Œæ¥å£ä¸»è¦ä½œç”¨æ˜¯æŸ¥çœ‹deviceä¸Šçš„æ˜¾å­˜å ç”¨å¦‚&lt;code&gt;memory_allocated&lt;/code&gt;å’Œ&lt;code&gt;memory_reserved&lt;/code&gt;ï¼Œä»¥åŠæ¸…ç†å’Œé‡ç½®ç›¸å…³çŠ¶æ€ã€‚
ä¸€èµ·æ¥çœ‹ä¸‹ç›¸å…³å‡½æ•°çš„è°ƒç”¨æµç¨‹ï¼Œä»¥cudaä¸ºä¾‹ï¼Œä»æºç åˆ†æè¿™ä¸ªæ¥å£æ˜¯æ€ä¹ˆä¸€æ­¥ä¸€æ­¥è·å–åº•å±‚çš„çŠ¶æ€çš„ã€‚é¦–å…ˆæ‰€æœ‰memoryç›¸å…³çš„æ•°æ®éƒ½æ˜¯ä»&lt;code&gt;memory_stats&lt;/code&gt;è¿™ä¸ªå­—å…¸å†…æå–å¯¹äºçš„value&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;memory_reserved&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_device_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Return the current :ref:`accelerator&amp;lt;accelerators&amp;gt;` device memory managed by the caching allocator
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    in bytes for a given device index.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        device_index (:class:`torch.device`, str, int, optional): the index of the device to target.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;            If not given, use :func:`torch.accelerator.current_device_index` by default.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;            If a :class:`torch.device` or str is provided, its type must match the current
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;            :ref:`accelerator&amp;lt;accelerators&amp;gt;` device type.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory_stats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;reserved_bytes.all.current&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;è€Œ&lt;code&gt;memory_stas&lt;/code&gt;é€šè¿‡åº•å±‚&lt;code&gt;_accelerator_getDeviceStats&lt;/code&gt;æ¥å£è·å–ç›¸å…³deviceçš„çŠ¶æ€&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>https://zhouyeyu.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://zhouyeyu.github.io/about/</guid>
      <description>about YuZhouye</description>
    </item>
  </channel>
</rss>
